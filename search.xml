<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubernetes 1.15.0 部署]]></title>
    <url>%2F2020%2F01%2F16%2F%E6%90%AD%E5%BB%BAk8s-1.15.0%2F</url>
    <content type="text"><![CDATA[运行环境 | CentOS 7.6 64位 | 阿里云 | kubernetes 1.15.0 |Docker 19.03.5, build 633a0ea 关闭防火墙1234567systemctl disable firewalld.service 禁用seliunxsetenforce 0swapoff -a 镜像源配置1234567891011121314151617181920yum install wget epel-release -ycd /etc/yum.repos.d/ &amp;&amp; mkdir bak &amp;&amp; mv -f *.repo bak/wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repowget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repowget -O /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repocat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum clean all &amp;&amp; yum makecacheyum -y install iotop iftop yum-utils net-tools git lrzsz expect gcc gcc-c++ make cmake libxml2-devel openssl-devel curl curl-devel unzip sudo ntp libaio-devel wget vim ncurses-devel autoconf automake zlib-devel python-devel bash-completion 配置无密钥访问12ssh-keygenssh-copy-id &lt;host&gt; 创建 /etc/sysctl.d/k8s.conf 文件1234cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF 安装 docker kubelet-1.15.0123yum -y install docker-ce-18.06.1.ceyum install -y kubelet-1.15.0 kubeadm-1.15.0 kubectl-1.15.0 slave节点执行 yum install -y kubeadm-1.15.0 kubelet-1.15.0docker kubelet 启动并开机自启123systemctl start docker &amp;&amp; systemctl enable dockersystemctl start kubelet.service &amp;&amp; systemctl enable kubelet.service 初始化12345kubeadm init --kubernetes-version=v1.15.0 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 --service-cidr=10.1.0.0/16 --apiserver-advertise-address=172.17.0.10mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 子节点加入12kubeadm join 172.17.0.10:6443 --token u97eh2.ba81kyeg93on6dc1 \ --discovery-token-ca-cert-hash sha256:5a533848690d47872d4370112bc0c2d22e49ac1971bc2486a40e6e6f221951ff Docker 配置 vi /etc/docker/daemon.json123456789&#123; &quot;insecure-registries&quot;: [&quot;172.17.0.20&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;20m&quot;, &quot;max-file&quot;: &quot;10&quot; &#125;, &quot;live-restore&quot;: true&#125; 重启服务 systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl restart kubelet配置flannel网络1kubectl apply -f kube-flannel.yml 查看各节点及pods123456789101112131415161718192021222324252627[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 24m v1.15.0slave0 Ready &lt;none&gt; 13m v1.15.0slave1 Ready &lt;none&gt; 13m v1.15.0slave2 Ready &lt;none&gt; 13m v1.15.0[root@master ~]# kubectl get pods,svc --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-system pod/coredns-bccdc95cf-9hmjg 1/1 Running 0 24m 10.244.0.2 master &lt;none&gt; &lt;none&gt;kube-system pod/coredns-bccdc95cf-vq4px 1/1 Running 0 24m 10.244.0.3 master &lt;none&gt; &lt;none&gt;kube-system pod/etcd-master 1/1 Running 0 23m 172.17.0.10 master &lt;none&gt; &lt;none&gt;kube-system pod/kube-apiserver-master 1/1 Running 0 23m 172.17.0.10 master &lt;none&gt; &lt;none&gt;kube-system pod/kube-controller-manager-master 1/1 Running 0 23m 172.17.0.10 master &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-cqvbg 1/1 Running 0 11m 172.17.0.11 slave0 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-k9pm2 1/1 Running 0 11m 172.17.0.10 master &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-klsvd 1/1 Running 0 11m 172.17.0.13 slave2 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-rmg9k 1/1 Running 0 11m 172.17.0.12 slave1 &lt;none&gt; &lt;none&gt;kube-system pod/kube-proxy-4twzz 1/1 Running 0 13m 172.17.0.12 slave1 &lt;none&gt; &lt;none&gt;kube-system pod/kube-proxy-kps2m 1/1 Running 0 13m 172.17.0.13 slave2 &lt;none&gt; &lt;none&gt;kube-system pod/kube-proxy-qhztw 1/1 Running 0 13m 172.17.0.11 slave0 &lt;none&gt; &lt;none&gt;kube-system pod/kube-proxy-s5h9s 1/1 Running 0 24m 172.17.0.10 master &lt;none&gt; &lt;none&gt;kube-system pod/kube-scheduler-master 1/1 Running 0 23m 172.17.0.10 master &lt;none&gt; &lt;none&gt;NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORdefault service/kubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443/TCP 24m &lt;none&gt;kube-system service/kube-dns ClusterIP 10.1.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 24m k8s-app=kube-dns 配置dashboard1kubectl apply -f kubernetes-dashboard.yaml 创建集群管理账号12345kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin查看 tokenkubectl describe secret -n kube-system dashboard-admin 常用命令查看pod详细信息 kubectl describe pod &lt;pod名称&gt; --namespace kube-system查看pod日志信息 kubectl logs &lt;pod名称&gt; -n kube-system删除应用 kubectl delete deployment &lt;pod名称&gt; -n kube-system根据label删除 kubectl delete pod -l app=flannel -n kube-system扩容 kubectl scale deployment &lt;pod名称&gt; --replicas=8flannel及dashboard文件flannel.yml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelrules: - apiGroups: - "" resources: - pods verbs: - get - apiGroups: - "" resources: - nodes verbs: - list - watch - apiGroups: - "" resources: - nodes/status verbs: - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannelsubjects:- kind: ServiceAccount name: flannel namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: flannel namespace: kube-system---kind: ConfigMapapiVersion: v1metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flanneldata: cni-conf.json: | &#123; "name": "cbr0", "plugins": [ &#123; "type": "flannel", "delegate": &#123; "hairpinMode": true, "isDefaultGateway": true &#125; &#125;, &#123; "type": "portmap", "capabilities": &#123; "portMappings": true &#125; &#125; ] &#125; net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule - key: node.kubernetes.io/not-ready operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: "100m" memory: "50Mi" limits: cpu: "100m" memory: "50Mi" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-arm64 namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: arm64 tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule - key: node.kubernetes.io/not-ready operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-arm64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-arm64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: "100m" memory: "50Mi" limits: cpu: "100m" memory: "50Mi" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-arm namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: arm tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule - key: node.kubernetes.io/not-ready operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-arm command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-arm command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: "100m" memory: "50Mi" limits: cpu: "100m" memory: "50Mi" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-ppc64le namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: ppc64le tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule - key: node.kubernetes.io/not-ready operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-ppc64le command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-ppc64le command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: "100m" memory: "50Mi" limits: cpu: "100m" memory: "50Mi" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-s390x namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: s390x tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule - key: node.kubernetes.io/not-ready operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-s390x command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-s390x command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: "100m" memory: "50Mi" limits: cpu: "100m" memory: "50Mi" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg dashboard.yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169# Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# Configuration to deploy release version of the Dashboard UI compatible with# Kubernetes 1.8.## Example usage: kubectl create -f &lt;this_file&gt;# ------------------- Dashboard Secret ------------------- #apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---# ------------------- Dashboard Service Account ------------------- #apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Role &amp; Role Binding ------------------- #kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-minimal namespace: kube-systemrules: # Allow Dashboard to create 'kubernetes-dashboard2-key-holder' secret.- apiGroups: [""] resources: ["secrets"] verbs: ["create"] # Allow Dashboard to create 'kubernetes-dashboard2-settings' config map.- apiGroups: [""] resources: ["configmaps"] verbs: ["create"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups: [""] resources: ["secrets"] resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"] verbs: ["get", "update", "delete"] # Allow Dashboard to get and update 'kubernetes-dashboard2-settings' config map.- apiGroups: [""] resources: ["configmaps"] resourceNames: ["kubernetes-dashboard-settings"] verbs: ["get", "update"] # Allow Dashboard to get metrics from heapster.- apiGroups: [""] resources: ["services"] resourceNames: ["heapster"] verbs: ["proxy"]- apiGroups: [""] resources: ["services/proxy"] resourceNames: ["heapster", "http:heapster:", "https:heapster:"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kubernetes-dashboard-minimal namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimalsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Deployment ------------------- #kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 80 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard]]></content>
      <categories>
        <category>linux</category>
        <category>k8s</category>
        <category>centos</category>
        <category>部署</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLServer向HDFS迁移(导入)]]></title>
    <url>%2F2019%2F11%2F13%2FSQLServer%E5%90%91HDFS%E8%BF%81%E7%A7%BB(%E5%AF%BC%E5%85%A5)%2F</url>
    <content type="text"><![CDATA[运行环境 | windows 7 | sqoop 1.4.7 | hadoop-2.7.7 | hive-1.2.2 | SQLServer 2005 | VMware Workstation 15 Pro SQLServer 导入HDFS 命令12345./sqoop import \--connect 'jdbc:sqlserver://192.168.1.171;username=sa;password=sa;database=db' \--table=data_dictonary_test \--target-dir /sqlserver/output \-m 1]]></content>
      <categories>
        <category>linux</category>
        <category>SQLServer</category>
        <category>windows</category>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>SQLServer</tag>
        <tag>HDFS</tag>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle向HDFS迁移(导入)数据报错]]></title>
    <url>%2F2019%2F11%2F13%2FOracle%E5%90%91HDFS%E8%BF%81%E7%A7%BB(%E5%AF%BC%E5%85%A5)%E6%95%B0%E6%8D%AE%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | sqoop 1.4.7 | hadoop-2.7.7 | hive-1.2.2 | oracle 12C | MySQL5.7.28 | jdk1.8 | VMware Workstation 15 Pro Oracle导入HDFS 命令1234./sqoop import --connect jdbc:oracle:thin:@192.168.11.53:1521:oracledb \--username HIVE --password root123 \--table EXPHIVE -m 1 \--target-dir /oracle/output 报错表或视图不存在 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505119/11/13 09:49:20 ERROR manager.SqlManager: Error executing statement: java.sql.SQLSyntaxErrorException: ORA-00942: 表或视图不存在java.sql.SQLSyntaxErrorException: ORA-00942: 表或视图不存在 at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494) at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446) at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623) at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252) at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612) at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226) at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59) at oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747) at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904) at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082) at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780) at oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343) at oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822) at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165) at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:777) at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786) at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289) at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260) at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246) at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327) at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1872) at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1671) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252)Caused by: Error : 942, Position : 16, Sql = SELECT t.* FROM TEST t WHERE 1=0, OriginalSql = SELECT t.* FROM TEST t WHERE 1=0, Error Msg = ORA-00942: 表或视图不存在 at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498) ... 31 more19/11/13 09:49:20 ERROR tool.ImportTool: Import failed: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1677) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252) 报错标识符无效123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960Error: java.io.IOException: SQLException in nextKeyValue at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277) at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556) at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.sql.SQLSyntaxErrorException: ORA-00904: "NAME": 标识符无效...19/11/13 09:52:08 INFO mapreduce.Job: Task Id : attempt_1573608552785_0010_m_000000_2, Status : FAILEDError: java.io.IOException: SQLException in nextKeyValue at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277) at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556) at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.sql.SQLSyntaxErrorException: ORA-00904: "NAME": 标识符无效 at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494) at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446) at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623) at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252) at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612) at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226) at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59) at oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747) at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904) at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082) at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780) at oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343) at oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822) at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165) at org.apache.sqoop.mapreduce.db.DBRecordReader.executeQuery(DBRecordReader.java:111) at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:235) ... 12 moreCaused by: Error : 904, Position : 11, Sql = SELECT id, name FROM TEST WHERE ( 1=1 ) AND ( 1=1 ), OriginalSql = SELECT id, name FROM TEST WHERE ( 1=1 ) AND ( 1=1 ), Error Msg = ORA-00904: "NAME": 标识符无效 at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498) ... 28 more...19/11/13 09:52:12 ERROR tool.ImportTool: Import failed: Import job failed! 解决方法： 以上错误将 Oracle数据库中表名与字段名改为大写，在次运行成功 报错java.sql.SQLException: Listener refused the connection with the following error:ORA-12505, TNS:listener does not currently know of SID given in connect descriptor123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354...19/11/13 09:54:16 ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: Listener refused the connection with the following error:ORA-12505, TNS:listener does not currently know of SID given in connect descriptor java.sql.SQLException: Listener refused the connection with the following error:ORA-12505, TNS:listener does not currently know of SID given in connect descriptor at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:774) at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:688) at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:39) at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:691) at java.sql.DriverManager.getConnection(DriverManager.java:664) at java.sql.DriverManager.getConnection(DriverManager.java:247) at org.apache.sqoop.manager.OracleManager.makeConnection(OracleManager.java:329) at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:59) at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:763) at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786) at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289) at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260) at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246) at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327) at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1872) at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1671) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252)Caused by: oracle.net.ns.NetException: Listener refused the connection with the following error:ORA-12505, TNS:listener does not currently know of SID given in connect descriptor at oracle.net.ns.NSProtocolNIO.negotiateConnection(NSProtocolNIO.java:271) at oracle.net.ns.NSProtocol.connect(NSProtocol.java:317) at oracle.jdbc.driver.T4CConnection.connect(T4CConnection.java:1438) at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:518) ... 24 more19/11/13 09:54:16 ERROR tool.ImportTool: Import failed: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1677) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252)... 解决方法： 填写Oracle数据库中对应SID(不是server ID)，在次运行成功 报错ERROR tool.ImportTool: Import failed: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://master:9000/oracle/output already exists12345678910111213141516171819202122232419/11/15 10:37:45 ERROR tool.ImportTool: Import failed: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://master:9000/oracle/output already exists at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146) at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308) at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:200) at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:173) at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:270) at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692) at org.apache.sqoop.manager.OracleManager.importTable(OracleManager.java:454) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:520) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252) 解决方法： HDFS 目录已存在，删除问题解决]]></content>
      <categories>
        <category>linux</category>
        <category>Oracle</category>
        <category>centos</category>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>sqoop</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx + tomcat 集群测试]]></title>
    <url>%2F2019%2F08%2F28%2FTomcatNginxab%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[运行环境| centos 7.0 | VMware Workstation 15 Pro | tomcat 8.5 | nginx 1.14.2 Tomcat集群 + Nginx负载均衡测试 测试命令：ab -n20000 -c1000 http://192.168.244.10/drp/index.jsp Nginx + Tomcat集群测试结果1234567891011121314151617181920212223242526272829303132333435363738Server Software: nginx/1.14.2Server Hostname: 192.168.244.10Server Port: 80Document Path: /drp/index.jspDocument Length: 216 bytesConcurrency Level: 1000Time taken for tests: 9.126 secondsComplete requests: 20000Failed requests: 488 (Connect: 0, Receive: 0, Length: 488, Exceptions: 0)Write errors: 0Non-2xx responses: 488Total transferred: 8973152 bytesHTML transferred: 4308776 bytesRequests per second: 2191.63 [#/sec] (mean)Time per request: 456.282 [ms] (mean)Time per request: 0.456 [ms] (mean, across all concurrent requests)Transfer rate: 960.24 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 226 628.0 0 7000Processing: 0 146 325.0 51 7122Waiting: 0 146 325.0 51 7122Total: 1 372 757.6 60 8422Percentage of the requests served within a certain time (ms) 50% 60 66% 147 75% 260 80% 385 90% 1070 95% 1831 98% 3062 99% 3230 100% 8422 (longest request) Tomcat 集群测试结果123456789101112131415161718192021222324252627282930313233343536Server Software: Server Hostname: 192.168.244.11Server Port: 8080Document Path: /drp/index.jspDocument Length: 216 bytesConcurrency Level: 1000Time taken for tests: 5.161 secondsComplete requests: 20000Failed requests: 0Write errors: 0Total transferred: 8733153 bytesHTML transferred: 4397112 bytesRequests per second: 3875.41 [#/sec] (mean)Time per request: 258.037 [ms] (mean)Time per request: 0.258 [ms] (mean, across all concurrent requests)Transfer rate: 1652.56 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 153 282.6 70 1125Processing: 17 96 51.3 85 2010Waiting: 0 74 44.9 69 2004Total: 28 250 294.5 167 2025Percentage of the requests served within a certain time (ms) 50% 167 66% 186 75% 198 80% 210 90% 293 95% 1185 98% 1219 99% 1232 100% 2025 (longest request)]]></content>
      <categories>
        <category>测试</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
        <tag>测试</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive启动报错2]]></title>
    <url>%2F2019%2F08%2F25%2FHive%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%992%2F</url>
    <content type="text"><![CDATA[启动Hive报错 Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=”/tmp”:root:supergroup:drwx—— 123456789101112131415161718192021222324Connecting to jdbc:hive2://master:10000Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode="/tmp":root:supergroup:drwx------ at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:279) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:206) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:507) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1612) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1630) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:551) at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:110) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3000) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1107) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:873) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489) (state=,code=0)Beeline version 1.2.2 by Apache Hive 进入Hive后1234Connecting to jdbc:hive2://master:10000Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)Beeline version 1.2.2 by Apache Hive0: jdbc:hive2://master:10000 (closed)&gt; 解决方法：HDFS 没有读写权限 配置文件中加入123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;&lt;/property&gt; 对 hdfs /tmp 加入777 权限 hdfs dfs -chmod -R 777 /tmp/]]></content>
      <categories>
        <category>改错</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive HBase 整合]]></title>
    <url>%2F2019%2F08%2F23%2FHive%E4%B8%8EHBase%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | hbase 1.2.10 | hive 1.2.2 Hive只支持insert和delete操作，并不支持update操作，所以无法实施更新hive里的数据，而HBase正好弥补了这一点，所以在某些场景下需要将Hive和HBase整合起来一起使用。Hive与HBase整合的实现是利用两者本身对外的API接口互相通信来完成的，其具体工作交由Hive的lib目录中的hive-hbase-handler-*.jar工具类来实现，通信原理如下图所示 Hive整合HBase后的使用场景： 通过Hive把数据加载到HBase中，数据源可以是文件也可以是Hive中的表。 通过整合，让HBase支持JOIN、GROUP等SQL查询语法。 通过整合，不仅可完成HBase的数据实时查询，也可以使用Hive查询HBase中的数据完成复杂的数据分析。 把HBASE_HOME/lib下所有的jar 复制到HIVE_HOME/lib/下cp -n $HBASE_HOME/lib/* $HIVE_HOME/lib/ -n 表示对于目标路径下已经存在的文件，则不复制过去 修改hive-site.xml文件，增加HBase的ZooKeeper集群信息1234&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt;&lt;/property&gt; 指定 hbase 所使用的 zookeeper 集群的地址：默认端口是 2181，可以不写set hbase.zookeeper.quorum=node01:2181,node02:2181,node03:2181;set zookeeper.znode.parent=/var/zookeeper/local; 指定 hbase 在 zookeeper 中使用的根目录该路径在hbase-site.xml可以查询，你在搭建hbase集群的时候，这个参数是必须设置的1234&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/var/zookeeper/local&lt;/value&gt;&lt;/property&gt; 把HIVE_HOME/lib/hive-hbase-handler-1.2.1.jar 复制到HBASE_HOME/lib/下cp */hive-1.2.1/lib/hive-hbase-handler-1.2.1.jar */hbase/lib/ 启动HBase,创建HBase表”student”12345678910111213141516171819202122232425create 'student','info'put 'student','0001','info:name','Tom'put 'student','0001','info:age','18'put 'student','0002','info:name','Madeline'put 'student','0002','info:age','15'put 'student','0003','info:name','jed'put 'student','0003','info:age','16'put 'student','0004','info:name','olivia'put 'student','0004','info:age','18'put 'student','0005','info:name','sarah'put 'student','0005','info:age','19'put 'student','0006','info:name','xiaohong'put 'student','0006','info:age','19'put 'student','0007','info:name','zhangsan'put 'student','0007','info:age','20'put 'student','0008','info:name','wangwu'put 'student','0008','info:age','22' Hive中创建表结构123CREATE EXTERNAL TABLE hive_student_name (key string,name string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,info:name") TBLPROPERTIES ("hbase.table.name" = "student");CREATE EXTERNAL TABLE hive_student_age (key string,name string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,info:age") TBLPROPERTIES ("hbase.table.name" = "student"); 通过Hive客户端可以查询该表的数据]]></content>
      <categories>
        <category>部署</category>
        <category>hive</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>hive</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cesium(笔记-5)]]></title>
    <url>%2F2019%2F08%2F21%2Fcesium(%E7%AC%94%E8%AE%B0-5)%2F</url>
    <content type="text"><![CDATA[运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro 记初步实现 cesium 通过鼠标完成模型创建 通过鼠标滑动实时抓取当前坐标与高度，动态刷新到左侧二维缩略图信息框中，用户可通过手动输入或鼠标实时获取坐标点，点击确认添加后，在指定坐标点生成对应模型 案例模型采用3D Tiles 及 GLTF3D Tiles官方介绍GLTF 官方介绍 3D Tiles介绍：3D Tiles 是： Open（开放） Optimized for streaming and rendering（针对流和渲染进行了优化） Designed for 3D（专为3D设计） Interactive（交互式互动） Styleable（设置样式） Adaptable（适应性强） Flexible（灵活） Heterogeneous（异构的 Precise（精确） Temporal（时间动态 渲染 3D Tiles是基于状态，从UNLOADING开始，通过一系列的request，完成最初的数据加载过程，结束LOADING状态，进入Pocessing过程，也就是数据解析。数据解析完后进入READY状态，通过selectTile，最终调用Content对应的update方法，构造最终的drawcommand，加入渲染队列。当然，如果有需要释放的Tile，则在unloadTiles中处理。细心的人会发现Pocessing和Ready状态。最终调用的都是update方法。这里解释一下：3D Tiles中主要的数据部分就是glTF，而glTF也是基于状态管理的，无论是glTF的解析还是构造DrawCommand，只是state不同，都是在update方法中完成的。 GLTF介绍：gltf的核心是一个JSON文件。这个文件描述了3D场景的全部内容。它由场景结构本身的描述组成，由定义场景图的节点层次结构给出。场景中出现的三维对象是使用附着到节点上的网格定义的。材质定义对象的外观。动画描述了随着时间的推移，三维对象是如何转换的（例如，旋转到转换），而蒙皮定义了基于骨架姿势的对象几何体是如何变形的。摄影机描述渲染器的视图配置。 基础结构 scene：整个场景的入口点，由node构成图（树）结构（类似OSG的场景组织）组成 node：场景层级中的一个节点，可以包含位置变换，可以有子节点。同时，node通过指向mesh、camera、skin来描述node的形变 camera：定义渲染场景的视点配置 mesh：描述了在场景中出现的几何物体，并通过accessor（访问器）来访问其真实的几何数据，通过material（材质）来确定渲染外观 skin：蒙皮描述模型绑定到骨骼上的参数，用来实现骨骼动画，其真实数据也是通过访问器来获取的 animation：骨骼动画，描述某个节点怎么随着时间运动 accessor：访问器，定义了如何从二进制数据源中获取数据，在mesh、animation、skin中都会用到访问器。其指向buffer和bufferview，在这里面存着真正的几何数据 material：材质包含了定义物体模型外观的参数，特别是纹理参数 texture：包括采样器和图片，定义如何将纹理映射到模型 对外部数据的引用二进制数据，如三维对象的几何图形和纹理，通常不包含在JSON文件中。相反，它们存储在专用文件中，JSON部分只包含指向这些文件的链接。这允许二进制数据以非常紧凑的形式存储，并且可以通过Web高效地传输。此外，数据可以直接存储在渲染器中，而不必解析、解码或预处理数据。 实现思路需要获取坐标点与高度 屏幕坐标 1234var handler= new Cesium.ScreenSpaceEventHandler(viewer.scene.canvas);handler.setInputAction(function (movement) &#123; console.log(movement.position);&#125;, Cesium.ScreenSpaceEventType.LEFT_CLICK); 世界坐标 12345var handler = new Cesium.ScreenSpaceEventHandler(viewer.scene.canvas);handler.setInputAction(function (movement) &#123; var position = viewer.scene.camera.pickEllipsoid(movement.position, viewer.scene.globe.ellipsoid); console.log(position);&#125;, Cesium.ScreenSpaceEventType.LEFT_CLICK); 场景坐标 12345var handler = new Cesium.ScreenSpaceEventHandler(viewer.scene.canvas);handler.setInputAction(function (movement) &#123; var position = viewer.scene.pickPosition(movement.position); console.log(position);&#125;, Cesium.ScreenSpaceEventType.LEFT_CLICK); 地标坐标 123456var handler = new Cesium.ScreenSpaceEventHandler(viewer.scene.canvas);handler.setInputAction(function (movement) &#123; var ray=viewer.camera.getPickRay(movement.position); var position = viewer.scene.globe.pick(ray, viewer.scene); console.log(position);&#125;, Cesium.ScreenSpaceEventType.LEFT_CLICK); 加载模型123456789var entity = viewer.entities.add(&#123; name: 'model/CesiumMilkTruck/CesiumMilkTruck.glb', position: position, orientation: orientation, model: &#123; uri: 'model/CesiumMilkTruck/CesiumMilkTruck.glb', &#125;&#125;);viewer.trackedEntity = entity; 完整代码实现JS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108 var viewer = new Cesium.Viewer('cesiumContainer', &#123; imageryProvider: Cesium.createOpenStreetMapImageryProvider(&#123; url: 'https://a.tile.openstreetmap.org/' &#125;), shouldAnimate: false, infoBox: false, selectionIndicator: false, shadows: false, geocoder: false, homeButton: false, sceneModePicker: false, baseLayerPicker: false, navigationHelpButton: false, // animation: false, // timeline: false, fullscreenButton: false &#125;); var scene = viewer.scene; // var handler = new Cesium.ScreenSpaceEventHandler(scene.canvas); handler.setInputAction(function (movement) &#123; var cartesian = scene.camera.pickEllipsoid(movement.endPosition, ellipsoid); var ellipsoid = scene.globe.ellipsoid; if (cartesian) &#123; var cartographic = ellipsoid.cartesianToCartographic(cartesian); var coords = 'lng' + Cesium.Math.toDegrees(cartographic.longitude).toFixed(6) + ', ' + 'lat' + Cesium.Math.toDegrees(cartographic.latitude).toFixed(6) + '，' + 'hig' + Math.ceil(viewer.camera.positionCartographic.height); console.log("--&gt;" + coords) $("#val1").val(Cesium.Math.toDegrees(cartographic.longitude).toFixed(6)) $("#val2").val(Cesium.Math.toDegrees(cartographic.latitude).toFixed(6)) $("#val3").val(Math.ceil(viewer.camera.positionCartographic.height)) &#125; else &#123; // console.log("--&gt;" + ERROR) &#125; &#125;, Cesium.ScreenSpaceEventType.MOUSE_MOVE); var params = &#123; tx: $("#val1").val(), //模型中心X轴坐标（经度，单位：十进制度） ty: $("#val2").val(), //模型中心Y轴坐标（纬度，单位：十进制度） tz: 0, //模型中心Z轴坐标（高程，单位：米） rx: 0, //X轴（经度）方向旋转角度（单位：度） ry: 0, //Y轴（纬度）方向旋转角度（单位：度） rz: 0 //Z轴（高程）方向旋转角度（单位：度） &#125;; function createModel() &#123; console.log("------&gt;" + $("#height").val()); // // viewer.entities.removeAll(); var position = Cesium.Cartesian3.fromDegrees($("#val1").val(), $("#val2").val(), $("#val3").val()); var heading = Cesium.Math.toRadians(135); var pitch = 0; var roll = 0; var hpr = new Cesium.HeadingPitchRoll(heading, pitch, roll); var orientation = Cesium.Transforms.headingPitchRollQuaternion(position, hpr); var entity = viewer.entities.add(&#123; name: 'model/CesiumMilkTruck/CesiumMilkTruck.glb', position: position, orientation: orientation, model: &#123; uri: 'model/CesiumMilkTruck/CesiumMilkTruck.glb', &#125; &#125;); viewer.trackedEntity = entity;// 3D Tiles /*viewer.scene.globe.depthTestAgainstTerrain = true; // var url = "model/daodan/tileset.json"; var url = Cesium.IonResource.fromAssetId(1); var tileset = new Cesium.Cesium3DTileset(&#123;url: url&#125;); var primitive1 = viewer.scene.primitives.add(tileset); primitive1.readyPromise.then(function (t) &#123; console.log("====&gt;"+$("#val1").val() + ","+$("#val2").val()) var originalSphere = t.boundingSphere; var radius = originalSphere.radius; viewer.zoomTo(t, new Cesium.HeadingPitchRange(0.5, -0.5, radius * 4.0)); // var mx = Cesium.Matrix3.fromRotationX(Cesium.Math.toRadians(params.rx)); var my = Cesium.Matrix3.fromRotationY(Cesium.Math.toRadians(params.ry)); var mz = Cesium.Matrix3.fromRotationZ(Cesium.Math.toRadians(params.rz)); var rotationX = Cesium.Matrix4.fromRotationTranslation(mx); var rotationY = Cesium.Matrix4.fromRotationTranslation(my); var rotationZ = Cesium.Matrix4.fromRotationTranslation(mz); // var position = Cesium.Cartesian3.fromDegrees(params.tx, params.ty, $("#val3").val()); var position = Cesium.Cartesian3.fromDegrees($("#val1").val(), $("#val2").val(), $("#val3").val()); var m = Cesium.Transforms.eastNorthUpToFixedFrame(position); // Cesium.Matrix4.multiply(m, rotationX, m); Cesium.Matrix4.multiply(m, rotationY, m); Cesium.Matrix4.multiply(m, rotationZ, m); tileset._root.transform = m; &#125;).otherwise(function (error) &#123; var msg = JSON.stringify(error); console.log("msg -------------&gt;" + msg); &#125;);*/ &#125; HTML 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 &lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"&gt; &lt;link href="css/widgets.css" rel="stylesheet"/&gt; &lt;title&gt;add model&lt;/title&gt; &lt;script src="https://cdn.bootcss.com/jquery/2.1.1/jquery.min.js"&gt;&lt;/script&gt; &lt;link rel="stylesheet" href="css/style.css"/&gt; &lt;link rel="stylesheet" href="bootstrap/css/bootstrap.css"/&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="bootstrap/js/bootstrap.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="js/Sandcastle-header.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="js/require.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="js/Cesium.js"&gt;&lt;/script&gt; &lt;!-- 开发环境版本，包含了有帮助的命令行警告 --&gt; &lt;script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"&gt;&lt;/script&gt; &lt;style&gt; html, body, #cesiumContainer &#123; width: 100%; height: 100%; margin: 0; padding: 0; overflow: hidden; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;style&gt; @import url(css/bucket.css); #toolbar &#123; background: rgba(42, 42, 42, 0.8); padding: 4px; border-radius: 4px; &#125; #toolbar input &#123; vertical-align: middle; padding-top: 2px; padding-bottom: 2px; &#125;&lt;/style&gt;&lt;div id="cesiumContainer" class="fullSize"&gt;&lt;/div&gt;&lt;div id="loadingOverlay"&gt;&lt;h1&gt;Loading...&lt;/h1&gt;&lt;/div&gt;&lt;div id="toolbar"&gt; &lt;!--&lt;div&gt;Height&lt;/div&gt;--&gt; &lt;!--&lt;input type="range" min="-100.0" max="100.0" step="1" data-bind="value: height, valueUpdate: 'input'"&gt;--&gt; &lt;!--&lt;input type="text" size="5" data-bind="value: height"&gt;--&gt; &lt;!--&lt;input value="0" id="height" type="text"&gt;--&gt; &lt;!--&lt;button onclick="createModel()"&gt;添加&lt;/button&gt;--&gt; &lt;div class="panel panel-default"&gt; &lt;div class="panel-heading"&gt;车&lt;/div&gt; &lt;div class="panel-body"&gt; &lt;div&gt; &lt;img src="img/car.jpg" style="height: 100px"&gt; &lt;/div&gt; &lt;hr/&gt; &lt;!-- Single button --&gt; &lt;div class="btn-group"&gt; &lt;button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"&gt; 参数 &lt;span class="caret"&gt;&lt;/span&gt; &lt;/button&gt; &lt;ul class="dropdown-menu"&gt; &lt;li&gt;&lt;a href="#"&gt;经度&lt;input id="val1" type="text" style="color: #000000"/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;纬度&lt;input id="val2" type="text" style="color: #000000"/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;高度&lt;input id="val3" type="text" style="color: #000000"/&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;button onclick="createModel()"&gt;&lt;a&gt;确认添加&lt;/a&gt;&lt;/button&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 效果图]]></content>
      <categories>
        <category>cesium</category>
        <category>js</category>
      </categories>
      <tags>
        <tag>cesium</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cesium(笔记-4)]]></title>
    <url>%2F2019%2F08%2F20%2Fcesium(%E7%AC%94%E8%AE%B0-4)%2F</url>
    <content type="text"><![CDATA[运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro 粒子系统是一种图形技术，可以模拟复杂的物理效果。粒子系统是小图像的集合，当它们一起观看时，会形成一个更复杂的“模糊”物体，如火、烟、天气或烟花fireworkds。通过使用诸如初始位置、速度和寿命等属性指定单个粒子的行为，可以控制这些复杂的效果。 粒子系统效应在电影和电子游戏中很常见。例如，为了表示飞机的损坏，技术艺术家可以使用粒子系统来表示飞机引擎上的爆炸，然后渲染不同的粒子系统，表示飞机坠毁时的烟雾轨迹。 Particle system basics 粒子系统基础1234567891011121314151617var particleSystem = viewer.scene.primitives.add(new Cesium.ParticleSystem(&#123; image : '../../SampleData/smoke.png', imageSize : new Cesium.Cartesian2(20, 20), startScale : 1.0, endScale : 4.0, particleLife : 1.0, speed : 5.0, emitter : new Cesium.CircleEmitter(0.5), emissionRate : 5.0, modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0&#125;));``` ## Emitters 发射器当粒子诞生时，其初始位置和速度矢量由ParticleEmitter控制。发射器将每秒生成一些粒子，由emissionRate参数指定，根据发射器类型用随机速度初始化。### BoxEmitter 盒形发射器 var particleSystem = scene.primitives.add(new Cesium.ParticleSystem({ image : ‘../../SampleData/smoke.png’, color: Cesium.Color.MAGENTA, emissionRate: 5.0, emitter: new Cesium.BoxEmitter(new Cesium.Cartesian3(5.0, 5.0, 5.0)), imageSize : new Cesium.Cartesian2(25.0, 25.0), modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0}));12### CircleEmitter 圆形发射器BoxEmitter在一个盒子内随机取样的位置初始化粒子，从六个盒子表面中的一个引导出来. 12345678910111213### CircleEmitter 圆形发射器CircleEmitter在发射器上轴线方向上的圆形内的随机采样位置初始化粒子。```jsvar particleSystem = scene.primitives.add(new Cesium.ParticleSystem(&#123; image : &apos;../../SampleData/smoke.png&apos;, color: Cesium.Color.MAGENTA, emissionRate: 5.0, emitter: new Cesium.CircleEmitter(5.0), imageSize : new Cesium.Cartesian2(25.0, 25.0), modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0&#125;)); ConeEmitter 锥形发射器ConeEmitter在圆锥体的顶端初始化粒子，并以随机的角度引导它们离开圆锥体。123456789var particleSystem = scene.primitives.add(new Cesium.ParticleSystem(&#123; image : '../../SampleData/smoke.png', color: Cesium.Color.MAGENTA, emissionRate: 5.0, emitter: new Cesium.ConeEmitter(Cesium.Math.toRadians(30.0)), imageSize : new Cesium.Cartesian2(25.0, 25.0), modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0&#125;)); SphereEmitter 球形发射器SphereEmitter在球体内随机取样的位置初始化粒子，并将它们从球体中心向外引导。123456789var particleSystem = scene.primitives.add(new Cesium.ParticleSystem(&#123; image : '../../SampleData/smoke.png', color: Cesium.Color.MAGENTA, emissionRate: 5.0, emitter: new Cesium.SphereEmitter(5.0), imageSize : new Cesium.Cartesian2(25.0, 25.0), modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0&#125;)); Particle emission rate 粒子发射率emissionRate控制每秒发射多少粒子，这会改变系统中粒子的密度。 指定一组突burst以在指定时间发射粒子burst（如上面的动画所示）。这会增加粒子系统的多样性或爆炸性。12345bursts : [ new Cesium.ParticleBurst(&#123;time : 5.0, minimum : 300, maximum : 500&#125;), new Cesium.ParticleBurst(&#123;time : 10.0, minimum : 50, maximum : 100&#125;), new Cesium.ParticleBurst(&#123;time : 15.0, minimum : 200, maximum : 300&#125;)] Life of the particle and life of the system 粒子寿命和系统寿命12345lifetime : 16.0,loop: false//要随机化每个粒子的输出，使用变量minimumParticleLife和maximumArticleLife。minimumParticleLife: 5.0,maximumParticleLife: 10.0 Styling particles 样式化粒子123456789101112131415//Color 颜色startColor : Cesium.Color.LIGHTSEAGREEN.withAlpha(0.7),endColor : Cesium.Color.WHITE.withAlpha(0.0),//Size 大小minimumImageSize : new Cesium.Cartesian2(30.0, 30.0),maximumImageSize : new Cesium.Cartesian2(60.0, 60.0)//可以通过startScale和endscale属性在其生命周期中进行调整，以使粒子随时间增长或收缩。startScale: 1.0,endScale: 4.0//速度由speed或minimumSpeed和maximumSpeed控制。minimumSpeed: 5.0,maximumSpeed: 10.0 通过应用更新函数，可以进一步自定义粒子系统。对于重力、风或颜色更改等效果，它充当每个粒子的手动更新程序。项目系统有一个updateCallback，它在模拟过程中修改粒子的属性。此函数采用粒子和模拟时间步骤。大多数基于物理的效果将修改速度矢量以改变方向或速度。下面是一个让粒子对重力作出反应的例子：1234567891011121314var gravityVector = new Cesium.Cartesian3();var gravity = -(9.8 * 9.8);function applyGravity(p, dt) &#123; // Compute a local up vector for each particle in geocentric space. var position = p.position; Cesium.Cartesian3.normalize(position, gravityVector); Cesium.Cartesian3.multiplyByScalar(gravityVector, gravity * dt, gravityVector); p.velocity = Cesium.Cartesian3.add(p.velocity, gravityVector, p.velocity);&#125;//该函数计算重力矢量，并使用重力加速度来改变粒子的速度。 将重力设置为粒子系统的updateFunction：updateCallback : applyGravity Positioning 定位123456789101112131415161718192021222324252627282930313233343536373839404142434445/*modelMatrix：将粒子系统从模型转换为世界坐标。emitterModelMatrix：在粒子系统的局部坐标系中变换粒子系统发射器。*/var entity = viewer.entities.add(&#123; model : &#123; uri : '../../SampleData/models/CesiumMilkTruck/CesiumMilkTruck-kmc.glb' &#125;, position : Cesium.Cartesian3.fromDegrees(-75.15787310614596, 39.97862668312678)&#125;);viewer.trackedEntity = entity;modelMatrix: entity.computeModelMatrix(time, new Cesium.Matrix4())function computeEmitterModelMatrix() &#123; hpr = Cesium.HeadingPitchRoll.fromDegrees(0.0, 0.0, 0.0, hpr); trs.translation = Cesium.Cartesian3.fromElements(-4.0, 0.0, 1.4, translation); trs.rotation = Cesium.Quaternion.fromHeadingPitchRoll(hpr, rotation); return Cesium.Matrix4.fromTranslationRotationScale(trs, emitterModelMatrix);&#125;var particleSystem = viewer.scene.primitives.add(new Cesium.ParticleSystem(&#123; image : '../../SampleData/smoke.png', startColor : Cesium.Color.LIGHTSEAGREEN.withAlpha(0.7), endColor : Cesium.Color.WHITE.withAlpha(0.0), startScale : 1.0, endScale : 4.0, particleLife : 1.0, minimumSpeed : 1.0, maximumSpeed : 4.0 imageSize : new Cesium.Cartesian2(25, 25), emissionRate : 5.0, lifetime : 16.0, modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()) emitterModelMatrix : computeEmitterModelMatrix()&#125;)); 完整示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218var viewer = new Cesium.Viewer('cesiumContainer');//Set the random number seed for consistent results.Cesium.Math.setRandomNumberSeed(3);//Set bounds of our simulation timevar start = Cesium.JulianDate.fromDate(new Date(2015, 2, 25, 16));var stop = Cesium.JulianDate.addSeconds(start, 120, new Cesium.JulianDate());//Make sure viewer is at the desired time.viewer.clock.startTime = start.clone();viewer.clock.stopTime = stop.clone();viewer.clock.currentTime = start.clone();viewer.clock.clockRange = Cesium.ClockRange.LOOP_STOP; //Loop at the endviewer.clock.multiplier = 1;viewer.clock.shouldAnimate = true;//Set timeline to simulation boundsviewer.timeline.zoomTo(start, stop);var viewModel = &#123; emissionRate : 5.0, gravity : 0.0, minimumParticleLife : 1.2, maximumParticleLife : 1.2, minimumSpeed : 1.0, maximumSpeed : 4.0, startScale : 1.0, endScale : 5.0, particleSize : 25.0&#125;;Cesium.knockout.track(viewModel);var toolbar = document.getElementById('toolbar');Cesium.knockout.applyBindings(viewModel, toolbar);var entityPosition = new Cesium.Cartesian3();var entityOrientation = new Cesium.Quaternion();var rotationMatrix = new Cesium.Matrix3();var modelMatrix = new Cesium.Matrix4();function computeModelMatrix(entity, time) &#123; return entity.computeModelMatrix(time, new Cesium.Matrix4());&#125;var emitterModelMatrix = new Cesium.Matrix4();var translation = new Cesium.Cartesian3();var rotation = new Cesium.Quaternion();var hpr = new Cesium.HeadingPitchRoll();var trs = new Cesium.TranslationRotationScale();function computeEmitterModelMatrix() &#123; hpr = Cesium.HeadingPitchRoll.fromDegrees(0.0, 0.0, 0.0, hpr); trs.translation = Cesium.Cartesian3.fromElements(-4.0, 0.0, 1.4, translation); trs.rotation = Cesium.Quaternion.fromHeadingPitchRoll(hpr, rotation); return Cesium.Matrix4.fromTranslationRotationScale(trs, emitterModelMatrix);&#125;var pos1 = Cesium.Cartesian3.fromDegrees(-75.15787310614596, 39.97862668312678);var pos2 = Cesium.Cartesian3.fromDegrees(-75.1633691390455, 39.95355089912078);var position = new Cesium.SampledPositionProperty();position.addSample(start, pos1);position.addSample(stop, pos2);var entity = viewer.entities.add(&#123; availability : new Cesium.TimeIntervalCollection([new Cesium.TimeInterval(&#123; start : start, stop : stop &#125;)]), model : &#123; uri : '../SampleData/models/CesiumMilkTruck/CesiumMilkTruck-kmc.glb', minimumPixelSize : 64 &#125;, viewFrom: new Cesium.Cartesian3(-100.0, 0.0, 100.0), position : position, orientation : new Cesium.VelocityOrientationProperty(position)&#125;);viewer.trackedEntity = entity;var scene = viewer.scene;var particleSystem = scene.primitives.add(new Cesium.ParticleSystem(&#123; image : '../SampleData/smoke.png', startColor : Cesium.Color.LIGHTSEAGREEN.withAlpha(0.7), endColor : Cesium.Color.WHITE.withAlpha(0.0), startScale : viewModel.startScale, endScale : viewModel.endScale, minimumParticleLife : viewModel.minimumParticleLife, maximumParticleLife : viewModel.maximumParticleLife, minimumSpeed : viewModel.minimumSpeed, maximumSpeed : viewModel.maximumSpeed, imageSize : new Cesium.Cartesian2(viewModel.particleSize, viewModel.particleSize), emissionRate : viewModel.emissionRate, bursts : [ // these burst will occasionally sync to create a multicolored effect new Cesium.ParticleBurst(&#123;time : 5.0, minimum : 10, maximum : 100&#125;), new Cesium.ParticleBurst(&#123;time : 10.0, minimum : 50, maximum : 100&#125;), new Cesium.ParticleBurst(&#123;time : 15.0, minimum : 200, maximum : 300&#125;) ], lifetime : 16.0, emitter : new Cesium.CircleEmitter(2.0), emitterModelMatrix : computeEmitterModelMatrix(), updateCallback : applyGravity&#125;));var gravityScratch = new Cesium.Cartesian3();function applyGravity(p, dt) &#123; // We need to compute a local up vector for each particle in geocentric space. var position = p.position; Cesium.Cartesian3.normalize(position, gravityScratch); Cesium.Cartesian3.multiplyByScalar(gravityScratch, viewModel.gravity * dt, gravityScratch); p.velocity = Cesium.Cartesian3.add(p.velocity, gravityScratch, p.velocity);&#125;viewer.scene.preUpdate.addEventListener(function(scene, time) &#123; particleSystem.modelMatrix = computeModelMatrix(entity, time); // Account for any changes to the emitter model matrix. particleSystem.emitterModelMatrix = computeEmitterModelMatrix(); // Spin the emitter if enabled. if (viewModel.spin) &#123; viewModel.heading += 1.0; viewModel.pitch += 1.0; viewModel.roll += 1.0; &#125;&#125;);Cesium.knockout.getObservable(viewModel, 'emissionRate').subscribe( function(newValue) &#123; particleSystem.emissionRate = parseFloat(newValue); &#125;);Cesium.knockout.getObservable(viewModel, 'particleSize').subscribe( function(newValue) &#123; var particleSize = parseFloat(newValue); particleSystem.minimumImageSize.x = particleSize; particleSystem.minimumImageSize.y = particleSize; particleSystem.maximumImageSize.x = particleSize; particleSystem.maximumImageSize.y = particleSize; &#125;);Cesium.knockout.getObservable(viewModel, 'minimumParticleLife').subscribe( function(newValue) &#123; particleSystem.minimumParticleLife = parseFloat(newValue); &#125;);Cesium.knockout.getObservable(viewModel, 'maximumParticleLife').subscribe( function(newValue) &#123; particleSystem.maximumParticleLife = parseFloat(newValue); &#125;);Cesium.knockout.getObservable(viewModel, 'minimumSpeed').subscribe( function(newValue) &#123; particleSystem.minimumSpeed = parseFloat(newValue); &#125;);Cesium.knockout.getObservable(viewModel, 'maximumSpeed').subscribe( function(newValue) &#123; particleSystem.maximumSpeed = parseFloat(newValue); &#125;);Cesium.knockout.getObservable(viewModel, 'startScale').subscribe( function(newValue) &#123; particleSystem.startScale = parseFloat(newValue); &#125;);Cesium.knockout.getObservable(viewModel, 'endScale').subscribe( function(newValue) &#123; particleSystem.endScale = parseFloat(newValue); &#125;);var options = [&#123; text : 'Circle Emitter', onselect : function() &#123; particleSystem.emitter = new Cesium.CircleEmitter(2.0); &#125;&#125;, &#123; text : 'Sphere Emitter', onselect : function() &#123; particleSystem.emitter = new Cesium.SphereEmitter(2.5); &#125;&#125;, &#123; text : 'Cone Emitter', onselect : function() &#123; particleSystem.emitter = new Cesium.ConeEmitter(Cesium.Math.toRadians(45.0)); &#125;&#125;, &#123; text : 'Box Emitter', onselect : function() &#123; particleSystem.emitter = new Cesium.BoxEmitter(new Cesium.Cartesian3(10.0, 10.0, 10.0)); &#125;&#125;];Sandcastle.addToolbarMenu(options); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&lt;style&gt; @import url(../templates/bucket.css); #toolbar &#123; background: rgba(42, 42, 42, 0.8); padding: 4px; border-radius: 4px; &#125; #toolbar input &#123; vertical-align: middle; padding-top: 2px; padding-bottom: 2px; &#125; #toolbar .header &#123; font-weight: bold; &#125;&lt;/style&gt;&lt;div id="cesiumContainer" class="fullSize"&gt;&lt;/div&gt;&lt;div id="loadingOverlay"&gt;&lt;h1&gt;Loading...&lt;/h1&gt;&lt;/div&gt;&lt;div id="toolbar"&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Rate&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="0.0" max="100.0" step="1" data-bind="value: emissionRate, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: emissionRate"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Size&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="2" max="60.0" step="1" data-bind="value: particleSize, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: particleSize"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Min Life&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="0.1" max="30.0" step="1" data-bind="value: minimumParticleLife, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: minimumParticleLife"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Max Life&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="0.1" max="30.0" step="1" data-bind="value: maximumParticleLife, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: maximumParticleLife"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Min Speed&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="0.0" max="30.0" step="1" data-bind="value: minimumSpeed, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: minimumSpeed"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Max Speed&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="0.0" max="30.0" step="1" data-bind="value: maximumSpeed, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: maximumSpeed"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Start Scale&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="0.0" max="10.0" step="1" data-bind="value: startScale, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: startScale"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;End Scale&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="0.0" max="10.0" step="1" data-bind="value: endScale, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: endScale"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gravity&lt;/td&gt; &lt;td&gt; &lt;input type="range" min="-20.0" max="20.0" step="1" data-bind="value: gravity, valueUpdate: 'input'"&gt; &lt;input type="text" size="5" data-bind="value: gravity"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/div&gt;]]></content>
      <categories>
        <category>cesium</category>
        <category>js</category>
      </categories>
      <tags>
        <tag>cesium</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cesium(笔记-3)]]></title>
    <url>%2F2019%2F08%2F19%2Fcesium(%E7%AC%94%E8%AE%B0-3)%2F</url>
    <content type="text"><![CDATA[运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro Cesium支持流式的、可视化的全球高程投影地形地势、水形数据，包括海洋、湖泊、河流、山峰、峡谷和其他能够被三维展示出来的且效果比二维好的地形数据。像图层数据一样，Cesium引擎会从一个服务器上请求流式地形数据，仅请求那些基于当前相机能看到的需要绘制的图层上的数据。12345678// Load Cesium World Terrainviewer.terrainProvider = Cesium.createWorldTerrain(&#123; requestWaterMask : true, // required for water effects requestVertexNormals : true // required for terrain lighting&#125;);// Enable depth testing so things behind the terrain disappear.viewer.scene.globe.depthTestAgainstTerrain = true; 高投影的arctic terrain12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// High-resolution arctic terrain from the Arctic DEM project (Release 4), tiled and hosted by Cesium ion.// https://www.pgc.umn.edu/data/arcticdem/var viewer = new Cesium.Viewer('cesiumContainer', &#123; terrainProvider: new Cesium.CesiumTerrainProvider(&#123; url: Cesium.IonResource.fromAssetId(3956) &#125;)&#125;);// Add Alaskan locationsSandcastle.addDefaultToolbarMenu([&#123; text: 'Denali', onselect: function() &#123; viewer.scene.camera.flyTo(&#123; destination: Cesium.Cartesian3.fromRadians(-2.6399828792482234, 1.0993550795541742, 5795), orientation: &#123; heading: 3.8455, pitch: -0.4535, roll: 0.0 &#125; &#125;); &#125;&#125;, &#123; text: 'Anchorage Area', onselect: function() &#123; viewer.scene.camera.flyTo(&#123; destination: Cesium.Cartesian3.fromRadians(-2.610708034601548, 1.0671172431736584, 1900), orientation: &#123; heading: 4.6, pitch: -0.341, roll: 0.0 &#125; &#125;); &#125;&#125;, &#123; text: 'Peaks', onselect: function() &#123; viewer.scene.camera.flyTo(&#123; destination: Cesium.Cartesian3.fromRadians(-2.6928866820212813, 1.072394255273859, 3700), orientation: &#123; heading: 1.6308222948889464, pitch: -0.6473480165020193, roll: 0.0 &#125; &#125;); &#125;&#125;, &#123; text: 'Riverbed', onselect: function() &#123; viewer.scene.camera.flyTo(&#123; destination: Cesium.Cartesian3.fromRadians(-2.6395623497608596, 1.0976443174490356, 2070), orientation: &#123; heading: 6.068794108659519, pitch: -0.08514161789475816, roll: 0.0 &#125; &#125;); &#125;&#125;], 'toolbar'); 高投影的Pennsylvania terrain12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// High resolution terrain of Pennsylvania curated by Pennsylvania Spatial Data Access (PASDA)// http://www.pasda.psu.edu/var viewer = new Cesium.Viewer('cesiumContainer', &#123; terrainProvider: new Cesium.CesiumTerrainProvider(&#123; url: Cesium.IonResource.fromAssetId(3957) &#125;)&#125;);// Add PA locationsSandcastle.addDefaultToolbarMenu([&#123; text : 'Pinnacle', onselect : function() &#123; viewer.scene.camera.flyTo(&#123; destination : Cesium.Cartesian3.fromRadians(-1.3324415110874286, 0.6954224325279967, 236.6770689945084), orientation : &#123; heading : Cesium.Math.toRadians(310), pitch : Cesium.Math.toRadians(-15), roll : 0.0 &#125; &#125;); &#125;&#125;, &#123; text : 'Mount Nittany', onselect : function() &#123; viewer.scene.camera.flyTo(&#123; destination : Cesium.Cartesian3.fromRadians(-1.358985133937573, 0.7123252393978314, 451.05748252867375), orientation : &#123; heading : Cesium.Math.toRadians(85), pitch : Cesium.Math.toRadians(0), roll : 0.0 &#125; &#125;); &#125;&#125;, &#123; text : 'Horseshoe Curve', onselect : function() &#123; viewer.scene.camera.flyTo(&#123; destination : Cesium.Cartesian3.fromRadians(-1.3700147546199826, 0.706808606166025, 993.7916313325215), orientation : &#123; heading : Cesium.Math.toRadians(90), pitch : Cesium.Math.toRadians(-15), roll : 0.0 &#125; &#125;); &#125;&#125;, &#123; text : 'Jim Thorpe', onselect : function() &#123; viewer.scene.camera.flyTo(&#123; destination : Cesium.Cartesian3.fromRadians(-1.3218297501066052, 0.713358272291525, 240.87968743408845), orientation : &#123; heading : Cesium.Math.toRadians(200), pitch : Cesium.Math.toRadians(-5), roll : 0.0 &#125; &#125;); &#125;&#125;, &#123; text : 'Grand Canyon of PA', onselect : function() &#123; viewer.scene.camera.flyTo(&#123; destination : Cesium.Cartesian3.fromRadians(-1.349379633251472, 0.720297672225785, 656.268309953562), orientation : &#123; heading : Cesium.Math.toRadians(200), pitch : Cesium.Math.toRadians(-5), roll : 0.0 &#125; &#125;); &#125;&#125;], 'toolbar'); 一些地形数据配置和格式]]></content>
      <categories>
        <category>cesium</category>
        <category>js</category>
      </categories>
      <tags>
        <tag>cesium</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cesium(笔记-2)]]></title>
    <url>%2F2019%2F08%2F18%2Fcesium(%E7%AC%94%E8%AE%B0-2)%2F</url>
    <content type="text"><![CDATA[运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro Cesium应用程序另一个关键元素是Imagery(图层)。瓦片图集合根据不同的投影方式映射到虚拟的三维数字地球表面。依赖于相机指向地表的方向和距离，Cesium会去请求和渲染不同层级的图层详细信息。多种图层能够被添加、移除、排序和适应到Cesium中。Cesium提供了一系列方法用于处理图层，比如颜色自适应，图层叠加融合 支持的图层格式：1.wms2.TMS3.WMTS (with time dynamic imagery)4.ArcGIS5.Bing Maps6.Google Earth7.Mapbox8.OpenStreetMap 基本图层123456789101112131415161718var viewer = new Cesium.Viewer('cesiumContainer', &#123; imageryProvider : Cesium.createWorldImagery(&#123; style : Cesium.IonWorldImageryStyle.AERIAL_WITH_LABELS &#125;), baseLayerPicker : false&#125;);var layers = viewer.scene.imageryLayers;var blackMarble = layers.addImageryProvider(new Cesium.IonImageryProvider(&#123; assetId: 3812 &#125;));blackMarble.alpha = 0.5;blackMarble.brightness = 2.0;layers.addImageryProvider(new Cesium.SingleTileImageryProvider(&#123; url : '../images/Cesium_Logo_overlay.png', rectangle : Cesium.Rectangle.fromDegrees(-75.0, 28.0, -67.0, 29.75)&#125;)); 自适应图层1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950var viewer = new Cesium.Viewer(&apos;cesiumContainer&apos;);var imageryLayers = viewer.imageryLayers;// The viewModel tracks the state of our mini application.var viewModel = &#123; brightness: 0, contrast: 0, hue: 0, saturation: 0, gamma: 0&#125;;// Convert the viewModel members into knockout observables.Cesium.knockout.track(viewModel);// Bind the viewModel to the DOM elements of the UI that call for it.var toolbar = document.getElementById(&apos;toolbar&apos;);Cesium.knockout.applyBindings(viewModel, toolbar);// Make the active imagery layer a subscriber of the viewModel.function subscribeLayerParameter(name) &#123; Cesium.knockout.getObservable(viewModel, name).subscribe( function(newValue) &#123; if (imageryLayers.length &gt; 0) &#123; var layer = imageryLayers.get(0); layer[name] = newValue; &#125; &#125; );&#125;subscribeLayerParameter(&apos;brightness&apos;);subscribeLayerParameter(&apos;contrast&apos;);subscribeLayerParameter(&apos;hue&apos;);subscribeLayerParameter(&apos;saturation&apos;);subscribeLayerParameter(&apos;gamma&apos;);// Make the viewModel react to base layer changes.function updateViewModel() &#123; if (imageryLayers.length &gt; 0) &#123; var layer = imageryLayers.get(0); viewModel.brightness = layer.brightness; viewModel.contrast = layer.contrast; viewModel.hue = layer.hue; viewModel.saturation = layer.saturation; viewModel.gamma = layer.gamma; &#125;&#125;imageryLayers.layerAdded.addEventListener(updateViewModel);imageryLayers.layerRemoved.addEventListener(updateViewModel);imageryLayers.layerMoved.addEventListener(updateViewModel);updateViewModel(); 控制调整图层顺序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188var viewer = new Cesium.Viewer('cesiumContainer', &#123; baseLayerPicker : false&#125;);var imageryLayers = viewer.imageryLayers;var viewModel = &#123; layers : [], baseLayers : [], upLayer : null, downLayer : null, selectedLayer : null, isSelectableLayer : function(layer) &#123; return this.baseLayers.indexOf(layer) &gt;= 0; &#125;, raise : function(layer, index) &#123; imageryLayers.raise(layer); viewModel.upLayer = layer; viewModel.downLayer = viewModel.layers[Math.max(0, index - 1)]; updateLayerList(); window.setTimeout(function() &#123; viewModel.upLayer = viewModel.downLayer = null; &#125;, 10); &#125;, lower : function(layer, index) &#123; imageryLayers.lower(layer); viewModel.upLayer = viewModel.layers[Math.min(viewModel.layers.length - 1, index + 1)]; viewModel.downLayer = layer; updateLayerList(); window.setTimeout(function() &#123; viewModel.upLayer = viewModel.downLayer = null; &#125;, 10); &#125;, canRaise : function(layerIndex) &#123; return layerIndex &gt; 0; &#125;, canLower : function(layerIndex) &#123; return layerIndex &gt;= 0 &amp;&amp; layerIndex &lt; imageryLayers.length - 1; &#125;&#125;;var baseLayers = viewModel.baseLayers;Cesium.knockout.track(viewModel);function setupLayers() &#123; // Create all the base layers that this example will support. // These base layers aren't really special. It's possible to have multiple of them // enabled at once, just like the other layers, but it doesn't make much sense because // all of these layers cover the entire globe and are opaque. addBaseLayerOption( 'Bing Maps Aerial', undefined); // the current base layer addBaseLayerOption( 'Bing Maps Road', new Cesium.BingMapsImageryProvider(&#123; url : 'https://dev.virtualearth.net', mapStyle : Cesium.BingMapsStyle.ROAD &#125;)); addBaseLayerOption( 'ArcGIS World Street Maps', new Cesium.ArcGisMapServerImageryProvider(&#123; url : 'https://services.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer' &#125;)); addBaseLayerOption( 'OpenStreetMaps', new Cesium.OpenStreetMapImageryProvider()); addBaseLayerOption( 'Stamen Maps', new Cesium.OpenStreetMapImageryProvider(&#123; url : 'https://stamen-tiles.a.ssl.fastly.net/watercolor/', fileExtension : 'jpg', credit : 'Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under CC BY SA.' &#125;)); addBaseLayerOption( 'Natural Earth II (local)', new Cesium.TileMapServiceImageryProvider(&#123; url : Cesium.buildModuleUrl('Assets/Textures/NaturalEarthII') &#125;)); addBaseLayerOption( 'USGS Shaded Relief (via WMTS)', new Cesium.WebMapTileServiceImageryProvider(&#123; url : 'http://basemap.nationalmap.gov/arcgis/rest/services/USGSShadedReliefOnly/MapServer/WMTS', layer : 'USGSShadedReliefOnly', style : 'default', format : 'image/jpeg', tileMatrixSetID : 'default028mm', maximumLevel : 19, credit : 'U. S. Geological Survey' &#125;)); // Create the additional layers addAdditionalLayerOption( 'United States GOES Infrared', new Cesium.WebMapServiceImageryProvider(&#123; url : 'https://mesonet.agron.iastate.edu/cgi-bin/wms/goes/conus_ir.cgi?', layers : 'goes_conus_ir', credit : 'Infrared data courtesy Iowa Environmental Mesonet', parameters : &#123; transparent : 'true', format : 'image/png' &#125; &#125;)); addAdditionalLayerOption( 'United States Weather Radar', new Cesium.WebMapServiceImageryProvider(&#123; url : 'https://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0r.cgi?', layers : 'nexrad-n0r', credit : 'Radar data courtesy Iowa Environmental Mesonet', parameters : &#123; transparent : 'true', format : 'image/png' &#125; &#125;)); addAdditionalLayerOption( 'TileMapService Image', new Cesium.TileMapServiceImageryProvider(&#123; url : '../images/cesium_maptiler/Cesium_Logo_Color' &#125;), 0.2); addAdditionalLayerOption( 'Single Image', new Cesium.SingleTileImageryProvider(&#123; url : '../images/Cesium_Logo_overlay.png', rectangle : Cesium.Rectangle.fromDegrees(-115.0, 38.0, -107, 39.75) &#125;), 1.0); addAdditionalLayerOption( 'Grid', new Cesium.GridImageryProvider(), 1.0, false); addAdditionalLayerOption( 'Tile Coordinates', new Cesium.TileCoordinatesImageryProvider(), 1.0, false);&#125;function addBaseLayerOption(name, imageryProvider) &#123; var layer; if (typeof imageryProvider === 'undefined') &#123; layer = imageryLayers.get(0); viewModel.selectedLayer = layer; &#125; else &#123; layer = new Cesium.ImageryLayer(imageryProvider); &#125; layer.name = name; baseLayers.push(layer);&#125;function addAdditionalLayerOption(name, imageryProvider, alpha, show) &#123; var layer = imageryLayers.addImageryProvider(imageryProvider); layer.alpha = Cesium.defaultValue(alpha, 0.5); layer.show = Cesium.defaultValue(show, true); layer.name = name; Cesium.knockout.track(layer, ['alpha', 'show', 'name']);&#125;function updateLayerList() &#123; var numLayers = imageryLayers.length; viewModel.layers.splice(0, viewModel.layers.length); for (var i = numLayers - 1; i &gt;= 0; --i) &#123; viewModel.layers.push(imageryLayers.get(i)); &#125;&#125;setupLayers();updateLayerList();//Bind the viewModel to the DOM elements of the UI that call for it.var toolbar = document.getElementById('toolbar');Cesium.knockout.applyBindings(viewModel, toolbar);Cesium.knockout.getObservable(viewModel, 'selectedLayer').subscribe(function(baseLayer) &#123; // Handle changes to the drop-down base layer selector. var activeLayerIndex = 0; var numLayers = viewModel.layers.length; for (var i = 0; i &lt; numLayers; ++i) &#123; if (viewModel.isSelectableLayer(viewModel.layers[i])) &#123; activeLayerIndex = i; break; &#125; &#125; var activeLayer = viewModel.layers[activeLayerIndex]; var show = activeLayer.show; var alpha = activeLayer.alpha; imageryLayers.remove(activeLayer, false); imageryLayers.add(baseLayer, numLayers - activeLayerIndex - 1); baseLayer.show = show; baseLayer.alpha = alpha; updateLayerList();&#125;); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;style&gt; @import url(../templates/bucket.css); #toolbar &#123; background: rgba(42, 42, 42, 0.8); padding: 4px; border-radius: 4px; &#125; #toolbar input &#123; vertical-align: middle; padding-top: 2px; padding-bottom: 2px; &#125; #toolbar table tr &#123; transform: translateY(0); transition: transform 0.4s ease-out; &#125; #toolbar table tr.up &#123; transform: translateY(33px); transition: none; &#125; #toolbar table tr.down &#123; transform: translateY(-33px); transition: none; &#125;&lt;/style&gt;&lt;div id="cesiumContainer" class="fullSize"&gt;&lt;/div&gt;&lt;div id="loadingOverlay"&gt;&lt;h1&gt;Loading...&lt;/h1&gt;&lt;/div&gt;&lt;div id="toolbar"&gt; &lt;table&gt; &lt;tbody data-bind="foreach: layers"&gt; &lt;tr data-bind="css: &#123; up: $parent.upLayer === $data, down: $parent.downLayer === $data &#125;"&gt; &lt;td&gt;&lt;input type="checkbox" data-bind="checked: show"&gt;&lt;/td&gt; &lt;td&gt; &lt;span data-bind="text: name, visible: !$parent.isSelectableLayer($data)"&gt;&lt;/span&gt; &lt;select data-bind="visible: $parent.isSelectableLayer($data), options: $parent.baseLayers, optionsText: 'name', value: $parent.selectedLayer"&gt;&lt;/select&gt; &lt;/td&gt; &lt;td&gt; &lt;input type="range" min="0" max="1" step="0.01" data-bind="value: alpha, valueUpdate: 'input'"&gt; &lt;/td&gt; &lt;td&gt; &lt;button type="button" class="cesium-button" data-bind="click: function() &#123; $parent.raise($data, $index()); &#125;, visible: $parent.canRaise($index())"&gt; ▲ &lt;/button&gt; &lt;/td&gt; &lt;td&gt; &lt;button type="button" class="cesium-button" data-bind="click: function() &#123; $parent.lower($data, $index()); &#125;, visible: $parent.canLower($index())"&gt; ▼ &lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/div&gt; 切割图层(卷帘)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647var viewer = new Cesium.Viewer(&apos;cesiumContainer&apos;, &#123; imageryProvider : new Cesium.ArcGisMapServerImageryProvider(&#123; url : &apos;https://services.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer&apos; &#125;), baseLayerPicker : false, infoBox : false&#125;);var layers = viewer.imageryLayers;var earthAtNight = layers.addImageryProvider(new Cesium.IonImageryProvider(&#123; assetId: 3812 &#125;));earthAtNight.splitDirection = Cesium.ImagerySplitDirection.LEFT; // Only show to the left of the slider.// Sync the position of the slider with the split positionvar slider = document.getElementById(&apos;slider&apos;);viewer.scene.imagerySplitPosition = (slider.offsetLeft) / slider.parentElement.offsetWidth;var handler = new Cesium.ScreenSpaceEventHandler(slider);var moveActive = false;function move(movement) &#123; if(!moveActive) &#123; return; &#125; var relativeOffset = movement.endPosition.x ; var splitPosition = (slider.offsetLeft + relativeOffset) / slider.parentElement.offsetWidth; slider.style.left = 100.0 * splitPosition + &apos;%&apos;; viewer.scene.imagerySplitPosition = splitPosition;&#125;handler.setInputAction(function() &#123; moveActive = true;&#125;, Cesium.ScreenSpaceEventType.LEFT_DOWN);handler.setInputAction(function() &#123; moveActive = true;&#125;, Cesium.ScreenSpaceEventType.PINCH_START);handler.setInputAction(move, Cesium.ScreenSpaceEventType.MOUSE_MOVE);handler.setInputAction(move, Cesium.ScreenSpaceEventType.PINCH_MOVE);handler.setInputAction(function() &#123; moveActive = false;&#125;, Cesium.ScreenSpaceEventType.LEFT_UP);handler.setInputAction(function() &#123; moveActive = false;&#125;, Cesium.ScreenSpaceEventType.PINCH_END);]]></content>
      <categories>
        <category>cesium</category>
        <category>js</category>
      </categories>
      <tags>
        <tag>cesium</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cesium(笔记-1)]]></title>
    <url>%2F2019%2F08%2F17%2Fcesium(%E7%AC%94%E8%AE%B0-1)%2F</url>
    <content type="text"><![CDATA[运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro 隐藏界面中的元素123456789101112var viewer = new Cesium.Viewer('cesiumContainer',&#123; geocoder:false, //查找位置工具，查找到之后会将镜头对准找到的地址，默认使用bing地图 homeButton:false, //视角返回初始位置 sceneModePicker:false, //选择视角的模式，有三种：3D，2D，哥伦布视图（CV） baseLayerPicker:false, //图层选择器，选择要显示的地图服务和地形服务. navigationHelpButton:false, //导航帮助按钮，显示默认的地图控制帮助. animation:false, //动画器件，控制视图动画的播放速度 creditContainer:"credit", timeline:false, //时间线,指示当前时间，并允许用户跳到特定的时间 fullscreenButton:false, // vrButton:false, // &#125;); 通过css控制123456789 .cesium-viewer-toolbar, /* 右上角按钮组 */ .cesium-viewer-animationContainer, /* 左下角动画控件 */ .cesium-viewer-timelineContainer, /* 时间线 */ .cesium-viewer-bottom /* logo信息 */ &#123; display: none; &#125; .cesium-viewer-fullscreenContainer /* 全屏按钮 */&#123; position: absolute; top: -999em; &#125; 显示帧速(FPS)1viewer.scene.debugShowFramesPerSecond = true;]]></content>
      <categories>
        <category>cesium</category>
        <category>js</category>
      </categories>
      <tags>
        <tag>cesium</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenStack部署(queens版)]]></title>
    <url>%2F2019%2F06%2F28%2FOpenStack%E9%83%A8%E7%BD%B2queens%E7%89%88%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | OpenStack queens | VMware Workstation 15 Pro 主机名 备注 操作系统 controller 控制节点 CentOs7 compute 计算节点 CentOs7 cinder 块存储节点 CentOs7 关闭虚拟机防火墙及selinux12345systemctl disable firewalld.servicesystemctl stop firewalld.servicevim /etc/sysconfig/selinuxSELINUX=disable //将enforcing修改为disable，永久关闭setenforce 0 https://blog.51cto.com/13643643/2171262 https://blog.csdn.net/qq_40791253/article/details/83241299#Controller_20]]></content>
      <categories>
        <category>linux</category>
        <category>集群部署</category>
        <category>centos</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>集群部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kylin部署]]></title>
    <url>%2F2019%2F05%2F25%2Fkylin%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 |kylin-2.6.2-hbase1x Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 kylin 部署 下载 apache-kylin-2.6.2-bin-hbase1x.tar.gz 解压1tar -zxvf apache-kylin-2.6.2-bin-hbase1x.tar.gz -C /opt 添加环境变量123vi /etc/profileexport hive_dependency=/opt/hive-1.2.2/conf:/opt/hive-1.2.2/lib*:/opt/hive-1.2.2/hcatalog/share/hcatalog/hive-hcatalog-core-2.0.0.jar 启动报错一1Something wrong with Hive CLI or Beeline, please execute Hive CLI or Beeline CLI in terminal to find the root cause. 修改 kylin/bin/find-hive-dependency.sh 部分代码 123456789101112131415161718if [ "$&#123;client_mode&#125;" == "beeline" ]then #beeline_shell=`$KYLIN_HOME/bin/get-properties.sh kylin.source.hive.beeline-shell` #beeline_params=`bash $&#123;KYLIN_HOME&#125;/bin/get-properties.sh kylin.source.hive.beeline-params` #hive_env=`$&#123;beeline_shell&#125; $&#123;hive_conf_properties&#125; $&#123;beeline_params&#125; --outputformat=dsv -e "set;" 2&gt;&amp;1 | grep --text 'env:CLASSPATH' ` beeline_params=`sh $&#123;KYLIN_HOME&#125;/bin/get-properties.sh kylin.hive.beeline.params` hive_env=`beeline $&#123;beeline_params&#125; --outputformat=dsv -e set | grep 'env:CLASSPATH'`else source $&#123;dir&#125;/check-hive-usability.sh hive_env=`hive -e set | grep 'env:CLASSPATH'` #hive_env=`hive $&#123;hive_conf_properties&#125; -e set 2&gt;&amp;1 | grep 'env:CLASSPATH'` #hive -e set &gt;/tmp/hive_env.txt 2&gt;&amp;1 #hive_env=`grep 'env:CLASSPATH' /tmp/hive_env.txt` #hive_env=`echo $&#123;hive_env#*env:CLASSPATH&#125;` #hive_env="env:CLASSPATH"$&#123;hive_env&#125;fi 修改 kylin/conf/kylin.properties 1234kylin.rest.servers=192.168.83.128:7070,192.168.83.129:7070,192.168.83.130:7070kylin.rest.timezone=GMT+8kylin.hive.client=beelinekylin.hive.beeline.params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u 'jdbc:hive2://192.168.83.128:10000' 分别验证 bin/find-hive-dependency.sh Retrieving hive dependency…Connecting to jdbc:hive2://192.168.83.128:10000Connected to: Apache Hive (version 1.2.2)Driver: Hive JDBC (version 1.2.2)Transaction isolation: TRANSACTION_REPEATABLE_READ1,484 rows selected (0.89 seconds)Beeline version 1.2.2 by Apache HiveClosing: 0: jdbc:hive2://192.168.83.128:10000 1bin/find-hbase-dependency.sh Retrieving hbase dependency… 启动报错二12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364java.net.UnknownHostException: master:2181: 未知的名称或服务 at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) at java.net.InetAddress.getAllByName0(InetAddress.java:1276) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) at org.apache.zookeeper.client.StaticHostProvider.&lt;init&gt;(StaticHostProvider.java:61) at org.apache.zookeeper.ZooKeeper.&lt;init&gt;(ZooKeeper.java:445) at org.apache.curator.utils.DefaultZookeeperFactory.newZooKeeper(DefaultZookeeperFactory.java:29) at org.apache.curator.framework.imps.CuratorFrameworkImpl$2.newZooKeeper(CuratorFrameworkImpl.java:154) at org.apache.curator.HandleHolder$1.getZooKeeper(HandleHolder.java:94) at org.apache.curator.HandleHolder.getZooKeeper(HandleHolder.java:55) at org.apache.curator.ConnectionState.reset(ConnectionState.java:219) at org.apache.curator.ConnectionState.start(ConnectionState.java:103) at org.apache.curator.CuratorZookeeperClient.start(CuratorZookeeperClient.java:190) at org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:256) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock$Factory.getZKClient(ZookeeperDistributedLock.java:85) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock$Factory.&lt;init&gt;(ZookeeperDistributedLock.java:109) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock$Factory.&lt;init&gt;(ZookeeperDistributedLock.java:105) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.kylin.common.util.ClassUtil.newInstance(ClassUtil.java:88) at org.apache.kylin.common.KylinConfigBase.getDistributedLockFactory(KylinConfigBase.java:458) at org.apache.kylin.storage.hbase.HBaseConnection.createHTableIfNeeded(HBaseConnection.java:327) at org.apache.kylin.storage.hbase.HBaseResourceStore.createHTableIfNeeded(HBaseResourceStore.java:114) at org.apache.kylin.storage.hbase.HBaseResourceStore.&lt;init&gt;(HBaseResourceStore.java:88) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.kylin.common.persistence.ResourceStore.createResourceStore(ResourceStore.java:92) at org.apache.kylin.common.persistence.ResourceStore.getStore(ResourceStore.java:111) at org.apache.kylin.rest.service.AclTableMigrationTool.checkIfNeedMigrate(AclTableMigrationTool.java:99) at org.apache.kylin.tool.AclTableMigrationCLI.main(AclTableMigrationCLI.java:43)2019-05-20 20:11:14,464 DEBUG [main] util.ZookeeperDistributedLock:147 : 27720@master trying to lock /kylin/kylin_metadata/create_htable/kylin_metadata/lock2019-05-20 20:11:29,653 ERROR [main] curator.ConnectionState:201 : Connection timed out for connection string (master:2181:2181,slave0:2181:2181,slave1:2181:2181) and timeout (15000) / elapsed (21695)org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:198) at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88) at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115) at org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:474) at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688) at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672) at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668) at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453) at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443) at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock.lock(ZookeeperDistributedLock.java:150) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock.lock(ZookeeperDistributedLock.java:171) at org.apache.kylin.storage.hbase.HBaseConnection.createHTableIfNeeded(HBaseConnection.java:328) at org.apache.kylin.storage.hbase.HBaseResourceStore.createHTableIfNeeded(HBaseResourceStore.java:114) at org.apache.kylin.storage.hbase.HBaseResourceStore.&lt;init&gt;(HBaseResourceStore.java:88) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.kylin.common.persistence.ResourceStore.createResourceStore(ResourceStore.java:92) at org.apache.kylin.common.persistence.ResourceStore.getStore(ResourceStore.java:111) at org.apache.kylin.rest.service.AclTableMigrationTool.checkIfNeedMigrate(AclTableMigrationTool.java:99) at org.apache.kylin.tool.AclTableMigrationCLI.main(AclTableMigrationCLI.java:43) hbase-site.xml文件```12345678910111213141516171819202122232425262728293031323334353637```xml&lt;configuration&gt; &lt;!-- 指定hbase在HDFS上存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slave0,slave1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.support.append&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;source&gt;2181默认值，来源hbase-default.xml&lt;/source&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/root/tmp/hbase/zookeeper&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; http://192.168.83.128:7070/kylin 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748492019-05-20 22:39:35,530 INFO [main] util.ZookeeperDistributedLock:238 : 70746@master released lock at /kylin/kylin_metadata/create_htable/kylin_metadata/lockException in thread "main" java.lang.IllegalArgumentException: Failed to find metadata store by url: kylin_metadata@hbase at org.apache.kylin.common.persistence.ResourceStore.createResourceStore(ResourceStore.java:99) at org.apache.kylin.common.persistence.ResourceStore.getStore(ResourceStore.java:111) at org.apache.kylin.rest.service.AclTableMigrationTool.checkIfNeedMigrate(AclTableMigrationTool.java:99) at org.apache.kylin.tool.AclTableMigrationCLI.main(AclTableMigrationCLI.java:43)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.kylin.common.persistence.ResourceStore.createResourceStore(ResourceStore.java:92) ... 3 moreCaused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=1, exceptions:Mon May 20 22:39:35 CST 2019, RpcRetryingCaller&#123;globalStartTime=1558363169520, pause=100, retries=1&#125;, java.io.IOException: Call to master/192.168.83.128:16000 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=7, waitTime=5001, operationTimeout=5000 expired. at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:157) at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:4297) at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:4289) at org.apache.hadoop.hbase.client.HBaseAdmin.createTableAsyncV2(HBaseAdmin.java:753) at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:674) at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:607) at org.apache.kylin.storage.hbase.HBaseConnection.createHTableIfNeeded(HBaseConnection.java:347) at org.apache.kylin.storage.hbase.HBaseResourceStore.createHTableIfNeeded(HBaseResourceStore.java:114) at org.apache.kylin.storage.hbase.HBaseResourceStore.&lt;init&gt;(HBaseResourceStore.java:88) ... 8 moreCaused by: java.io.IOException: Call to master/192.168.83.128:16000 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=7, waitTime=5001, operationTimeout=5000 expired. at org.apache.hadoop.hbase.ipc.AbstractRpcClient.wrapException(AbstractRpcClient.java:292) at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1276) at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:227) at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:336) at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.createTable(MasterProtos.java:58551) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$4.createTable(ConnectionManager.java:1864) at org.apache.hadoop.hbase.client.HBaseAdmin$5.call(HBaseAdmin.java:762) at org.apache.hadoop.hbase.client.HBaseAdmin$5.call(HBaseAdmin.java:754) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136) ... 16 moreCaused by: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=7, waitTime=5001, operationTimeout=5000 expired. at org.apache.hadoop.hbase.ipc.Call.checkAndSetTimeout(Call.java:73) at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1250) ... 23 more2019-05-20 22:39:35,682 INFO [close-hbase-conn] hbase.HBaseConnection:136 : Closing HBase connections...2019-05-20 22:39:35,683 INFO [close-hbase-conn] client.ConnectionManager$HConnectionImplementation:2155 : Closing master protocol: MasterService2019-05-20 22:39:35,684 INFO [close-hbase-conn] client.ConnectionManager$HConnectionImplementation:1726 : Closing zookeeper sessionid=0x36ad53f634300072019-05-20 22:39:35,749 INFO [main-EventThread] zookeeper.ClientCnxn:512 : EventThread shut down2019-05-20 22:39:35,749 INFO [close-hbase-conn] zookeeper.ZooKeeper:684 : Session: 0x36ad53f63430007 closed2019-05-20 22:39:36,113 INFO [Thread-6] zookeeper.ZooKeeper:684 : Session: 0x36ad53f63430008 closed2019-05-20 22:39:36,114 INFO [main-EventThread] zookeeper.ClientCnxn:512 : EventThread shut downERROR: Unknown error. Please check full log. 提取出重要的内容 123Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hbase.TableExistsException): kylin_metadata Caused by: org.apache.hadoop.hbase.TableExistsException: kylin_metadata 进入 zookeeper zkCli.sh 12345/opt/zookeeper-3.4.6/bin/zkCli.sh ls /hbase/tablermr /hbase/table/kylin_metadata重启hbase 登陆 http://host:7070/kylin 没有反应注释 kylin/tomcat/conf/server.xml1234&lt;Connector port="7443" protocol="org.apache.coyote.http11.Http11Protocol" maxThreads="150" SSLEnabled="true" scheme="https" secure="true" keystoreFile="conf/.keystore" keystorePass="changeit" clientAuth="false" sslProtocol="TLS" /&gt; 补充：12hive 启动时无法访问spark-assembly-*.jar的解决办法将加载原来的lib/spark-assembly-*.jar`替换成jars/*.jar 部分代码如下1234567# add Spark assembly jar to the classpathif [[ -n &quot;$SPARK_HOME&quot; ]]then #sparkAssemblyPath=`ls $&#123;SPARK_HOME&#125;/lib/spark-assembly-*.jar` sparkAssemblyPath=`ls $&#123;SPARK_HOME&#125;/jars/*.jar` CLASSPATH=&quot;$&#123;CLASSPATH&#125;:$&#123;sparkAssemblyPath&#125;&quot;fi]]></content>
      <categories>
        <category>linux</category>
        <category>kylin</category>
        <category>centos</category>
        <category>部署</category>
      </categories>
      <tags>
        <tag>kylin</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka生产者启动报错及kafka常用命令]]></title>
    <url>%2F2019%2F05%2F02%2Fkafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99%E5%8F%8Akafka%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 6.6 | kafka_2.11-1.1.1 kafka启动生产者后报错WARN [Producer clientId=console-producer] Got error produce response with correlation id 3 on topic-partition test-2, retrying (2 attempts left). Error: NETWORK_EXCEPTION (org.apache.kafka.clients.producer.internals.Sender) 运行生产者命令后报错123./kafka-console-producer.sh --broker-list 192.168.83.128:9092 --topic test&gt;[2019-04-29 10:37:39,906] WARN [Producer clientId=console-producer] Got error produce response with correlation id 3 on topic-partition test-2, retrying (2 attempts left). Error: NETWORK_EXCEPTION (org.apache.kafka.clients.producer.internals.Sender) 运行消费者报错123./kafka-console-consumer.sh -zookeeper 192.168.83.129:2181 --from-beginning --topic testUsing the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. 查看日志12345678910111213141516171819202122[2019-05-02 16:10:25,182] ERROR Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)kafka.common.InconsistentBrokerIdException: Configured broker.id 2 doesn't match stored broker.id 0 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs). at kafka.server.KafkaServer.getBrokerIdAndOfflineDirs(KafkaServer.scala:673) at kafka.server.KafkaServer.startup(KafkaServer.scala:209) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38) at kafka.Kafka$.main(Kafka.scala:75) at kafka.Kafka.main(Kafka.scala)[2019-05-02 16:10:25,185] INFO shutting down (kafka.server.KafkaServer)[2019-05-02 16:10:25,209] WARN (kafka.utils.CoreUtils$)java.lang.NullPointerException at kafka.server.KafkaServer$$anonfun$shutdown$5.apply$mcV$sp(KafkaServer.scala:572) at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:85) at kafka.server.KafkaServer.shutdown(KafkaServer.scala:572) at kafka.server.KafkaServer.startup(KafkaServer.scala:329) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38) at kafka.Kafka$.main(Kafka.scala:75) at kafka.Kafka.main(Kafka.scala)[2019-05-02 16:10:25,211] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)[2019-05-02 16:10:25,244] INFO Session: 0x26a778905c40001 closed (org.apache.zookeeper.ZooKeeper)[2019-05-02 16:10:25,246] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)[2019-05-02 16:10:25,250] INFO shut down completed (kafka.server.KafkaServer)[2019-05-02 16:10:25,250] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable) 原因： 所有配置 broker.id=0修改 kafka-logs-data/meta.properties修改后重启正常生产和消费 常用命令生产者操作：./kafka-console-producer.sh --broker-list master:9092 --topic test 消费者操作：./kafka-console-consumer.sh --bootstrap-server master:9092 --topic test --from-beginning 创建topic./kafka-topics.sh --create --zookeeper master:2181 --replication-factor 2 --partitions 3 --topic test 查看topic列表./kafka-topics.sh --list --zookeeper master:2181 如果需要查看topic的详细信息，需要使用describe命令./kafka-topics.sh --describe --zookeeper node1:2181 --topic test-topic 若不指定topic，则查看所有topic的信息./kafka-topics.sh --describe --zookeeper node1:2181 删除topic./kafka-topics.sh --delete --zookeeper master:2181 --topic test]]></content>
      <categories>
        <category>centos</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka压力、数据丢失测试]]></title>
    <url>%2F2019%2F04%2F20%2Fkafka%E5%8E%8B%E5%8A%9B%E4%B8%8E%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | kafka_2.11-1.1.1 | opt/zookeeper-3.4.6 | flume-1.8.0 | nginx-openresty/1.13.6.2 本文主要通过Nginx做日志服务器 以POST请求生成日志（每次请求约为275字节，JSON格式），利用lua脚本变为相应格式，flume指定Source为TailDirSouce、kafka Channel、配置ACK应答机制a1.channels.c1.kafka.producer.acks = -1失败重试次数a1.channels.c1.kafka.producer.retries = 3;kafka主题设定三个分区两个副本 主机名 备注 操作系统 运行服务 配置 master 控制节点 CentOs7 kafka、zookeeper、nginx、flume 1C 2GM slave0 计算节点 CentOs7 kafka、zookeeper 1C 1.5GM slave1 块存储节点 CentOs7 kafka、zookeeper 1C 1.5GM 注：C-核 M-内存 启动nginx1/usr/local/openresty/nginx/sbin/nginx 启动zookeeper1/opt/zookeeper-3.4.6/bin/zkServer.sh start 启动flume1bin/flume-ng agent -n a1 -c conf/ -f myconf/nginx-kafka-kill.conf -Dflume.root.logger=INFO,consoledate 启动kafka1/opt/kafka_2.11-1.1.1/bin/kafka-server-start.sh -daemon /opt/kafka_2.11-1.1.1/config/server.properties 以下为 IDEA 程序输出log数 与 Nginx采集到的log数、各个节点kafka消费到的log数对比 IDEA log Nginx log Master消费 slave0消费 slave1消费 1000 842 701 722 732 1500 1269 1041 1086 1111 2000 1727 1480 1493 1461 2500 2022 1665 1739 1655 3000 2315 1892 1983 2163 5000 4800 2924 2924 —- 期间杀掉master节点kafka进程，slave0节点停止消费（这里很疑惑）查看主题详情12345bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic testkillTopic:testkill PartitionCount:3 ReplicationFactor:2 Configs: Topic: testkill Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1 Topic: testkill Partition: 1 Leader: 2 Replicas: 1,2 Isr: 2,1 Topic: testkill Partition: 2 Leader: 2 Replicas: 2,0 Isr: 2 IDEA程序中设定5000次请求，请求完后，启动kafka进程恢复后查看节点详情12345bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic testkillTopic:testkill PartitionCount:3 ReplicationFactor:2 Configs: Topic: testkill Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1,0 Topic: testkill Partition: 1 Leader: 2 Replicas: 1,2 Isr: 2,1 Topic: testkill Partition: 2 Leader: 2 Replicas: 2,0 Isr: 2,0 kafka启动后，消费之前没有消费的数据,进本地磁盘各分区查找比对数据，均在 结论：初步认定kafka高可用，数据0丢失（杀掉master节点kafka进程,slave0节点停止消费这里有待验证），吞吐量2000条一下压力正常，随着时间推移数据量越大积压越大kafka offset 维护方式 zookeeper维护 使用zookeeper来维护offsetkafka 0.9 以前的版本是将offset 存储在zookeeper上的，kafka在传输数据时，数据消费成功就会修改偏移量，这样就可以保证数据不会丢失而导致传输出错；但是这也存在一个问题：那就是每次消费数据时都要将数据的offset写入一次，效率比较低，而且zookeeper与kafka的offset变化确认也需要走网络IO，这样就会给offset的维护带来不稳定性和低效。 kafka自己维护offset 使用broker来维护offsetkafka 0.9 以后，offset的使用了内部的roker来管理，这样仅仅只需要broker，而不要zookeeper来维护，都是将topic提交给__consumer_offsets函数来执行。 也可以将kafka偏移量存入redis中，利用redis事务性维护offset kafka消息的位置 含义 名称 earlieastLeaderOffsets 存储在broker上的leader节点的最早的消息偏移量 consumerOffsets 消费者消费的消息偏移量位置 情况一：正常情况下，消费的消息偏移量应该大于broker上存储的最早的消息偏移量，即 A &lt; B 情况二：如果A 依然小于 B，则仍可以正常消费 情况三：然而，当 A &gt; B 时，则说明还没有被消费的消息已经被清除 此种情况会抛出 kafka.common.OffsetOutOfRangeException 异常。 在没有外部系统清除kafka消息的情况下，协调设置broker的最大保留大小 log.retention.bytes 和 最大保留时间log.retention.hours 等，来配合消费者端的读取消息。可以通过读取和监控消费者消费的offsets，来保证消息不会被意外清除。]]></content>
      <categories>
        <category>linux</category>
        <category>集群</category>
        <category>测试</category>
        <category>kafka</category>
        <category>flume</category>
        <category>zookeeper</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>zookeeper</tag>
        <tag>测试</tag>
        <tag>nginx</tag>
        <tag>flume</tag>
        <tag>集群</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 启动报错(3)]]></title>
    <url>%2F2019%2F04%2F12%2Felasticsearch%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%993%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | elasticsearch 6.5.4 elasticsearch 启动报错org.elasticsearch.discovery.MasterNotDiscoveredException: null 1234567891011121314151617181920212223242526272829303132[2019-04-12T22:25:44,586][WARN ][r.suppressed ] [master] path: /_cluster/health, params: &#123;&#125;org.elasticsearch.discovery.MasterNotDiscoveredException: null at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:246) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:317) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:244) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:559) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:624) [elasticsearch-6.5.4.jar:6.5.4] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131][2019-04-12T22:25:44,594][DEBUG][o.e.a.a.c.h.TransportClusterHealthAction] [master] timed out while retrying [cluster:monitor/health] after failure (timeout [30s])[2019-04-12T22:25:44,594][WARN ][r.suppressed ] [master] path: /_cluster/health, params: &#123;&#125;org.elasticsearch.discovery.MasterNotDiscoveredException: null at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:246) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:317) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:244) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:559) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:624) [elasticsearch-6.5.4.jar:6.5.4] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131][2019-04-12T22:25:45,108][DEBUG][o.e.a.a.c.h.TransportClusterHealthAction] [master] timed out while retrying [cluster:monitor/health] after failure (timeout [30s])[2019-04-12T22:25:45,108][WARN ][r.suppressed ] [master] path: /_cluster/health, params: &#123;pretty=&#125;org.elasticsearch.discovery.MasterNotDiscoveredException: null at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:246) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:317) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:244) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:559) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:624) [elasticsearch-6.5.4.jar:6.5.4] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131] 原因：elasticsearch没有发现master和集群初始化主节点的配置为空解决办法：修改elasticsearch.yml，添加如下配置 cluster.initial_master_nodes: [“master”] 可能是网络原因，下次遇到的话先检查网络是否畅通(仅供参考)： discovery.zen.fd.ping_timeout: 1mdiscovery.zen.fd.ping_retries: 5]]></content>
      <categories>
        <category>改错</category>
        <category>centos</category>
        <category>elasticsearch</category>
        <category>es</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>centos</tag>
        <tag>elasticsearch</tag>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 启动报错(2)]]></title>
    <url>%2F2019%2F04%2F11%2Felasticsearch%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%992%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | elasticsearch 6.5.4 | VMware Workstation Pro 初次启动elasticsearch报错 [2019-04-11T17:32:11,932][INFO ][o.e.e.NodeEnvironment ] [uITtGcF] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [41.5gb], net total_space [49.9gb], types [rootfs] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[doit@master ~]$ /opt/elasticsearch-6.5.4/bin/elasticsearch[2019-04-11T17:32:11,932][INFO ][o.e.e.NodeEnvironment ] [uITtGcF] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [41.5gb], net total_space [49.9gb], types [rootfs][2019-04-11T17:32:11,990][INFO ][o.e.e.NodeEnvironment ] [uITtGcF] heap size [1015.6mb], compressed ordinary object pointers [true][2019-04-11T17:32:11,993][INFO ][o.e.n.Node ] [uITtGcF] node name derived from node ID [uITtGcFxRLSFwkuE1FUtwg]; set [node.name] to override[2019-04-11T17:32:11,994][INFO ][o.e.n.Node ] [uITtGcF] version[6.5.4], pid[2833], build[default/tar/d2ef93d/2018-12-17T21:17:40.758843Z], OS[Linux/3.10.0-514.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_131/25.131-b11][2019-04-11T17:32:11,994][INFO ][o.e.n.Node ] [uITtGcF] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/tmp/elasticsearch.nQEcpVMV, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -Des.path.home=/opt/elasticsearch-6.5.4, -Des.path.conf=/opt/elasticsearch-6.5.4/config, -Des.distribution.flavor=default, -Des.distribution.type=tar][2019-04-11T17:32:29,133][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [aggs-matrix-stats][2019-04-11T17:32:29,133][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [analysis-common][2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [ingest-common][2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [lang-expression][2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [lang-mustache][2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [lang-painless][2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [mapper-extras][2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [parent-join][2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [percolator][2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [rank-eval][2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [reindex][2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [repository-url][2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [transport-netty4][2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [tribe][2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-ccr][2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-core][2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-deprecation][2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-graph][2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-logstash][2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-ml][2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-monitoring][2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-rollup][2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-security][2019-04-11T17:32:29,138][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-sql][2019-04-11T17:32:29,138][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-upgrade][2019-04-11T17:32:29,138][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-watcher][2019-04-11T17:32:29,138][INFO ][o.e.p.PluginsService ] [uITtGcF] no plugins loaded[2019-04-11T17:32:56,126][INFO ][o.e.x.s.a.s.FileRolesStore] [uITtGcF] parsed [0] roles from file [/opt/elasticsearch-6.5.4/config/roles.yml][2019-04-11T17:33:00,018][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [uITtGcF] [controller/2892] [Main.cc@109] controller (64 bit): Version 6.5.4 (Build b616085ef32393) Copyright (c) 2018 Elasticsearch BV[2019-04-11T17:33:03,297][DEBUG][o.e.a.ActionModule ] [uITtGcF] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security[2019-04-11T17:33:05,848][INFO ][o.e.d.DiscoveryModule ] [uITtGcF] using discovery type [zen] and host providers [settings][2019-04-11T17:33:09,911][INFO ][o.e.n.Node ] [uITtGcF] initialized[2019-04-11T17:33:09,912][INFO ][o.e.n.Node ] [uITtGcF] starting ...[2019-04-11T17:33:11,314][INFO ][o.e.t.TransportService ] [uITtGcF] publish_address &#123;192.168.83.128:9300&#125;, bound_addresses &#123;192.168.83.128:9300&#125;[2019-04-11T17:33:11,462][INFO ][o.e.b.BootstrapChecks ] [uITtGcF] bound or publishing to a non-loopback address, enforcing bootstrap checksERROR: [1] bootstrap checks failed[1]: max number of threads [3782] for user [doit] is too low, increase to at least [4096][2019-04-11T17:33:11,505][INFO ][o.e.n.Node ] [uITtGcF] stopping ...[2019-04-11T17:33:11,891][INFO ][o.e.n.Node ] [uITtGcF] stopped[2019-04-11T17:33:11,891][INFO ][o.e.n.Node ] [uITtGcF] closing ...[2019-04-11T17:33:11,957][INFO ][o.e.n.Node ] [uITtGcF] closed[2019-04-11T17:33:11,985][INFO ][o.e.x.m.j.p.NativeController] [uITtGcF] Native controller process has stopped - no new native processes can be started 问题原因 &amp; 解决方法 内存不足,加大内存,报错前内存是1G，修改为2G问题解决]]></content>
      <categories>
        <category>改错</category>
        <category>centos</category>
        <category>elasticsearch</category>
        <category>es</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>centos</tag>
        <tag>elasticsearch</tag>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 启动报错(1)]]></title>
    <url>%2F2019%2F04%2F11%2Felasticsearch%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%991%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | elasticsearch 6.5.4 elasticsearch 启动报错 2019-04-11 21:24:31,724 main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253542019-04-11 21:24:31,724 main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:235) at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:135) at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:959) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:899) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:891) at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:514) at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:238) at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:250) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:547) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:263) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:234) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:127) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:302) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86)2019-04-11 21:24:31,734 main ERROR RollingFileManager (/opt/elasticsearch-6.5.4/logs/mycluster_index_search_slowlog.log) java.io.FileNotFoundException: /opt/elasticsearch-6.5.4/logs/mycluster_index_search_slowlog.log (权限不够) java.io.FileNotFoundException: /opt/elasticsearch-6.5.4/logs/mycluster_index_search_slowlog.log (权限不够) at java.io.FileOutputStream.open0(Native Method) at java.io.FileOutputStream.open(FileOutputStream.java:270) at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213) at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:133) at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:640) at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:608) at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:113) at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:114) at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:188) at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:145) at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:61) at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:123) at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:959) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:899) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:891) at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:514) at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:238) at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:250) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:547) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:263) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:234) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:127) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:302) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 检查文件权限问题 系日志文件造成，删除执行命令的输出日志 以非root账号启动，启动成功 百度参考解决方法：elastic官网红黑联盟]]></content>
      <categories>
        <category>改错</category>
        <category>centos</category>
        <category>elasticsearch</category>
        <category>es</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>centos</tag>
        <tag>elasticsearch</tag>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume启动agent报错]]></title>
    <url>%2F2019%2F04%2F06%2Fflume%E5%90%AF%E5%8A%A8agent%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[运行环境 | ubuntu 18.4 | flume 1.8.0 flume 启动agent报错 Including Hadoop libraries found via (/opt/hadoop-2.8.5/bin/hadoop) for HDFS accessInfo: Including Hive libraries found via (/opt/hive-1.2.2) for Hive access exec /usr/jdk1.8/bin/java -Xmx20m -Dflume.root.logger=INFO,console 12345678910111213antler@antler:/opt/flume-1.8.0$ ./bin/flume-ng agent -c conf/ -f myconf/flume.conf -Dflume.root.logger=INFO,consoleInfo: Including Hadoop libraries found via (/opt/hadoop-2.8.5/bin/hadoop) for HDFS accessInfo: Including Hive libraries found via (/opt/hive-1.2.2) for Hive access+ exec /usr/jdk1.8/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp &apos;/opt/flume-1.8.0/conf:/opt/flume-1.8.0/lib/*:/opt/hadoop-2.8.5/etc/hadoop:/opt/hadoop-2.8.5/share/hadoop/common/lib/*:/opt/hadoop-2.8.5/share/hadoop/common/*:/opt/hadoop-2.8.5/share/hadoop/hdfs:/opt/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/opt/hadoop-2.8.5/share/hadoop/hdfs/*:/opt/hadoop-2.8.5/share/hadoop/yarn/lib/*:/opt/hadoop-2.8.5/share/hadoop/yarn/*:/opt/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.8.5/share/hadoop/mapreduce/*:/opt/hadoop-2.8.5/contrib/capacity-scheduler/*.jar:/opt/hive-1.2.2/lib/*&apos; -Djava.library.path=:/opt/hadoop-2.8.5/lib/native org.apache.flume.node.Application -f myconf/flume.confSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/opt/flume-1.8.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.2019-04-06 19:58:40,247 (main) [ERROR - org.apache.flume.node.Application.main(Application.java:348)] A fatal error occurred while running. Exception follows.org.apache.commons.cli.MissingOptionException: Missing required option: n at org.apache.commons.cli.Parser.checkRequiredOptions(Parser.java:299) at org.apache.commons.cli.Parser.parse(Parser.java:231) at org.apache.commons.cli.Parser.parse(Parser.java:85) at org.apache.flume.node.Application.main(Application.java:263) 问题原因 &amp; 解决方法 命令错误 正确命令： bin/flume-ng agent -n a1 -c conf/ -f myconf/tail-avro.conf -Dflume.root.logger=INFO,console]]></content>
      <categories>
        <category>改错</category>
        <category>ubuntu</category>
        <category>flume</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>flume</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-sql启动报错util.NativeCodeLoader]]></title>
    <url>%2F2019%2F03%2F16%2Fsparksql%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99utilNativeCodeLoader%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | spark 2.2.0 | scala-2.11.8 | mysql-connector-java-5.1.38.jar 启动saprk-sql报错 19/03/16 20:40:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148[root@master bin]# ./spark-sql --master spark://master:7077 --jars /root/mysql-connector-java-5.1.38.jar 19/03/16 20:40:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable19/03/16 20:40:48 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore19/03/16 20:40:48 INFO metastore.ObjectStore: ObjectStore, initialize called19/03/16 20:40:49 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored19/03/16 20:40:49 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored19/03/16 20:40:58 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;19/03/16 20:41:03 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.19/03/16 20:41:03 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.19/03/16 20:41:05 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.19/03/16 20:41:05 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.19/03/16 20:41:06 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY19/03/16 20:41:06 INFO metastore.ObjectStore: Initialized ObjectStore19/03/16 20:41:07 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.019/03/16 20:41:07 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException19/03/16 20:41:11 INFO metastore.HiveMetaStore: Added admin role in metastore19/03/16 20:41:11 INFO metastore.HiveMetaStore: Added public role in metastore19/03/16 20:41:11 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty19/03/16 20:41:12 INFO metastore.HiveMetaStore: 0: get_all_databases19/03/16 20:41:12 INFO HiveMetaStore.audit: ugi=root ip=unknown-ip-addr cmd=get_all_databases19/03/16 20:41:12 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*19/03/16 20:41:12 INFO HiveMetaStore.audit: ugi=root ip=unknown-ip-addr cmd=get_functions: db=default pat=*19/03/16 20:41:12 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.19/03/16 20:41:13 INFO session.SessionState: Created local directory: /tmp/6a909914-1d3e-40f9-b997-ec14d17ef1db_resources19/03/16 20:41:14 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/6a909914-1d3e-40f9-b997-ec14d17ef1db19/03/16 20:41:14 INFO session.SessionState: Created local directory: /tmp/root/6a909914-1d3e-40f9-b997-ec14d17ef1db19/03/16 20:41:14 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/6a909914-1d3e-40f9-b997-ec14d17ef1db/_tmp_space.db19/03/16 20:41:15 INFO spark.SparkContext: Running Spark version 2.2.019/03/16 20:41:15 INFO spark.SparkContext: Submitted application: SparkSQL::192.168.83.1019/03/16 20:41:15 INFO spark.SecurityManager: Changing view acls to: root19/03/16 20:41:15 INFO spark.SecurityManager: Changing modify acls to: root19/03/16 20:41:15 INFO spark.SecurityManager: Changing view acls groups to: 19/03/16 20:41:15 INFO spark.SecurityManager: Changing modify acls groups to: 19/03/16 20:41:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set()19/03/16 20:41:17 INFO util.Utils: Successfully started service &apos;sparkDriver&apos; on port 58142.19/03/16 20:41:17 INFO spark.SparkEnv: Registering MapOutputTracker19/03/16 20:41:17 INFO spark.SparkEnv: Registering BlockManagerMaster19/03/16 20:41:17 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information19/03/16 20:41:17 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up19/03/16 20:41:17 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f02e18ee-fc40-4a05-80ba-0c60c369ed5a19/03/16 20:41:17 INFO memory.MemoryStore: MemoryStore started with capacity 413.9 MB19/03/16 20:41:18 INFO spark.SparkEnv: Registering OutputCommitCoordinator19/03/16 20:41:18 INFO util.log: Logging initialized @42429ms19/03/16 20:41:19 INFO server.Server: jetty-9.3.z-SNAPSHOT19/03/16 20:41:19 INFO server.Server: Started @42899ms19/03/16 20:41:19 WARN util.Utils: Service &apos;SparkUI&apos; could not bind on port 4040. Attempting port 4041.19/03/16 20:41:19 INFO server.AbstractConnector: Started ServerConnector@2e49b91c&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:4041&#125;19/03/16 20:41:19 INFO util.Utils: Successfully started service &apos;SparkUI&apos; on port 4041.19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18c7f6b5&#123;/jobs,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53bb71e5&#123;/jobs/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15994b0b&#123;/jobs/job,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cb00fa5&#123;/jobs/job/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@698d6d30&#123;/stages,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3407aa4f&#123;/stages/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@538b3c88&#123;/stages/stage,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22ae905f&#123;/stages/stage/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fbaa7f5&#123;/stages/pool,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d4a05f7&#123;/stages/pool/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b476233&#123;/storage,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f235e8e&#123;/storage/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29dcdd1c&#123;/storage/rdd,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@524f5ea5&#123;/storage/rdd/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17134190&#123;/environment,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5599b5bb&#123;/environment/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4264beb8&#123;/executors,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7cd3e0da&#123;/executors/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67e77f52&#123;/executors/threadDump,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d1bf7bf&#123;/executors/threadDump/json,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d43a1b7&#123;/static,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44aa91e2&#123;/,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@650a1aff&#123;/api,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11c7a0b4&#123;/jobs/job/kill,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@653a5967&#123;/stages/stage/kill,null,AVAILABLE,@Spark&#125;19/03/16 20:41:19 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.83.10:404119/03/16 20:41:19 INFO spark.SparkContext: Added JAR file:/root/mysql-connector-java-5.1.38.jar at spark://192.168.83.10:58142/jars/mysql-connector-java-5.1.38.jar with timestamp 155274007977019/03/16 20:41:20 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077...19/03/16 20:41:20 INFO client.TransportClientFactory: Successfully created connection to master/192.168.83.10:7077 after 132 ms (0 ms spent in bootstraps)19/03/16 20:41:40 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077...19/03/16 20:42:00 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077...19/03/16 20:42:20 ERROR cluster.StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.19/03/16 20:42:20 WARN cluster.StandaloneSchedulerBackend: Application ID is not initialized yet.19/03/16 20:42:20 INFO util.Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 34217.19/03/16 20:42:20 INFO netty.NettyBlockTransferService: Server created on 192.168.83.10:3421719/03/16 20:42:20 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy19/03/16 20:42:20 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.83.10, 34217, None)19/03/16 20:42:20 INFO server.AbstractConnector: Stopped Spark@2e49b91c&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:4041&#125;19/03/16 20:42:20 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.83.10:404119/03/16 20:42:20 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.83.10:34217 with 413.9 MB RAM, BlockManagerId(driver, 192.168.83.10, 34217, None)19/03/16 20:42:20 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors19/03/16 20:42:20 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.83.10, 34217, None)19/03/16 20:42:20 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.83.10, 34217, None)19/03/16 20:42:20 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down19/03/16 20:42:20 WARN client.StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master19/03/16 20:42:20 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!19/03/16 20:42:21 INFO memory.MemoryStore: MemoryStore cleared19/03/16 20:42:21 INFO storage.BlockManager: BlockManager stopped19/03/16 20:42:21 INFO storage.BlockManagerMaster: BlockManagerMaster stopped19/03/16 20:42:21 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!19/03/16 20:42:21 ERROR spark.SparkContext: Error initializing SparkContext.java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem at scala.Predef$.require(Predef.scala:224) at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:524) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509) at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909) at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901) at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:48) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.&lt;init&gt;(SparkSQLCLIDriver.scala:293) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:138) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)19/03/16 20:42:21 INFO spark.SparkContext: SparkContext already stopped.19/03/16 20:42:21 INFO spark.SparkContext: Successfully stopped SparkContextException in thread &quot;main&quot; java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem at scala.Predef$.require(Predef.scala:224) at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:524) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509) at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909) at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901) at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:48) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.&lt;init&gt;(SparkSQLCLIDriver.scala:293) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:138) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)19/03/16 20:42:21 INFO util.ShutdownHookManager: Shutdown hook called19/03/16 20:42:21 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-06aa1468-390b-4e70-9151-405bd556805a19/03/16 20:42:22 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-6774f957-ed7c-4d8b-b3ce-eaf17bda1df1 问题原因 &amp; 解决方法 由于spark 配置为高可用模式下，要启动zookeeper 启动zookeeper后正常启动12345678[root@master sbin]# ./start-all.sh starting org.apache.spark.deploy.master.Master, logging to /opt/spark-2.2.0/logs/................. 日志量太大 此处省略 ...........................19/03/16 20:47:52 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/opt/spark-2.2.0/sbin/spark-warehousespark-sql&gt;]]></content>
      <categories>
        <category>改错</category>
        <category>centos</category>
        <category>spark-sql</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>centos</tag>
        <tag>spark-sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive启动客户端报错]]></title>
    <url>%2F2019%2F02%2F24%2Fhive%E5%90%AF%E5%8A%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | hive 1.2.2 | mysql-connector-java-5.1.38.jar 启动hive客户端报错 Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=”/tmp”:root:supergroup:drwx—— 12345678910111213141516171819202122232425262728Connecting to jdbc:hive2://master:10000Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=&quot;/tmp&quot;:root:supergroup:drwx------ at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:279) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:206) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:507) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1612) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1630) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:551) at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:110) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3000) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1107) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:873) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489) (state=,code=0)Beeline version 1.2.2 by Apache HiveConnecting to jdbc:hive2://master:10000Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)Beeline version 1.2.2 by Apache Hive 问题原因 &amp; 解决方法 hdfs 没有读写权限,配置文件中加入 123456789101112131415161718192021222324&lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;2&lt;/value&gt;&lt;/property&gt; 对 hdfs /tmp 加入777 权限1hdfs dfs -chmod -R 777 /tmp/]]></content>
      <categories>
        <category>改错</category>
        <category>centos</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>centos</tag>
        <tag>hive</tag>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive客户端启动 Error Failed to open new session]]></title>
    <url>%2F2019%2F02%2F23%2Fhive%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99ErrorFailedtoopennewsession%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 6.6 | VMware Workstation Pro|hive-1.2.2|HADOOP 2.8.5 hive客户端连接hive服务端报错Connecting to jdbc:hive2://master:10000Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException 服务端启动:1nohup ./hiveserver2 1&gt;/root/hiveout.log 2&gt;&amp;1 &amp; 客户端连接：1./beeline -u jdbc:hive2://master:10000 启动后报错：123456789101112131415161718192021222324Connecting to jdbc:hive2://master:10000Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=&quot;/tmp&quot;:root:supergroup:drwx------ at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:279) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:206) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:507) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1612) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1630) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:551) at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:110) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3000) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1107) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:873) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489) (state=,code=0)Beeline version 1.2.2 by Apache Hive 在Hadoop core-site.xml 文件中加入(添加完需要重启服务)：123456789101112131415161718192021222324&lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;2&lt;/value&gt;&lt;/property&gt; 在 hive-site.xml 中加入：1234&lt;property&gt; &lt;name&gt;hive.scratch.dir.permission&lt;/name&gt; &lt;value&gt;777&lt;/value&gt;&lt;/property&gt; 再次启动 报错：1234Connecting to jdbc:hive2://master:10000Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)Beeline version 1.2.2 by Apache Hive0: jdbc:hive2://master:10000 (closed)&gt; show databases; 对 hdfs /tmp添加 777 权限1hdfs dfs -chmod -R 777 /tmp/ 再次启动成功1234567891011121314[root@slave0 bin]# ./beeline -u jdbc:hive2://master:10000Connecting to jdbc:hive2://master:10000Connected to: Apache Hive (version 1.2.2)Driver: Hive JDBC (version 1.2.2)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.2.2 by Apache Hive0: jdbc:hive2://master:10000&gt; show databases;+----------------+--+| database_name |+----------------+--+| default || test_db |+----------------+--+2 rows selected (9.603 seconds)]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos6.6启动报错 /dev/mapper/VolGroup-lv_root:UNEXPECTED......]]></title>
    <url>%2F2019%2F02%2F22%2Fcentos6.6%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99Checkingfilesystems%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 6.6 | VMware Workstation Pro centos6.6启动报错/dev/mapper/VolGroup-lv_root:UNEXPECTED INCONSISTENCY;RUN fsck MANUALLY.(i.e., without -a or -p options) [FAILED]… 解决方案：根据提示输入密码 输入：fsck -y /dev/root fsck -y /VolGroup-lv_root/root fsck -y /dev/sda1 中途会有提示输入y 重启后问题解决 1234567891011121314151617fsckfsck（file system check）用来检查和维护不一致的文件系统。若系统掉电或磁盘发生问题，可利用fsck命令对文件系统进行检查。使用方式 : fsck [-sACVRP] [-t fstype] [–] [fsck-options] filesys […]filesys ： 磁盘设备名称(eg./dev/sda1)，挂载（mount）点 (eg. / 或 /usr)-t : 给定档案系统的型式，若在 /etc/fstab 中已有定义或 kernel 本身已支援的则不需加上此参数-s : 依序一个一个地执行 fsck 的指令来检查-A : 对/etc/fstab 中所有列出来的 分区（partition）做检查-C : 显示完整的检查进度-d : 打印出 e2fsck 的 debug 结果-p : 同时有 -A 条件时，同时有多个 fsck 的检查一起执行-R : 同时有 -A 条件时，省略 / 不检查-V : 详细显示模式-a : 如果检查有错则自动修复-r : 如果检查有错则由使用者回答是否修复-y : 选项指定检测每个文件是自动输入yes，在不确定那些是不正常的时候，可以执行 # fsck -y 全部检查修复。例子 :检查 msdos 档案系统的 /dev/sda1 是否正常，如果有异常便自动修复 :fsck -t msdos -a /dev/sda1]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware安装centos6并修改静态IP与关闭防火墙]]></title>
    <url>%2F2019%2F01%2F23%2FVMware%E5%AE%89%E8%A3%85centos6%E5%B9%B6%E4%BF%AE%E6%94%B9%E9%9D%99%E6%80%81IP%E4%B8%8E%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[运行环境 | 操作系统:centos6.6 | VMware Workstation Pro 12 进入VMware一般选择典型就能完成基本设置（这个教程选择典型）选择稍后安装后下一步选择Linux—&gt;版本为centos64位填写虚拟机的名字和虚拟机存放路径磁盘大小根据个人需求调整如果对硬件有调整可以进入自定义硬件调整硬件设置完成后在左侧出现刚刚创建的虚拟机双击CD进入设置界面选着ISO景象文件选择第一个正在进入键盘→选择skip选择后回车进入安装首页，这里下一步选着语言界面，本教程下一步，根据个人选择语言默认下一步下一步选择Yes,discard any data 自动下一步Hostname 填写主机名时区选择shagnhai填写密码，两次要一致，选着Use Anyway选择第一个选择Write changes to disk自动进入下一步正在加载教程这里选择Minimal(最小化配置)，根据个人需求选择下一步后进入安装界面正在安装Reboot 后系统安装完成正在重启进入系统界面输入用户名和密码修改网卡配置修改内容 123456789101112131415161718192021修改内容：[root@master ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0HWADDR=00:0C:29:07:2C:EETYPE=EthernetUUID=82dfdeac-4a6d-4117-aaea-a5002d9898f6ONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=staticIPADDR=192.168.83.10NETMASK=255.255.255.0GATEWAY=192.168.83.1修改成功后，重启networkservice network restart需要注意的是：修改的IP地址是本机的网卡地址 修改成功后，重启network]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse多余工作空间记录删除方法]]></title>
    <url>%2F2019%2F01%2F23%2Feclipse%E5%A4%9A%E4%BD%99%E5%B7%A5%E4%BD%9C%E7%A9%BA%E9%97%B4%E8%AE%B0%E5%BD%95%E5%88%A0%E9%99%A4%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[运行环境 | windows | Eclipse Jee Latest Released 进入eclipse安装目录,在.settings 下 org.eclipse.ui.ide.prefs 文件1eclipse\configuration\.settings\org.eclipse.ui.ide.prefs 第二行 每个记录以 \n结束]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[comparator 排序报错IllegalArgumentException]]></title>
    <url>%2F2019%2F01%2F05%2Fcomparator%E6%8E%92%E5%BA%8F%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[运行环境 | eclipse 12 | jdk1.8 | 数据量 100万 使用comparator 排序报错，十几条不出任何问题，正常排序，当超过100条数据时跑错java.lang.IllegalArgumentException 程序要对100万条json数据进行排序 123456789java.lang.IllegalArgumentException: Comparison method violates its general contract! at java.util.TimSort.mergeLo(Unknown Source) at java.util.TimSort.mergeAt(Unknown Source) at java.util.TimSort.mergeCollapse(Unknown Source) at java.util.TimSort.sort(Unknown Source) at java.util.Arrays.sort(Unknown Source) at java.util.ArrayList.sort(Unknown Source) at java.util.Collections.sort(Unknown Source) at movieDemo.Demo4.main(Demo4.java:46) 解决方法：需要判断 结果 等于0的情况 加上等于零的情况，正常排序if(o1.getValue() == o2.getValue()) { return 0;}或者使用三目运算嵌套return o1.getValue() - o2.getValue() == 0 ? 0 : o1.getValue() - o2.getValue() &gt; 0 ?-1 : 1 ; 反例：return o1.getValue() - o2.getValue() &gt; 0 ? -1 : 1;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rpc 报错 ClassNotFoundException]]></title>
    <url>%2F2019%2F01%2F05%2Frpc%E6%8A%A5%E9%94%99ClassNotFoundException%2F</url>
    <content type="text"><![CDATA[运行环境 | eclipse 12 | jdk1.8 rpc发送Request对象报错 java.lang.ClassNotFoundException1234567891011121314Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: rpc_client.pojo.Request at java.net.URLClassLoader.findClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Unknown Source) at java.io.ObjectInputStream.resolveClass(Unknown Source) at java.io.ObjectInputStream.readNonProxyDesc(Unknown Source) at java.io.ObjectInputStream.readClassDesc(Unknown Source) at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source) at java.io.ObjectInputStream.readObject0(Unknown Source) at java.io.ObjectInputStream.readObject(Unknown Source) at rpc_server.admin.Server.main(Server.java:20) 解决方法： request 和 user 路径要一致（两文件路径必须一致） import rpc_client.pojo.Request; import rpc_client.pojo.User; 改为 import pojo.Request; import pojo.User;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 默认进入桌面和命令模式]]></title>
    <url>%2F2019%2F01%2F05%2Fcentos7%E9%BB%98%E8%AE%A4%E8%BF%9B%E5%85%A5%E6%A1%8C%E9%9D%A2%E5%92%8C%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 systemctl get-default 查看当前的模式12[root@master ~]# systemctl get-default graphical.target systemctl set-default multi-user.target 设置默认进入 命令行模式 123[root@master ~]# systemctl set-default multi-user.target Removed symlink /etc/systemd/system/default.target.Created symlink from /etc/systemd/system/default.target to /usr/lib/systemd/system/multi-user.target.]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[删除C盘轻度测试]]></title>
    <url>%2F2018%2F12%2F21%2F%E5%88%A0%E9%99%A4C%E7%9B%98%E8%BD%BB%E5%BA%A6%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[运行环境 | 操作系统:windows10专业版 事先把各种防火墙、Windows Defender安全关掉然后先跑一个kafan病毒……. 运行完没有反应，看进程kafan病毒在运行… 因为懒得安装eclipse，直接用命令行编译运行开始删除 桌面和任务栏的图标被删掉了 kafan病毒有反应了… 看下C盘，文件还在 Windows下文件基本都在洗洗睡吧]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启动 jenkins Job for jenkins service failed....]]></title>
    <url>%2F2018%2F09%2F12%2F%E5%90%AF%E5%8A%A8JenkinsJobForJenkinsServiceFailed%2F</url>
    <content type="text"><![CDATA[搭建环境 | 系统：centos 7 | Java 1.8 | jenkins 2.121.3-1.1 2.1 [root@master ~]# /etc/init.d/jenkins start Starting jenkins (via systemctl): Job for jenkins.service failedbecause the control process exited with error code. See “systemctlstatus jenkins.service” and “journalctl -xe” for details.[FAILED] 查看日志 ● jenkins.service - LSB: Jenkins Automation Serve Loaded: loaded (/etc/rc.d/init.d/jenkins; bad; vendor preset: disabled) Active: failed (Result: exit-code) since Tue 2018-09-11 20:39:41 EDT; 2min 48s ago Docs: man:systemd-sysv-generator(8) Sep 11 20:39:38 master systemd[1]: Starting LSB: Jenkins Automation Server… _Sep 11 20:39:40 master runuser[11110]: pam_unix(runuser:session): session opened for …=0)_ Sep 11 20:39:41 master jenkins[11104]: Starting Jenkins bash: /usr/jdk1.8: Is a directory _Sep 11 20:39:41 master runuser[11110]: pam_unix(runuser:session): session closed for …oot_ Sep 11 20:39:41 master jenkins[11104]: [FAILED] Sep 11 20:39:41 master systemd[1]: jenkins.service: control process exited, code=exit…s=1 Sep 11 20:39:41 master systemd[1]: Failed to start LSB: Jenkins Automation Server. Sep 11 20:39:41 master systemd[1]: Unit jenkins.service entered failed state. Sep 11 20:39:41 master systemd[1]: jenkins.service failed. Hint: Some lines were ellipsized, use -l to show in full. 修改配置文件： candidates=“ #/etc/alternatives/java #/usr/lib/jvm/java-1.8.0/bin/java #/usr/lib/jvm/jre-1.8.0/bin/java #/usr/lib/jvm/java-1.7.0/bin/java #/usr/lib/jvm/jre-1.7.0/bin/java #/usr/bin/java #/usr/jdk1.8 /usr/jdk1.8/bin/java ” （注释掉原有jdk路径,添加自己的） 在candidates里插入自己的jdk java路径，其他的全部注释掉 #JENKINS_USER=”jenkins”JENKINS_USER=”root”JENKINS_USER 改成root 再次启动 jenkins 错误解决 [root@master ~]# /etc/init.d/jenkins start Starting jenkins (via systemctl): Warning: jenkins.service changed on disk. Run ‘systemctl daemon-reload’ to reload units. [ OK ]]]></content>
      <categories>
        <category>改错</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>centos</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 初始化msyql报错 SQL Error code 1045]]></title>
    <url>%2F2018%2F09%2F10%2Fhive%E5%88%9D%E5%A7%8B%E5%8C%96msyql%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[搭建环境 | 系统：centos 7 | Hadoop：3.4.12 | hive：2.3.3 | mysql：5.6.40 | Java 1.8 msyql单独建立一个hive的用户的数据库，为了方便存hive的元数据 初始化mysql 报错： org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version. Underlying cause: java.sql.SQLException : Access denied for user ‘root‘@’hive’ (using password: YES) SQL Error code: 1045 Use –verbose for detailed stacktrace. *** schemaTool failed *** 按照百度教程，进mysql 修改了数据库的权限，不好使 最后用native for mysql挨个修改用户的密码，为了不出错，密码和root用户的密码保持一致，再次初始化 成功。]]></content>
      <categories>
        <category>改错</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7启动zookeeper 报错]]></title>
    <url>%2F2018%2F08%2F27%2Fcentos7%E5%90%AF%E5%8A%A8zookeeper%20%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[运行环境 | 操作系统:centos7 | jdk:1.8 | zookeeper：4.2.1 zookeeper 已经安装成功启动时报错。运行 systemctl start zookeeper 后显示： Failed to start zookeeper.service: Unit not found. jps 也没有QuorumPeerMain进程 #zkServer.sh status 查看，反馈如下： #JMX enabled by default 先后检查配置文件 conf/zoo.cfg tickTime=2000initLimit=5syncLimit=2dataDir=/opt/zookeeper/data dataLogDir=/opt/zookeeper/logsclientPort=2181 server.1=192.168.88.128:2888:3888server.2=192.168.88.129:2888:3888 server.3=192.168.88.130:2888:3888 data/myid1 三个节点中两个配置文件没有错误（myid文件分别为 1、2、 3） 检查 /etc/profile jdk和zookeeper的配置环境 #JDK1.8 configuration JAVA_HOME=/usr/jdk1.8 JRE_HOME=$ JAVA_HOME/jre PATH=$ PATH:$ JAVA_HOME/bin CLASSPATH=.:$ JAVA_HOME/lib/dt.jar:$ JAVA_HOME/lib/tools.jar export JAVA_HOME export JRE_HOME export PATH export CLASSPATH ZOOKEEPER_HOME=/opt/zookeeper export PATH=$ ZOOKEEPER_HOME/bin:$PATH export ZOOKEEPER_HOME export PATH 都没问题，最后是因为，没有产生环境变量source下profile文件在启动zookeeper启动成功。]]></content>
      <categories>
        <category>改错</category>
        <category>centos</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>zookeeper</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 初始化msyql报错 SQL Error code 1045]]></title>
    <url>%2F2018%2F08%2F24%2FHive%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[Hive初始化报错 org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:234) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) 完整报错信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899ls: 无法访问/opt/spark-2.2.0/lib/spark-assembly-*.jar: 没有那个文件或目录SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/hive-1.2.2/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]Logging initialized using configuration in jar:file:/opt/hive-1.2.2/lib/hive-common-1.2.2.jar!/hive-log4j.propertiesException in thread &quot;main&quot; java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:234) at org.apache.hadoop.util.RunJar.main(RunJar.java:148)Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) ... 8 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ... 14 moreCaused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factoryNestedThrowables:java.lang.reflect.InvocationTargetException at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5768) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:199) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:74) ... 19 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) at org.datanucleus.store.AbstractStoreManager.&lt;init&gt;(AbstractStoreManager.java:240) at org.datanucleus.store.rdbms.RDBMSStoreManager.&lt;init&gt;(RDBMSStoreManager.java:286) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ... 48 moreCaused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the &quot;BONECP&quot; plugin to create a ConnectionPool gave an error : The specified datastore driver (&quot;com.mysql.jdbc.Driver&quot;) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.&lt;init&gt;(ConnectionFactoryImpl.java:85) ... 66 moreCaused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (&quot;com.mysql.jdbc.Driver&quot;) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58) at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) ... 68 more 原因：hivehome/lib下没有mysql jar包 放入不报错]]></content>
      <categories>
        <category>改错</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 安装mysql 及修改密码]]></title>
    <url>%2F2018%2F07%2F21%2Fcentos7%E5%AE%89%E8%A3%85mysql%E5%8F%8A%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | mysql 5.7 mysql安装 官网下载MySQL rpm包12rpm -ivh mysql57-community-release-el7-8.noarch.rpmyum install mysql-community-server 安装完后启动MySQL服务1service mysqld restart 设置密码：1update mysql.user set authentication_string=password(&apos;新密码&apos;) where user=&apos;用户&apos;; 重启数据库后登陆不上 在 /etc/my.cnf 最后加上 skip-grant-tables 重启服务，免密码登陆，修改密码1alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;新密码&apos;; 修改数据库远程登陆权限：12use mysql;grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;新密码&apos; with grant option; 创建用户密码规则修改1CREATE USER &apos;用户&apos;@&apos;%&apos; IDENTIFIED BY &apos;密码&apos;; 创建报错 ERROR 1819 (HY000): Your password does not satisfy the current policy requirements 查看权限1select @@validate_password_policy; MEDIUM 1SHOW VARIABLES LIKE &apos;validate_password%&apos;; Variable_name Value validate_password_check_user_name OFF validate_password_dictionary_file validate_password_length 8 validate_password_mixed_case_count 1 validate_password_number_count 1 validate_password_policy MEDIUM validate_password_special_char_count 1 修改密码等级1set global validate_password_policy=LOW; 设置密码长度1set global validate_password_length=5; 刷新1FLUSH PRIVILEGES;]]></content>
      <categories>
        <category>centos</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop 启动报错]]></title>
    <url>%2F2018%2F07%2F20%2FHadoop%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[运行环境 | centos 7.0 | Hadoop 2.8.1 [root@master sbin]# ./start-all.shThis script is Deprecated. Instead use start-dfs.sh and start-yarn.shStarting namenodes on [master]The authenticity of host ‘master (192.168.91.10)’ can’t be established.RSA key fingerprint is 7e:3f:e7:5b:69:74:e3:0e:87:7b:2b:df:3d:64:b3:1e.Are you sure you want to continue connecting (yes/no)? yesmaster: Warning: Permanently added ‘master,192.168.91.10’ (RSA) to the list of known hosts.master: Error: JAVA_HOME is not set and could not be found.master: Error: JAVA_HOME is not set and could not be found.slave0: Error: JAVA_HOME is not set and could not be found.slave1: Error: JAVA_HOME is not set and could not be found.Starting secondary namenodes [0.0.0.0]The authenticity of host ‘0.0.0.0 (0.0.0.0)’ can’t be established.RSA key fingerprint is 7e:3f:e7:5b:69:74:e3:0e:87:7b:2b:df:3d:64:b3:1e.Are you sure you want to continue connecting (yes/no)? yes0.0.0.0: Warning: Permanently added ‘0.0.0.0’ (RSA) to the list of known hosts.0.0.0.0: Error: JAVA_HOME is not set and could not be found.starting yarn daemonsstarting resourcemanager, logging to /opt/hadoop/logs/yarn-root-resourcemanager-master.outslave0: Error: JAVA_HOME is not set and could not be found.slave1: Error: JAVA_HOME is not set and could not be found.master: Error: JAVA_HOME is not set and could not be found. 查看各个配置文件 1.core-site.xml 2.hdfs-site.xml 3.mapred-site.xml 4.yarn-site.xml 修改 profile 文件 export HADOOP_HOME=/opt/hadoop export PATH=$PATH:${HADOOP_HOME}/bin (这里要修改成自己的安装路径) 修改文件： [root@master hadoop]# vi hadoop-env.sh文件中：export JAVA_HOME=${JAVA_HOME} 修改为（修改成自己的jdk路径）： export JAVA_HOME=/usr/jdk1.8 在重新启动,启动成功 查看进程,进程运行错误解决 [root@master sbin]# ./start-all.shThis script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on[master] master: starting namenode, logging to /opt/hadoop/logs/hadoop-root-namenode-master.out slave1: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave1.out master: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-master.out slave0: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave0.out Starting secondary namenodes [0.0.0.0] 0.0.0.0:starting secondarynamenode, logging to /opt/hadoop/logs/hadoop-root-secondarynamenode-master.out starting yarn daemons resourcemanager running as process 3253. Stop it first.slave1: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave1.out master: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-master.out slave0: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave0.out[root@master sbin]# jps4176 NodeManager4289 Jps3253 ResourceManager3669 NameNode3957 SecondaryNameNode3771 DataNode]]></content>
      <categories>
        <category>改错</category>
        <category>centos</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>error</tag>
        <tag>zookeeper</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础（四）]]></title>
    <url>%2F2017%2F12%2F02%2Fjava%E5%9F%BA%E7%A1%80(%E5%9B%9B)%2F</url>
    <content type="text"><![CDATA[一、顺序结构12### 二、选择结构 也叫分支结构选择结构会根据执行的结果选择不同的代码执行 两种形式：1&lt;!--more--&gt; if语句 switch语句 1. if语句 a． if语句的第一种形式 if（关系表达式）{ 语句体；}执行流程：首先判断关系表达式看其结果是true还是false 如果是true就执行语句体如果是false就不执行语句体 1) if语句的注意事项：if语句中的大括号是可以省略的，一旦省略，只控制到第一条语句结束（第一个分号结束） b． if语句的第二种形式 if（关系表达式）{ 语句体1；}else{ 语句体2；} c． 语句的第三种形式 if(关系表达式1) { 语句体1;}else if (关系表达式2) { 语句体2;} …else { 语句体n+1;} 2. switch语句 a.switch(表达式) { case 常量值1： 语句体1; break; case 常量值2： 语句体2; break; … default： 语句体n+1; break;} Switch 表达式的取值：byte,short,int,char,JDK7以后可以是String b. 执行流程：首先计算出表达式的值其次，和case依次比较，一旦有对应的值，就会执行相应的语句，在执行的过程中，遇到break就会结束。最后，如果所有的case都和表达式的值不匹配，就会执行default语句体部分，然后程序结束掉 c. 注意事项： 1. case后面只能跟常量，不能跟变量 2. 多个case后面的常量值不能相同 3. case的顺序没有要求，可以放到任意位置 4. default也可以放到任意位置 5. default可以省略 6. break可以省略，如果省略，代码会继续向下执行，不管case是否匹配成功，一直再次遇到break，或者执行到了switch语句结束 7. switch语句遇到break，或者switch语句结束 12### 三、循环结构 让一段代码反复执行很多次 for循环 a. 语法格式：for(初始化语句;判断条件语句;控制条件语句) { 循环体语句体; } b. 执行流程： A:执行初始化语句 B:执行判断条件语句，看其结果是true还是false如果是false，循环结束。如果是true，继续执行。C:执行循环体语句D:执行控制条件语句E:回到B继续 while循环：a. 语法格式：初始化语句;while(判断条件语句) { 循环体语句体; 控制条件语句;}练习: //打印1-100 //求1-10之和 //求1-100 奇数和 偶数和 int i=1; while(i&lt;=100) { System.out.println(i); i++; } int sum = 0; int j=1; while(j&lt;=10) { sum += j; j++; } System.out.println(sum);//55 int singleSum = 0; int doubleSum = 0; int k=1; while(k&lt;=100) { if(k%2 == 0) { doubleSum += k; }else { singleSum += k; } k++; } System.out.println(doubleSum);//2550 System.out.println(singleSum);//2500 3. For和while的区别： for循环适合针对一个范围判断进行操作while循环适合判断次数不明确操作教室高度：8848m，我现在有一张足够大的纸张， 厚度为：0.12m。请问，我折叠多少次，就可以保证厚度不低于教室的高度?/** 教室高度：8848m，我现在有一张足够大的纸张， 厚度为：0.12m。请问，我折叠多少次，就可以保证厚度不低于教室的高度? 不低于:&gt;= 循环次数不知道: while: 变量: 厚度 次数 循环条件: 什么时候折叠: 厚度小于8848,没有等于,因为等于就已经符合条件了不需要在折叠 循环体: 折叠的操作” height*=2 次数加1/public class WhileTest3 {public static void main(String[] args) {double height = 0.12;int count = 0;while(height&lt;8848) { height =2; count++;}System.out.println(count);}} do…while循环：a. 格式:初始化语句;do {循环体语句;控制条件语句;} while(判断条件语句);b. 流程: 先执行初始化语句在执行循环体语句在执行条件控制语句在做条件的判断,true, 继续执行循环体和条件控制false: 循环结束特点:循环体语句至少执行一次c. 案例: 打印100句话:public class DoWhileDemo {public static void main(String[] args) {int i=0;//初始化语句do { System.out.println(“沉迷学习,日渐消瘦”+i);//循环体 i++;//条件控制}while(i&lt;100);//条件判断}} 1### 四、 循环综合案例 贪吃蛇：需求: 在键盘上输入一个数，如果是1，代表蛇吃食物，得分加10，并继续输入，如果输入的是一个非1的数字，停止程序，输出得分分析:有键盘录入: 使用Scanner几个变量: 存储输入的结果oper 分数score并继续输入: 说明需要循环反复的执行,使用循环语句循环的次数知道? 不知道,一般使用while循环循环的条件: 输入的结果是1循环体: 得分加10, 继续输入条件控制语句: 继续输入public static void main(String[] args) {Scanner sc = new Scanner(System.in);System.out.println(“欢迎来到多易[www.51doit.com]贪吃蛇,蛇还有5秒到达战场,请做好准备!&quot;);//分数int score = 0;System.out.println(“请输入您的操作(1/0)”);int oper = sc.nextInt();while(oper == 1) {// 操作:得分加10, 继续输入score += 10;System.out.println(“请输入您的操作(1/0)”);oper = sc.nextInt();//对oper重新赋值,(条件控制语句)}//代码如果能够执行到这里,说明循环已经结束了System.out.println(“GAME OVER:您的分数为:”+score);//打印得分} 2. 循环登录练习 需求： 1.控制台提示用户输入密码 2.用户输入密码 3.如果用户输入的密码不等于1234，回到步骤1 4.如果用户输入的密码等于1234，提示登录成功 分析: 分析: 需要用到Scanner 循环: 执行多少次: 不确定: while 循环的条件: 用户输入的密码不等于1234 循环体: 提示用户输入密码,用户输入密码public class CircleLoginDemo { public static void main(String[] args) { //1 Scanner Scanner sc = new Scanner(System.in); //2. 提示用户输入密码 System.out.println(“请输入密码”); //3. 用户输入密码 int password = sc.nextInt(); //4. 循环 while(password != 1234) { System.out.println(“请输入密码”); password = sc.nextInt();//重新赋值 } System.out.println(&quot;登录成功&quot;); } } 案例一、上述代码将密码改写成字符串 字符串在比较内容的时候是不能使用 == != 的, 字符串在比较内容的时候,用的是equals 用法: 字符串a.equals(字符串b); 如果内容相同 返回true, 否则返回false 在使用equals比较字符串内容的时候,要把已知存在的字符串放到前面 public class CircleLoginDemo2 { public static void main(String[] args) { //1 Scanner Scanner sc = new Scanner(System.in); //2. 提示用户输入密码 System.out.println(&quot;请输入密码&quot;); //3. 用户输入密码 String password = sc.nextLine(); //4. 循环 //abc1234是已知存在的,所以放到前面 while(!&quot;abc1234&quot;.equals(password)) { // password != &quot;abc1234&quot; System.out.println(&quot;请输入密码&quot;); password = sc.nextLine();//重新赋值 } System.out.println(&quot;登录成功&quot;); } } 案例二、上述代码使用do…while实现 public class CircleLoginDemo3 { public static void main(String[] args) { //1 Scanner Scanner sc = new Scanner(System.in); String password;//变量的声明, 局部变量不赋初值不能使用 do { //2. 提示用户输入密码 System.out.println(&quot;请输入密码&quot;); //3. 用户输入密码 password = sc.nextLine();//赋值 }while(!&quot;abc1234&quot;.equals(password)); System.out.println(&quot;登录成功&quot;); } } 案例三、do you love me 需求: 问老婆一个问题: 你爱我么? 如果回答的不是 爱,继续问 如果回答的是爱, 循环结束 分析: 至少执行一次,可以使用do...while 条件: 不是爱 equals 循环体: 继续问 继续获取答案 public class DoYouLoveMeDemo { public static void main(String[] args) { //1. Scanner sc = new Scanner(System.in); System.out.println(&quot;在一个月黑风高的夜晚,由于多看了旁边小姐姐一眼,被老婆大人赏了一个耳光,我满眼泛起委屈的泪花,含情脉脉的问她:&quot;); String answer; do { System.out.println(&quot;老婆,你爱我么?&quot;); answer = sc.nextLine(); }while(!&quot;爱&quot;.equals(answer)); System.out.println(&quot;执子之手与子偕老,用我三生烟火换你一世迷离!!&quot;); } } 1### 五、 循环嵌套 在循环中继续使用循环:案例: 请输出一个4行5列的星星(*)图案/** 循环的嵌套: 循环中还有循环 两层嵌套 外层循环控制行 内存循环控制列 列不换行 内层循环结束后,在换行 /public class CircleQTDemo {public static void main(String[] args) { //请输出一个4行5列的星星(*)图案 for(int i=1;i&lt;=4;i++) {for(int j=1;j&lt;=5;j++) { System.out.print(&quot;*&quot;); } System.out.println(); }}} 案例:99乘法表的打印: 思路: 行: 共9行:外层循环控制行,共9次 列: 第一行有1列,第二行有2列…以此类推,第i行有i列 内层循环控制列,1-i 内存循环打印乘法表达式,多列不换行,内层循环结束后换行 /** \ 代表转义 tab: \t 换行: \n 回车: \r* 使用while 循环如何打印99*/public class NineNineXTableDemo { public static void main(String[] args) {//打印 &quot;helloworld&quot; System.out.println(&quot;\&quot;helloworld\&quot;&quot;); for(int i =1;i&lt;=9;i++) { for(int j=1;j&lt;=i;j++) { System.out.print(j+&quot;x&quot;+i+&quot;=&quot;+j*i+&quot;\t&quot;); } System.out.println(); } }} 1### 六、 跳转控制语句 break: 结束单层循环,在循环语句和switch语句中使用continue:结束本次循环,在循环语句中使用return: 结束整个方法,返回给生成调用者public class JumpControlTest { public static void main(String[] args) { for(int x=1; x&lt;=10; x++) { if(x%3==0) { //在此处填写代码 //break; //continue; System.out.println(“Java基础班”); } System.out.println(“Java基础班”); }// 我想在控制台输出2次:“Java基础班“ break;return// 我想在控制台输出7次:“Java基础班“// 我想在控制台输出13次:“Java基础班“ }} 1### 七、 方法: 1. 方法的定义 目的：解决代码重复编写的问题格式：修饰符 返回值类型 方法名(参数类型 参数名1，参数类型 参数名2…) { 函数体; return 返回值;}a. 方法格式的解释说明: • 修饰符 比较多，后面会详细介绍。目前public static • 返回值类型 用于限定返回值的数据类型 • 方法名 一个名称，为了方便我们调用方法 • 参数类型 限定调用方法时传入参数的数据类型 • 参数名 是一个变量，接收调用方法时传入的参数 • 方法体 完成功能的代码 • return 结束方法以及返回方法指定类型的值 • 返回值 程序被return带回的结果，返回给调用者 b. 案例: public class MethodDemo1 { public static void main(String[] args) { //1 [一万行]+张三 yiWanHang(“张三”);//调用 //2[一万行]+李四 yiWanHang(&quot;李四&quot;); //3[一万行]+王五 yiWanHang(&quot;王五&quot;); int a = 10; int b = 20; printSum(10,50);//调用的时候必须给具体的值 } //定义 public static void yiWanHang(String s) { System.out.println(&quot;一万行太短,只争朝夕&quot;+s); } //需求: 定义一个方法,打印两个数的和 public static void printSum(int a,int b) {//定义的时候,用的是变量的声明方式 System.out.println(a+b); } } 方法的调用方法名(符合参数类型和个数的具体值);练习:/** 定义两个方法: 打印两个double类型的值的差 打印圆的面积和周长 在main方法中调用: 分别求 12.4 和 12.1 的差 求半径是4的周长和面积 定义一个方法,传入一个行数和列数, 输出对应的星形(x行n列的星星) printStar(4,5);//打印4行4列的星星*/public class MethodTest { public static void main(String[] args) {printCha(12.5,34);printAreaAndZHOfCircle(12);printStar(8,10);} public static void printCha(double a,double b) {System.out.println(a-b);} public static void printAreaAndZHOfCircle(double r) {System.out.println(“圆的周长为:”+23.14r);System.out.println(“圆的面积为:”+3.14rr);} public static void printStar(int x,int y) {for(int i=1;i&lt;=x;i++) {for(int j=1;j&lt;=y;j++) { System.out.print(&quot;*&quot;); }System.out.println();}}} 方法的返回值 无返回值的方法: 返回值类型 void有返回值的方法 返回值类型 数据类型 必须return return 后面的值要和返回值类型相一致/** 定义一个方法，传入一个整数，并判断这个整数是不是偶数，如果是，返回true, 如果不是返回false 定义一个方法：传入两个double值，返回这两个值得差 定义一个方法,传入三个float类型的值,返回三个数中的最大值 在main方法中调用上面的方法*/public class MethodTest2 { public static void main(String[] args) {boolean re = isDouble(13);System.out.println(re);System.out.println(getCha(12.3,34));//只有有返回值的方法可以直接打印System.out.println(getMax(12.4f,12,34));} public static boolean isDouble(int a) {/if(a%2==0) { return true;}else { return false;}/// return a%2==0?true:false;return a%2==0;} public static double getCha(double x,double y) {return x-y;}public static float getMax(float a,float b,float c) {if(a&gt;b&amp;a&gt;c) { return a;}else if(b&gt;c) { return b;}else { return c;}} } 方法的重载a. 概念:一个类中可以存在多个名字相同的方法,但是必须保证参数的个数或类型不同,与返回值无关b. 案例:public class OverLoadDemo {public static void main(String[] args) { /System.out.println(getSum(3,4)); System.out.println(getSum(34L,45L));/ System.out.println(getSum(3,4L));//ambiguous: 模棱两可的 System.out.println();//println也是一个重载的方法} public static long getSum(long a,int b) { System.out.println(1); return a+b;} public static long getSum(int a,long b) { System.out.println(2); return a+b;} /*public static int getSum(int a,int b) {// 优先找类型一模一样的 System.out.println(&quot;int&quot;); return a+b; } public static long getSum(long a,long b) { System.out.println(&quot;long&quot;); return a+b; }*/ /*public static int getSum(int a,int b,int c) { return a+getSum(b,c); }*/ }`]]></content>
      <categories>
        <category>java</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础（二）]]></title>
    <url>%2F2017%2F12%2F01%2Fjava%E5%9F%BA%E7%A1%80(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[课程目标: 1. 了解关键字的概念及特点.了解保留字 2. 熟练掌握标识符的含义,特点,可使用字符,及注意事项 3. 了解常量的概念,进制,进制之间相互转换,了解有符号标识法的运算方式 4. 掌握变量的概念及定义过程 5. 掌握java中的数据类型及注意事项 6. 掌握强制类型转换的格式 一、关键字1、概念: 被java语言赋予特定含义的单词 2、特点： 所有的字母都是小写的，在一些高级开发工具中，会有特殊颜色显示 3、保留字： 目前版本中还不是关键字，但是有可能在后续版本中升级为关键字goto const 4、注意事项： 在一些资料中，可能会把关键字称为保留字 二、标识符1. 概念：给类、接口、方法、变量起名字的字符序列 2. 组成规则：英文字母的大小写 数字 ￥、_ 3. 注意事项： 不能以数字开头 不能是java中的关键字（包括保留字） 严格区分大小写（hello != Hello） 上述为强制要求，否则编译不通过 4. 注意点：见名知意 5. 约定俗称的命名规则： ａ.类和接口：大驼峰命名法 首字母大写，多个单词，每个单词的首字母都大写 eg：HelloWorld，StudentManagementSystem b.方法和变量：小驼峰命名法 首字母小写，多个单词，每个单词的首字母都大写 eg：main，sayHello c.包：本质是文件夹（方便管理，解决同名文件） 域名反转，用 . 隔开 d.常量：java程序运行的过程中，值保持不变的量 所有的字母都大写，多个单词用 _ 隔开 三、常量1. 概念：在程序中运行的过程中其值保持不变的量 2. 分类： a.字面值常量： 字符串常量-用双引号括起来的内容 整数常量-所有的整数 小数常量-所有的小数 字符常量-用单引号括起来的内容，只能有一个字符（字母、数字、符号、中文） 布尔常量-true、false 空常量-null b.自定义常量： final 修饰的变量 3. print 和 println的区别 print打印完内容不换行 println打印完内容自动换行 四、进制：1. 逢几进一：计算机中的都是二进制 2. 常用的进制有四种： 二进制： 0,1 0b 0，1，10，11，100..... 八进制： 0-7 0 0,1,....7,10,.....17,20...27,30...77,100.... 十进制： 0-9 默认 0,1....9,10... 十六进制： 0-9,a-f 0x 0,1...9,a,b,c,d,e,f,10....... 3. 二进制之间的相互转换： 其他进制转成10进制： 1234 = 1000+200+30+4 = 1*10^3 + 2*10^2 + 3*10^1 + *4 *10^0 结论：每一位上的数乘以对应进制的位数减一次方，做累加 0b10100 –&gt; 10进制 0b10100 = 1 *2^4 + 0* 2^3 + 1 *2^2 + 0*2^1 + 0*2^0 = 16 + 0 + 4 + 0 + 0 + 0 = 20 0137 –&gt; 10进制 0137 = 1 * 8^2 + 3 * 8^1 + 7 *8^0 = 95 0x13d -&gt; 10进制 0x123d = 1*16^2 + 3*16^1 + 13*16^0 = 256+61 = 317 4. 十进制之间的相互转换： 结论：转成几进制就处以几，直到商为0，把余数反转 76 -&gt;2 进制 76/2------0 38/2------0 19/2------0 9/2------1 4/2——12/2——01/2——1二进制：1001100 转八进制： 76/8—–0 9/8——4 1/8——1 ——1 5. 快速转换法： a. 8421码： 用于2进制和10进制之间的相互转换 8位二进制，每一位都是1，代表的数是几 0b 1111 1111-&gt; 128 64 32 16 8 4 2 1 0b10011001 -&gt; 10进制 128+16+8+1 = 153 0b11101100 -&gt; 10进制 =128+64+32+12=236 b. 十进制转二进制，和码表逐位相减，能减是1，不能减是0 93 = 0b0101 1101 c. 二进制和八进制，十六进制之间的转换 三位二进制 000-111 一位八进制 0-7 二进制和八进制相互转换： 二转八：三位二进制转换成一位的八进制 八转二：把一位的八进制拆成三位的二进制 0b10101010101110 -&gt; 0b010 101 010 101 110 = 025256 0b1 110 101 011 110 = 016536 07764 = 0b 111111110100 0752=0b111101010 d. 二进制和十六进制相互转换 0000-1111 0-15 0-f 0-15 二转十六：把四位的进制转成一位的十六进制 十六转二：把一位的十六进制拆成四位的二进制 0b11 1010 1011 1111 0101 = 0x3abf5 0b1 0101 1110 1011 = 0x15db 0x7ade -&gt;2进制0b0111 1010 1101 1110 e. x进制和y进制相互转换 六进制转七进制： 中间只用十进制作为桥梁 五、有符号标识法7： 0000 0111 为了解决负数在计算机中的存储，出现：原码，反码，补码 原码： 用最高位代表符号,其他位代表数值,正数的符号位是0,负数的符号位是1 都使用八位的二进制来表示 -7：1000 0111 反码: 正数的反码和原码相同,负数的反码,符号位不变,其他位取反 7的反码： 0 000 0111 -7的反码：1 111 1000 补码： 正数的补码和原码相同,负数的补码,在反码的基础上,末位加1 7的补码： 0 000 0111 -7的补码：1 111 1001 计算机中最终存储的都是二进制的补码 六、变量1. 概念： 在java程序运行的过程中,其值可以在一定范围内发生变化的量 2. 定义格式： 数据类型 变量名 = 初始化值; 数据类型： 整数： int 字符串：String 变量名：小驼峰 初始值：变量的初始化值 3. 变量的声明格式: 数据类型 变量名; int max; 4. 注意事项: 1. 定义在方法中的变量叫做局部变量,局部变量不赋初值不能使用 2. 变量所在的大括号叫做变量的作用域,同一个作用域中不能定义多个名字相同的变量,但是可以重新赋值 3. 同一行可以定义多个变量,但是不建议这么使用,可读性查 七、数据类型1. 分类和范围： a. 基本数据类型 四类八种 整数： byte、short、int、long 小数： float、double 字符： char 布尔： boolean b. 引用数据类型: 类、接口、数组、String…除了基本数据类型以外的所有类型 基本数据类型的范围 类型 字节 位数 默认值 范围 byte 1 8 0 -128~127 -2^7 ---2^7-1 short 2 16 0 -32768~32767 -2^15---2^15-1 int 4 32 0 -2147483648~2147483647 long 8 64 0 -9223372036854775808~9223372036854775807 float 4 32 0.0 -3.4E38~3.4028235E38 double 8 64 0.0 -1.79E-308~1.7976931348623157E308 char 2 16 \u0000 0~65535 boolean 1 8 false true或false bit: 一个二进制位 0,1 byte(字节): 8个二进制位 2^8 = 256 1kb = 1024byte 1mb = 1024kb 1gb = 1024mb 1tb = 1024gb 1pb = 1024tb 1eb = 1024pb 1zb = 1024eb 1yb = 1024zb 2. 注意事项: a. 整数的默认类型是int ,小数的默认类型是double b. 定义float类型的小数,需要在小数的后面加一个F(f) c. 定义byte,short类型的整数时, 如果等号右边的数值在左边类型的范围之内,则可以直接赋值 d. 定义long类型的变量时,如果等号右边的值在int的范围之内,则可以直接赋值.如果超出了int的范围 需要在数的末尾加 L,推荐使用大写 3. 默认值 a. 定义在类中方法外的变量才有默认值,局部变量是没有默认值的,静态的方法中只能调用外部静态的变量 4. 运算规则 a. byte,short,char 不能直接做运算,做运算的时候需要转成int b. 定义byte,short,类型的变量的时候, 如果等号右边都是常量,看其结果是否在左边类型的范围之内,在就可以直接赋值 如果等号右边存在变量,按照上面的规则进行运算,如果结果类型比左边的范围大,则不能直接赋值 5. 默认转换 a. 可以把一个long类型的值赋值给float类型的变量,因为float的最大值远大于long的最大值 float f = 123L; 正确的 float f1= 123; 正确的 float f2 = 123.1; 错误的,加 f byte,short,char -&gt;int -&gt;long - &gt;float -&gt; double b. 通过程序演示,发现可以把一个8字节的long类型数值赋值给4个字节的float类型变量: float 3.4*10 ^38 long: 2^63 -1 3.4*10^38 &gt; 3.4*8^38 = 3.4*(2^3)^38 = 3.4*2^114 2^63 通过最大值的比较,得出结论, float的最大值远大于long的最大值 所以我们可以把long赋值给float类型的变量 6.char类型 a. char类型的变量直接使用的时候,使用的是他所代表的符号,参与运算的时候,用的是他在码表中的编号 b. char 占两个字节（0-65535），用的是Unicode utf-8:表示英文和数字用1一个字节， gbk：表示英文和数字用一个字节，中文用两个字节 char:类型直接打印，用的是她所代表的字符，参与运算，用的是底层的编码 定义char类型： char ch = ‘a’； System.out.println(&apos;z&apos; + 0);//122 char ch2 = 97; System.out.println(ch2);//a 7.加法运算 a. 字符串和任意类型做加法运算,得到的都是字符串,做拼接 b. 运算的顺序是从左往右,右括号先算括号里的 c. 布尔类型不能和其他基本数据类型做运算 d. +的用法: 正号 加法运算 字符串的连接 ###八. 强制类型转换 1. 强制类型转换的格式: 目标类型 变量名 = (目标类型) 要转的值(常量或变量); a. 基本数据类型: 除了boolean 类型以外,其他7中类型都可以 互相转换 一般是在把大类型转成小类型的时候用强转 b. 引用数据类型: 必须具备子父类的关系 向下转型]]></content>
      <categories>
        <category>java</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础（一）]]></title>
    <url>%2F2017%2F12%2F01%2Fjava%E5%9F%BA%E7%A1%80(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[一、常用的dos命令1231. 打开dos控制台： 开始菜单-查找命令提示符 win + r -&gt; 输入 cmd -&gt; 确认 1234567891011121314152. 命令 切换盘符（不区分大小写，一定要加：）： d: 列出该目录下的所有文件（夹）： dir 进入指定目录： cd 目标路径 跨盘符操作（前后顺序可颠倒）： 1、cd 要进入的目录 2、盘符： 退回到上级目录： cd .. 退回到根目录： cd \ 创建文件夹： md 文件夹名称 删除文件夹： rd 文件夹名称 删除文件： del 文件名称 格式相同的文件 *.* 清屏： cls 退出： exit 二、java语言概述12345678910111213141516171819202122232425262728293031詹姆斯高斯林：java语言之父诞生于 sun公司，2009年被Oracle收购1995年java发布了第一个版本，96年java1.0从jdk1.5命名方式发生了改变 1.5 jdk5 1.6 jdk6 ...java平台版本 j2se 标准版 j2me 小型版 j2ee 企业版 1. java语言特点 简单性 面向对象 分布式处理 开源 跨平台：不同的操作系统上安装对应版本的java虚拟机 java是跨平台的jvm不是 2. jdk jvm：java virtual machine java虚拟机 jre：java runtime environment java运行环境 jdk：java Development Kit java开发工具包 jdk的安装 1. 下载 www.oracle.com 2. 安装 注意安装目录，不安装在c盘 3. 验证 看文件结构是否正确 三、Hellow World案例123456789101112131415161718192021222324252627 显示文件的扩展名 1. 创建HelloWorld.java编辑该文件，写代码： 创建一个class，类名和文件名保持一致： class HelloWorld&#123;&#125; 在类中创建一个main方法： public static void main（String[] args）&#123;&#125; 在main方法中写我们程序的功能： 打印一句话：HelloWorld System.out.println(&quot;hello world&quot;); 2. 运行代码 编译： 把.java文件转成.class文件（字节码文件,计算机能认识的文件) 先打开DOS控制台，win+r -&gt; cmd 确定 进入到HelloWorld.java所在的文件夹 cd 文件目录 运行编译的命令： javac 文件名.java javac HelloWorld.java 查看是否生成了.class文件，如果生成，代表编译通过 运行： java 类名 java HelloWorld 四、环境变量配置123456789101112131415161718192021为了可以在计算机的任意位置使用javac java...java命令这样就可以不必把代码写到bin文件下 1. 配置环境变量的步骤 1.计算机右键，选择属性，选择高级系统设置，选择环境变量 2.在系统变量中新建 变量名：JAVA_HOME 变量值：jdk目录 编辑系统变量中的Path 新建：%JAVA_HOME%\bin 3.把该变量移到最上边 点击三个确定 2. win7小心不要覆盖原有环境变量 验证是否配置成功： 重新打开cmd 控制台 输入java -version 如果提示java不是内部命令，则代表失败，重新配置 五、注释123456789101112131415起到解释说明的文字，不影响程序的运行注释不等于注解 1. 文档注释 /** 注释内容 */ 2. 多行注释 /* 注释内容 */ 3. 单行注释 // 注释内容]]></content>
      <categories>
        <category>java</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础（三）]]></title>
    <url>%2F2017%2F12%2F01%2Fjava%E5%9F%BA%E7%A1%80(%E4%B8%89)%2F</url>
    <content type="text"><![CDATA[一. 运算符:1234567 1. 算数运算符 a) + - * / % +： 正号，加法，字符串连接-： 负号，减法*： 乘法/： 取商 %： 取余 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 注意事项： 整数和整数相运算得到的还是整数，运算时从左往右，先算乘除后算加减，右括号先算括号 b)前加加减减和后加加减减 ++ 前加加: 先加1 ,后运算 后加加:先运算后加1 -- 前减减： 先减1 ,后运算 后减减:先运算后减 单独使用：效果一样，都是做加一操作： int a = 10; int b = 10; a++ ++b System.out.println(a); //10 System.out.println(b); //11 2. 赋值运算符 = += -= *= /= %= 包含了一个强制类型转换，尽量使用这样的写法 short s = 1; s=s+1; s+=1; 3. 关系运算符:关系运算符（也叫比较运算符）特点是返回的结果都是布尔类型的值，（true或false） 返回结果是布尔类型的值 == != &gt;= &lt;= &gt; &lt; instanceof 用于引用数据类型 &quot;hell&quot; instanceof String -&gt; true 4 instanceof int –&gt; 错误 4. 逻辑运算符逻辑运算符通常用来连接布尔类型的值 &amp;: AND 两个都是true结果才是true false&amp;true=false | : OR 只要有一个是true结果就是true false | ture = true ^: XOR 相同为false不同为true true ^ true = false !: 非 !true = false &amp; | 不管前面运算的结果是什么，后面都进行运算 &amp;&amp;: 前面是false ,后面则不再进行运算 ||: 前面是true,则后面不再进行运算 ~:把数字转成二进制逐位取反，包括符号位 &amp; 先于 ^ 先于 | a. 连接数字 &amp; | ^ 除了可以连接布尔类型外，还可以用来连接整数，连接数字,转成二进制补码,逐位运算,把1当成true,把0当成false 5. 位运算符针对二进制的补码做移位的操作 &lt;&lt;: 空位补0,最高位丢弃 &gt;&gt;: 空位补最高位 &gt;&gt;&gt;:无符号右移，空位补0 左移几位相当于乘以2的几次方 右移几位相当于除以2的几次方 6. 三目运算符（关系表达式）? 表达式1：表达式2； a&gt; b?a:b;如果条件为true，运算表达式1，条件为false，运算表达式2，表达式1和2，最终是一个明确的值，不能写输出语句 二、键盘录入123456789101112131415161718191. 导包: import java.util.Scanner;(放到class 上面)2. 创建对象: 使用对象获取键盘录入的信息：对象名.nextInt(); int对象名.nextDouble() double没有nextChar这个方法对象名.nextLine(); 获取字符串的 Scanenr sc = new Scanner(System.in);3. 获取键盘录入的值 int a = sc.nextInt(); String b = sc.nextLine();4. 注意事项：如果一个程序中，即使用了nextInt（nextDouble.nextLong…），同时还使用了nextLine() 如果把xextLine放到了nextInt（nextDouble.nextLong…）的下面 ，会导致nextLine()还未接收到内容就结束了5. 解决方案： a. 把nextLine()放到nextInt（nextDouble.nextLong…）上面 b. 可以使用next 替换nextLine 三、eclipse的使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849501. 下载 www.eclipse.org/downloads/packages2. 解压 a. 解压，进入目录，将eclipse.exe发送到桌面快捷方式 b. 选择工作空间：就是存放java代码的目录,不要勾选，选择工作空间3. 启动选择工作空间: 就是存放java代码的目录，不要勾选,选定工作空间,启动4. 直接x掉欢迎界面5. 配置a. 配置编码格式： c. 打开控制台windows-&gt;show view-&gt; console d. 布局介绍 e. 切换java视图 f. 创建一个项目 g. 快捷键自动提示: alt + / 生成main: main + alt+ / + 回车 生成输出语句 syso + alt+ / + 回车自动导包: ctrl +shift+o能导包,也能删除没有用到的包删除一行 ctrl+d在下方自动生成一行: shift+enter加上或取消单行注释 ctrl +/加上多行注释: ctrl +shift+/先把多行代码选中打开多行注释: ctrl +shift+\在下方复制代码 ctrl+alt+ down向上下移动代码 alt+up/down]]></content>
      <categories>
        <category>java</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Linux中安装和使用cpustat]]></title>
    <url>%2F2016%2F08%2F20%2F%E5%9C%A8Linux%E4%B8%AD%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8cpustat%2F</url>
    <content type="text"><![CDATA[cpustat 是 Linux 下一个强大的系统性能测量程序，它用 Go 编程语言 编写。它通过使用 “用于分析任意系统的性能的方法(USE)”，以有效的方式显示 CPU 利用率和饱和度。 它高频率对系统中运行的每个进程进行取样，然后以较低的频率汇总这些样本。例如，它能够每 200ms 测量一次每个进程，然后每 5 秒汇总这些样本，包括某些度量的最小/平均/最大值(min/avg/max)。 cpustat 能用两种方式输出数据：定时汇总的纯文本列表和每个取样的彩色滚动面板。如何在 Linux 中安装 cpustat 为了使用 cpustat，你的 Linux 系统中必须安装有 Go 语言(GoLang)，如果你还没有安装它，点击下面的链接逐步安装 GoLang： 在 Linux 下安装 GoLang(Go 编程语言) 安装完 Go 以后，输入下面的 go get 命令安装 cpustat，这个命令会将 cpustat 二进制文件安装到你的 GOBIN 变量(所指的路径)： # go get github.com/uber-common/cpustat如何在 Linux 中使用 cpustat 安装过程完成后，如果你不是以 root 用户控制系统，像下面这样使用 sudo 命令获取 root 权限运行 cpustat，否则会出现下面显示的错误信息： $ $GOBIN/cpustat This program uses the netlink taskstats interface, so it must be run as root.注意：想要像你系统中已经安装的其它 Go 程序那样运行 cpustat，你需要把 GOBIN 变量添加到 PATH 环境变量。打开下面的链接学习如何在 Linux 中设置 PATH 变量。 学习如何在 Linux 中永久设置你的 $PATH 变量cpustat 是这样工作的：在每个时间间隔查询 /proc 目录获取当前进程 ID 列表，然后： 对于每个 PID，读取 /proc/pid/stat，然后计算和前一个样本的差别。 如果是一个新的 PID，读取 /proc/pid/cmdline。 对于每个 PID，发送 netlink 消息获取 taskstat，计算和前一个样本的差别。 读取 /proc/stat 获取总的系统统计信息。根据获取所有这些统计信息所花费的时间，会调整每个休息间隔。另外，通过每次取样之间实际经过的时间，每个样本也会记录它用于测量的时间。这可用于计算 cpustat 自身的延迟。 当不带任何参数运行时，cpustat 默认会显示以下信息：样本间隔：200ms;汇总间隔：2s(10 个样本);显示前 10 个进程;用户过滤器：all;pid 过滤器：all。正如下面截图所示： $ sudo $GOBIN/cpustat cpustat – 监控 Linux CPU 使用在上面的输出中，之前显示的系统范围的度量字段意义如下： usr - 用户模式运行时间占 CPU 百分比的 min/avg/max 值。 sys - 系统模式运行时间占 CPU 百分比的 min/avg/max 值。 nice - 用户模式低优先级运行时间占 CPU 百分比的 min/avg/max 值。 idle - 用户模式空闲时间占 CPU 百分比的 min/avg/max 值。 iowait - 等待磁盘 IO 的 min/avg/max 延迟时间。 prun - 处于可运行状态的 min/avg/max 进程数量(同“平均负载”一样)。 pblock - 被磁盘 IO 阻塞的 min/avg/max 进程数量。 pstat - 在本次汇总间隔里启动的进程/线程数目。同样还是上面的输出，对于一个进程，不同列的意思分别是： name - 从 /proc/pid/stat 或 /proc/pid/cmdline 获取的进程名称。 pid - 进程 ID，也被用作 “tgid” (线程组 ID)。 min - 该 pid 的用户模式+系统模式时间的最小样本，取自 /proc/pid/stat。比率是 CPU 的百分比。 max - 该 pid 的用户模式+系统模式时间的最大样本，取自 /proc/pid/stat。 usr - 在汇总期间该 pid 的平均用户模式运行时间，取自 /proc/pid/stat。 sys - 在汇总期间该 pid 的平均系统模式运行时间，取自 /proc/pid/stat。 nice - 表示该进程的当前 “nice” 值，取自 /proc/pid/stat。值越高表示越好(nicer)。 runq - 进程和它所有线程可运行但等待运行的时间，通过 netlink 取自 taskstats。比率是 CPU 的百分比。 iow - 进程和它所有线程被磁盘 IO 阻塞的时间，通过 netlink 取自 taskstats。比率是 CPU 的百分比，对整个汇总间隔平均。 swap - 进程和它所有线程等待被换入(swap in)的时间，通过 netlink 取自 taskstats。Scale 是 CPU 的百分比，对整个汇总间隔平均。 vcx 和 icx - 在汇总间隔期间进程和它的所有线程自动上下文切换总的次数，通过 netlink 取自 taskstats。 rss - 从 /proc/pid/stat 获取的当前 RSS 值。它是指该进程正在使用的内存数量。 ctime - 在汇总间隔期间等待子进程退出的用户模式+系统模式 CPU 时间总和，取自 /proc/pid/stat。 注意长时间运行的子进程可能导致混淆这个值，因为只有在子进程退出后才会报告时间。但是，这对于计算高频 cron 任务以及 CPU 时间经常被多个子进程使用的健康检查非常有帮助。 thrd - 汇总间隔最后线程的数目，取自 /proc/pid/stat。 sam - 在这个汇总间隔期间该进程的样本数目。最近启动或退出的进程可能看起来比汇总间隔的样本数目少。下面的命令显示了系统中运行的前 10 个 root 用户进程： $ sudo $GOBIN/cpustat -u root 查找 root 用户正在运行的进程要想用更好看的终端模式显示输出，像下面这样用 -t 选项： $ sudo $GOBIN/cpustat -u root -t root 用户正在运行的进程要查看前 x 个进程(默认是 10)，你可以使用 -n 选项，下面的命令显示了系统中 正在运行的前 20 个进程： $ sudo $GOBIN/cpustat -n 20你也可以像下面这样使用 -cpuprofile 选项将 CPU 信息写到文件，然后用 cat 命令查看文件： $ sudo $GOBIN/cpustat -cpuprofile cpuprof.txt $ cat cpuprof.txt要显示帮助信息，像下面这样使用 -h 选项： $ sudo $GOBIN/cpustat -h可以从 cpustat 的 Github 仓库：https://github.com/uber-common/cpustat 查阅其它资料。 就是这些!在这篇文章中，我们向你展示了如何安装和使用 cpustat，Linux 下的一个有用的系统性能测量工具。通过下面的评论框和我们分享你的想法吧。 作者简介： Aaron Kili 是一个 Linux 和 F.O.S.S(Free and Open-Source Software) 爱好者，一个 Linux 系统管理员、web 开发员，现在也是 TecMint 的内容创建者，他喜欢和电脑一起工作，他相信知识共享。]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在Ubuntu上使用pm2和Nginx部署Node.js应用]]></title>
    <url>%2F2016%2F07%2F08%2F%E5%A6%82%E4%BD%95%E5%9C%A8Ubuntu%E4%B8%8A%E4%BD%BF%E7%94%A8pm2%E5%92%8CNginx%E9%83%A8%E7%BD%B2Node.js%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文主要展示如何安装和配置 pm2 用于这个简单的 ‘Express’ 应用，然后配置 Nginx 作为运行在 pm2 下的 node 应用的反向代理。 pm2 是一个 Node.js 应用的进程管理器，它可以让你的应用程序保持运行，还有一个内建的负载均衡器。它非常简单而且强大，你可以零间断重启或重新加载你的 node 应用，它也允许你为你的 node 应用创建集群。 在这篇博文中，我会向你展示如何安装和配置 pm2 用于这个简单的 ‘Express’ 应用，然后配置 Nginx 作为运行在 pm2 下的 node 应用的反向代理。前提： Ubuntu 16.04 - 64bit Root 权限第一步 - 安装 Node.js LTS在这篇指南中，我们会从零开始我们的实验。首先，我们需要在服务器上安装 Node.js。我会使用 Nodejs LTS 6.x 版本，它能从 nodesource 仓库中安装。从 Ubuntu 仓库安装 python-software-properties 软件包并添加 “nodesource” Nodejs 仓库。 sudo apt-get install -y python-software-properties curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -安装最新版本的 Nodejs LTS： sudo apt-get install -y nodejs安装完成后，查看 node 和 npm 版本。 node -v npm -v检查 node.js 版本第二步 - 生成 Express 示例 App我会使用 express-generator 软件包生成的简单 web 应用框架进行示例安装。express-generator 可以使用 npm 命令安装。用 npm安装 express-generator： npm install express-generator -g-g ： 在系统内部安装软件包。我会以普通用户运行应用程序，而不是 root 或者超级用户。我们首先需要创建一个新的用户。创建一个名为 yume 的用户： useradd -m -s /bin/bash yume passwd yume使用 su 命令登录到新用户： su - yume下一步，用 express 命令生成一个新的简单 web 应用程序： express hakase-app命令会创建新项目目录 hakase-app。用 express-generator 生成应用框架进入到项目目录并安装应用需要的所有依赖。 cd hakase-app npm install然后用下面的命令测试并启动一个新的简单应用程序： DEBUG=myapp:* npm start默认情况下，我们的 express 应用会运行在 3000 端口。现在访问服务器的 IP 地址：192.168.33.10:3000 ：express nodejs 运行在 3000 端口这个简单 web 应用框架现在以 ‘yume’ 用户运行在 3000 端口。第三步 - 安装 pm2pm2 是一个 node 软件包，可以使用 npm 命令安装。(用 root 权限，如果你仍然以 yume 用户登录，那么运行命令 exit 再次成为 root 用户)： npm install pm2 -g现在我们可以为我们的 web 应用使用 pm2 了。进入应用目录 hakase-app： su - yume cd ~/hakase-app/这里你可以看到一个名为 package.json 的文件，用 cat 命令显示它的内容。 cat package.json配置 express nodejs 服务你可以看到 start 行有一个 nodejs 用于启动 express 应用的命令。我们会和 pm2 进程管理器一起使用这个命令。像下面这样使用 pm2 命令运行 express 应用： pm2 start ./bin/www现在你可以看到像下面这样的结果：使用 pm2 运行 nodejs app我们的 express 应用正在 pm2 中运行，名称为 www，id 为 0。你可以用 show 选项 show nodeid|name 获取更多 pm2 下运行的应用的信息。 pm2 show wwwpm2 服务状态如果你想看我们应用的日志，你可以使用 logs 选项。它包括访问和错误日志，你还可以看到应用程序的 HTTP 状态。 pm2 logs wwwpm2 服务日志你可以看到我们的程序正在运行。现在，让我们来让它开机自启动。 pm2 startup systemd systemd： Ubuntu 16 使用的是 systemd。你会看到要用 root 用户运行命令的信息。使用 exit 命令回到 root 用户然后运行命令。 sudo env PATH=$PATH:/usr/bin /usr/lib/node_modules/pm2/bin/pm2 startup systemd -u yume –hp /home/yume它会为启动应用程序生成 systemd 配置文件。当你重启服务器的时候，应用程序就会自动运行。pm2 添加服务到开机自启动第四步 - 安装和配置 Nginx 作为反向代理在这篇指南中，我们会使用 Nginx 作为 node 应用的反向代理。Ubuntu 仓库中有 Nginx，用 apt 命令安装它： sudo apt-get install -y nginx下一步，进入到 sites-available 目录并创建新的虚拟主机配置文件。 cd /etc/nginx/sites-available/ vim hakase-app粘贴下面的配置： upstream hakase-app { # Nodejs app upstream server 127.0.0.1:3000; keepalive 64; } # Server on port 80 server { listen 80; server_name hakase-node.co; root /home/yume/hakase-app; location / { # Proxy_pass configuration proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection “upgrade”; proxy_max_temp_file_size 0; proxy_pass http://hakase-app/; proxy_redirect off; proxy_read_timeout 240s; } }保存文件并退出 vim。在配置中： node 应用使用域名 hakase-node.co 运行。 所有来自 nginx 的流量都会被转发到运行在 3000 端口的 node app。测试 Nginx 配置确保没有错误。 nginx -t启用 Nginx 并使其开机自启动。 systemctl start nginx systemctl enable nginx第五步 - 测试打开你的 web 浏览器并访问域名：http://hakase-app.co 。你可以看到 express 应用正在 Nginx web 服务器中运行。Nodejs app 在 pm2 和 Nginx 中运行下一步，重启你的服务器，确保你的 node app 能开机自启动： pm2 save sudo reboot如果你再次登录到了你的服务器，检查 node app 进程。以 yume 用户运行下面的命令。 su - yume pm2 status wwwnodejs 在 pm2 下开机自启动Node 应用在 pm2 中运行并使用 Nginx 作为反向代理。相关链接 Ubuntu Node.js Nginx]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nginx</tag>
        <tag>ubuntu</tag>
        <tag>node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内核态抢占机制分析]]></title>
    <url>%2F2016%2F06%2F15%2FLinux%E5%86%85%E6%A0%B8%E6%80%81%E6%8A%A2%E5%8D%A0%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文首先介绍非抢占式内核(Non-Preemptive Kernel)和可抢占式内核(Preemptive Kernel)的区别。接着分析Linux下有两种抢占：用户态抢占(User Preemption)、内核态抢占(Kernel Preemption)。然后分析了在内核态下：如何判断能否抢占内核(什么是可抢占的条件);何时触发重新调度(何时设置可抢占条件);抢占发生的时机(何时检查可抢占的条件);什么时候不能抢占内核。最后分析了2.6kernel中如何支持抢占内核。 1. 非抢占式和可抢占式内核的区别 为了简化问题，我使用嵌入式实时系统uC/OS作为例子。首先要指出的是，uC/OS只有内核态，没有用户态，这和Linux不一样。多任务系统中，内核负责管理各个任务，或者说为每个任务分配CPU时间，并且负责任务之间的通讯。内核提供的基本服务是任务切换。调度(Scheduler),英文还有一词叫dispatcher，也是调度的意思。这是内核的主要职责之一，就是要决定该轮到哪个任务运行了。多数实时内核是基于优先级调度法的。每个任务根据其重要程度的不同被赋予一定的优先级。基于优先级的调度法指，CPU总是让处在就绪态的优先级最高的任务先运行。然而，究竟何时让高优先级任务掌握CPU的使用权，有两种不同的情况，这要看用的是什么类型的内核，是不可剥夺型的还是可剥夺型内核。 非抢占式内核 非抢占式内核是由任务主动放弃CPU的使用权。非抢占式调度法也称作合作型多任务，各个任务彼此合作共享一个CPU。异步事件还是由中断服务来处理。中断服务可以使一个高优先级的任务由挂起状态变为就绪状态。但中断服务以后控制权还是回到原来被中断了的那个任务，直到该任务主动放弃CPU的使用权时，那个高优先级的任务才能获得CPU的使用权。非抢占式内核如下图所示。 非抢占式内核的优点有： 中断响应快(与抢占式内核比较); 允许使用不可重入函数; 几乎不需要使用信号量保护共享数据。运行的任务占有CPU，不必担心被别的任务抢占。这不是绝对的，在打印机的使用上，仍需要满足互斥条件。非抢占式内核的缺点有： 任务响应时间慢。高优先级的任务已经进入就绪态，但还不能运行，要等到当前运行着的任务释放CPU。 非抢占式内核的任务级响应时间是不确定的，不知道什么时候最高优先级的任务才能拿到CPU的控制权，完全取决于应用程序什么时候释放CPU。抢占式内核 使用抢占式内核可以保证系统响应时间。最高优先级的任务一旦就绪，总能得到CPU的使用权。当一个运行着的任务使一个比它优先级高的任务进入了就绪态，当前任务的CPU使用权就会被剥夺，或者说被挂起了，那个高优先级的任务立刻得到了CPU的控制权。如果是中断服务子程序使一个高优先级的任务进入就绪态，中断完成时，中断了的任务被挂起，优先级高的那个任务开始运行。抢占式内核如下图所示。 抢占式内核的优点有： 使用抢占式内核，最高优先级的任务什么时候可以执行，可以得到CPU的使用权是可知的。使用抢占式内核使得任务级响应时间得以最优化。抢占式内核的缺点有： 不能直接使用不可重入型函数。调用不可重入函数时，要满足互斥条件，这点可以使用互斥型信号量来实现。如果调用不可重入型函数时，低优先级的任务CPU的使用权被高优先级任务剥夺，不可重入型函数中的数据有可能被破坏。2. Linux下的用户态抢占和内核态抢占 Linux除了内核态外还有用户态。用户程序的上下文属于用户态，系统调用和中断处理例程上下文属于内核态。在2.6 kernel以前，Linux kernel只支持用户态抢占。 2.1 用户态抢占(User Preemption) 在kernel返回用户态(user-space)时，并且need_resched标志为1时，scheduler被调用，这就是用户态抢占。当kernel返回用户态时，系统可以安全的执行当前的任务，或者切换到另外一个任务。当中断处理例程或者系统调用完成后，kernel返回用户态时，need_resched标志的值会被检查，假如它为1，调度器会选择一个新的任务并执行。 中断和系统调用的返回路径(return path)的实现在entry.S中(entry.S不仅包括kernel entry code，也包括kernel exit code)。 2.2 内核态抢占(Kernel Preemption) 在2.6 kernel以前，kernel code(中断和系统调用属于kernel code)会一直运行，直到code被完成或者被阻塞(系统调用可以被阻塞)。在 2.6 kernel里，Linux kernel变成可抢占式。当从中断处理例程返回到内核态(kernel-space)时，kernel会检查是否可以抢占和是否需要重新调度。kernel可以在任何时间点上抢占一个任务(因为中断可以发生在任何时间点上)，只要在这个时间点上kernel的状态是安全的、可重新调度的。 3. 内核态抢占的设计 3.1 可抢占的条件 要满足什么条件，kernel才可以抢占一个任务的内核态呢? 没持有锁。锁是用于保护临界区的，不能被抢占。 Kernel code可重入(reentrant)。因为kernel是SMP-safe的，所以满足可重入性。如何判断当前上下文(中断处理例程、系统调用、内核线程等)是没持有锁的?Linux在每个每个任务的thread_info结构中增加了preempt_count变量作为preemption的计数器。这个变量初始为0，当加锁时计数器增一，当解锁时计数器减一。 3.2 内核态需要抢占的触发条件 内核提供了一个need_resched标志(这个标志在任务结构thread_info中)来表明是否需要重新执行调度。 3.3 何时触发重新调度 set_tsk_need_resched()：设置指定进程中的need_resched标志 clear_tsk need_resched()：清除指定进程中的need_resched标志 need_resched()：检查need_ resched标志的值;如果被设置就返回真，否则返回假 什么时候需要重新调度： 时钟中断处理例程检查当前任务的时间片，当任务的时间片消耗完时，scheduler_tick()函数就会设置need_resched标志; 信号量、等到队列、completion等机制唤醒时都是基于waitqueue的，而waitqueue的唤醒函数为default_wake_function，其调用try_to_wake_up将被唤醒的任务更改为就绪状态并设置need_resched标志。 设置用户进程的nice值时，可能会使高优先级的任务进入就绪状态; 改变任务的优先级时，可能会使高优先级的任务进入就绪状态; 新建一个任务时，可能会使高优先级的任务进入就绪状态; 对CPU(SMP)进行负载均衡时，当前任务可能需要放到另外一个CPU上运行;3.4 抢占发生的时机(何时检查可抢占条件) 当一个中断处理例程退出，在返回到内核态时(kernel-space)。这是隐式的调用schedule()函数，当前任务没有主动放弃CPU使用权，而是被剥夺了CPU使用权。 当kernel code从不可抢占状态变为可抢占状态时(preemptible again)。也就是preempt_count从正整数变为0时。这也是隐式的调用schedule()函数。 一个任务在内核态中显式的调用schedule()函数。任务主动放弃CPU使用权。 一个任务在内核态中被阻塞，导致需要调用schedule()函数。任务主动放弃CPU使用权。3.5 禁用/使能可抢占条件的操作 对preempt_count操作的函数有add_preempt_count()、sub_preempt_count()、inc_preempt_count()、dec_preempt_count()。 使能可抢占条件的操作是preempt_enable()，它调用dec_preempt_count()函数，然后再调用preempt_check_resched()函数去检查是否需要重新调度。 禁用可抢占条件的操作是preempt_disable()，它调用inc_preempt_count()函数。 在内核中有很多函数调用了preempt_enable()和preempt_disable()。比如spin_lock()函数调用了preempt_disable()函数，spin_unlock()函数调用了preempt_enable()函数。 3.6 什么时候不允许抢占 preempt_count()函数用于获取preempt_count的值，preemptible()用于判断内核是否可抢占。 有几种情况Linux内核不应该被抢占，除此之外，Linux内核在任意一点都可被抢占。这几种情况是： 内核正进行中断处理。在Linux内核中进程不能抢占中断(中断只能被其他中断中止、抢占，进程不能中止、抢占中断)，在中断例程中不允许进行进程调度。进程调度函数schedule()会对此作出判断，如果是在中断中调用，会打印出错信息。 内核正在进行中断上下文的Bottom Half(中断的下半部)处理。硬件中断返回前会执行软中断，此时仍然处于中断上下文中。 内核的代码段正持有spinlock自旋锁、writelock/readlock读写锁等锁，处干这些锁的保护状态中。内核中的这些锁是为了在SMP系统中短时间内保证不同CPU上运行的进程并发执行的正确性。当持有这些锁时，内核不应该被抢占，否则由于抢占将导致其他CPU长期不能获得锁而死等。 内核正在执行调度程序Scheduler。抢占的原因就是为了进行新的调度，没有理由将调度程序抢占掉再运行调度程序。 内核正在对每个CPU“私有”的数据结构操作(Per-CPU date structures)。在SMP中，对于per-CPU数据结构未用spinlocks保护，因为这些数据结构隐含地被保护了(不同的CPU有不一样的per-CPU数据，其他CPU上运行的进程不会用到另一个CPU的per-CPU数据)。但是如果允许抢占，但一个进程被抢占后重新调度，有可能调度到其他的CPU上去，这时定义的Per-CPU变量就会有问题，这时应禁抢占。4. Linux内核态抢占的实现 4.1 数据结构 [cpp] view plain copy struct thread_info { struct task_struct task; / main task structure / struct exec_domain exec_domain; / execution domain / / 如果有TIF_NEED_RESCHED标志，则必须调用调度程序。 / unsigned long flags; / low level flags / / 线程标志: TS_USEDFPU:表示进程在当前执行过程中，是否使用过FPU、MMX和XMM寄存器。 / unsigned long status; / thread-synchronous flags / /** 可运行进程所在运行队列的CPU逻辑号。 / __u32 cpu; / current CPU / __s32 preempt_count; / 0 =&gt; preemptable, BUG / mm_segment_t addr_limit; / thread address space: 0-0xBFFFFFFF for user-thead 0-0xFFFFFFFF for kernel-thread / struct restart_block restart_block; unsigned long previous_esp; / ESP of the previous stack in case of nested (IRQ) stacks / __u8 supervisor_stack[0]; };4.2 代码流程禁用/使能可抢占条件的函数[cpp] view plain copy #ifdef CONFIG_DEBUG_PREEMPT extern void fastcall add_preempt_count(int val); extern void fastcall sub_preempt_count(int val); #else # define add_preempt_count(val) do { preempt_count() += (val); } while (0) # define sub_preempt_count(val) do { preempt_count() -= (val); } while (0) #endif #define inc_preempt_count() add_preempt_count(1) #define dec_preempt_count() sub_preempt_count(1) /** 在thread_info描述符中选择preempt_count字段 / #define preempt_count() (current_thread_info()-&gt;preempt_count) #ifdef CONFIG_PREEMPT asmlinkage void preempt_schedule(void); /** 使抢占计数加1 / #define preempt_disable() do { inc_preempt_count(); barrier(); } while (0) /** 使抢占计数减1 / #define preempt_enable_no_resched() do { barrier(); dec_preempt_count(); } while (0) #define preempt_check_resched() do { if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) preempt_schedule(); } while (0) /** 使抢占计数减1，并在thread_info描述符的TIF_NEED_RESCHED标志被置为1的情况下，调用preempt_schedule() / #define preempt_enable() do { preempt_enable_no_resched(); preempt_check_resched(); } while (0) #else #define preempt_disable() do { } while (0) #define preempt_enable_no_resched() do { } while (0) #define preempt_enable() do { } while (0) #define preempt_check_resched() do { } while (0) #endif设置need_resched标志的函数[cpp] view plain copy static inline void set_tsk_need_resched(struct task_struct tsk) { set_tsk_thread_flag(tsk,TIF_NEED_RESCHED); } static inline void clear_tsk_need_resched(struct task_struct *tsk) { clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED); }]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS+Postfix+Dovecot+Postfixadmin+Roundcube邮件服务器配置POP3+IMAP+SMTP]]></title>
    <url>%2F2016%2F05%2F14%2FCentOS%2BPostfix%2BDovecot%2BPostfixadmin%2BRoundcube%E9%82%AE%E4%BB%B6%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AEPOP3%2BIMAP%2BSMTP%2F</url>
    <content type="text"><![CDATA[之前一直使用Postfix+Courier+Sasl+Extmail 邮件服务器方案，并配置了MailDrop 做邮件转发和Mailman邮件列表，在两年多时间里，运行良好。可是现生产环境使用Nginx 越来越多，为了一个Webmail 单独配置Apache+Perl 资源开销大，Courier+Sasl 配置也相对比较繁琐，抽了两天时间，结合网上一些实例，对邮件服务器做了一些改进。 Dovecot 不仅可以做POP3,IMAP服务器，也可以用来做SMTP验证，省去了Cyrus Sasl，并且效率资源占用也相对Courier 好很多，同时支持LOGIN验证方式，可以满足Outlook,Foxmail 客户端登录访问，本文配置了SMTP发件认证，POP3接收认证，WEBMail 功能，邮件转发，防病毒未有进行配置。 软件包说明:Postfix-2.8.12.tar.gz Postfix MTA(邮件传输代理)Dovecot-2.1.8.tar.gz IMAP 和 POP3 邮件服务器 Postfixadmin-2.3.5.tar.gz 采用PHP编写的开源WEB邮箱及域名账号管理工具 Roundcubemail-0.8.1.tar.gz 采用PHP编写的开源IMAP邮件WEB客户端 1.准备工作：安装配置在 CentOS 6.3 x64 最小化安装环境上进行，先安装好 Nginx(Apache)+PHP+MySQL，Roundcube 需要PHP IMAP扩展支持，如果在编译PHP时没开启IMAP支持，可以用下面的方法添加扩展 # yum install libc-client-devel libc-client安装 php-imap 扩展依赖的的廉包 进入到php源码包 imap 扩展库路径下 如 /opt/php-5.3.12/ext/imap 执行 # /usr/local/php/bin/phpize ./configure –with-php-config=/usr/local/php/bin/php-config –with-kerberos –with-imap-ssl检查系统配置过程中国如果提示出错，可以尝试将libc-client 库做个链接到lib 下 # ln -s /usr/lib64/libc-client.so /usr/lib/libc-client.so make &amp;&amp; make install安装后修改 php.ini 的 extension_dir路径 ，并加入 extension=”imap.so” 扩展 卸载系统自带的sendmail 或 postfix # yum remove sendmail postfix 2. 编译安装Postfix # yum -y install db4-devel//安装依赖的DB4开发包 # useradd -M -s /sbin/nologin postfix useradd -M -s /sbin/nologin postdrop//添加Postfix,maildrop 用户 # tar zxf postfix-2.8.12.tar.gzcd postfix-2.8.12/make makefiles ‘CCARGS=-DHAS_MYSQL -I/usr/local/mysql/include -DUSE_SASL_AUTH -DDEF_SERVER_SASL_TYPE=”dovecot”‘ ‘AUXLIBS=-L/usr/local/mysql/lib -lmysqlclient -lz -lm -lssl -lcrypto’//如果MySQL 安装在其他路径，请注意修改MySQL Include 和 lib 路径 #make#make install 安装路径和参数配置 install_root: / //相对目录 tempdir: /tmp //临时目录 config_directory: /etc/postfix //配置文件位置 command_directory /usr/local/postfix/sbin //命令执行路径 daemon_directory /usr/local/postfix/libexec //Daemon路径 data_directory /var/lib/postfix //邮件数据 html_directory no mail_owner postfix //postfix 所有者账号 mailq_path /usr/bin/mailq //mailq 位置 manpage_directory /usr/local/postfix/man //帮助文档 newaliases_path /usr/bin/newaliases //newaliases 位置 queue_directory /var/spool/postfix //队列路径 readme_directory no sendmail_path /usr/sbin/sendmail //Sendmail 路径 setgid_group: postdrop 3.配置Postfix# vi /etc/postfix/main.cf 查找并修改 mynetworks = all 否则SMTP发件认证不生效 在最底部添加虚拟邮箱配置和验证设置 # Virtual mailbox settings.virtual_mailbox_base = /var/vmailvirtual_mailbox_maps = mysql:/etc/postfix/mysql_virtual_mailbox_maps.cfvirtual_mailbox_domains = mysql:/etc/postfix/mysql_virtual_domains_maps.cfvirtual_alias_maps = mysql:/etc/postfix/mysql_virtual_alias_maps.cfvirtual_uid_maps = static:邮件POSTFIX用户IDvirtual_gid_maps = static:邮件POSTFIX组IDvirtual_transport = virtualmessage_size_limit = 10240000virtual_mailbox_limit = 209715200virtual_create_maildirsize = yesvirtual_mailbox_extended = yesvirtual_mailbox_limit_maps = mysql:/etc/postfix/mysql_virtual_limit_maps.cfvirtual_mailbox_limit_override = yesvirtual_maildir_limit_message = Sorry, the user’s maildir has exceeded the quota.virtual_overquota_bounce = yes SASL settingssmtpd_sasl_auth_enable = yessmtpd_sasl_local_domain = $mydomainsmtpd_sasl_security_options = noanonymoussmtpd_sasl_type = dovecotsmtpd_sasl_path = /var/lib/dovecot/run/dovecot/auth-loginbroken_sasl_auth_clients = yessmtpd_recipient_restrictions = permit_mynetworks, permit_sasl_authenticated, reject_unauth_destination, reject_invalid_hostname, reject_non_fqdn_hostname, reject_non_fqdn_sender, reject_non_fqdn_recipientvirtual_uid_maps 和 virtual_gid_maps 改成postfix 用户的uid和gid 如果不知道ID，可用 id postfix 命令获取 添加数据库查询配置文件，根据需要修改数据库用户名密码，主机名，数据库名配置信息，在后面创建MYSQL数据库，登录用户的时候需要用到。 建立数据库别名查询配置文件 vi /etc/postfix/mysql_virtual_alias_maps.cf user = postfixpassword = postfixpasswordhosts = localhostdbname = postfixtable = aliasselect_field = gotowhere_field = addressadditional_conditions = and active = ‘1’ #query = SELECT goto FROM alias WHERE address=’%s’ AND active = ‘1’建立数据库虚拟域查询配置文件 vi /etc/postfix/mysql_virtual_domains_maps.cf user = postfixpassword = postfixpasswordhosts = localhostdbname = postfixtable = domainselect_field = domainwhere_field = domainadditional_conditions = and active = ‘1’ #query = SELECT domain FROM domain WHERE domain=’%s’ AND active = ‘1’建立数据库邮箱配额查询配置文件 vi /etc/postfix/mysql_virtual_mailbox_limit_maps.cf user = postfixpassword = postfixpasswordhosts = localhostdbname = postfixtable = mailboxselect_field = quotawhere_field = usernameadditional_conditions = and active = ‘1’ #query = SELECT quota FROM mailbox WHERE username=’%s’ AND active = ‘1’建立数据库虚拟邮箱查询配置文件 vi /etc/postfix/mysql_virtual_mailbox_maps.cf user = postfixpassword = postfixpasswordhosts = localhostdbname = postfixtable = mailboxselect_field = CONCAT(domain,’/‘,maildir)where_field = usernameadditional_conditions = and active = ‘1’ #query = SELECT CONCAT(domain,’/‘,maildir) FROM mailbox WHERE username=’%s’ AND active = ‘1’启动postfix # /usr/local/postfix/sbin/postfix start 4.安装 Dovecot # tar zxf dovecot-2.1.8.tar.gz#cd dovecot-2.1.8#./configure –prefix=/usr/local/dovecot –sysconfdir=/etc –localstatedir=/var –with-sql –with-mysql –with-zlib –with-ssl LDFLAGS=-L/usr/local/mysql/lib//指定安装及配置文件路径//如果mysql安装在其他位置，需要手动指定 mysql lib库路径 # make# make install cp -r /usr/local/dovecot/share/doc/dovecot/example-config/* /etc/dovecot///复制配置文件示例到配置文件夹中，此步骤不是必须的，只是方便查阅，后面的配置不会用到这些文件 5. 配置 Doevcotmv /etc/dovecot/dovecot.conf /etc/dovecot/dovecot.old 如果复制了示例配置，先改名备份 vi /etc/dovecot/dovecot.conf 建立新配置文件，加入以下内容，此配置文件只适用于 Doevcot 2.x 不能用于1.x rotocols = pop3 imap 开启pop3 imap listen = * default_login_user=postfix default_internal_user=postfix 使用postfix 用户disable_plaintext_auth = no log_path = /var/log/dovecot.log 日志路径#info_log_path = /var/log/dovecot.info log_timestamp = “%Y-%m-%d %H:%M:%S “ ssl = no mail_location = maildir:/var/vmail/%d/%u 邮件存储路径mail_privileged_group = mail first_valid_uid = 502 postfix 用户UID protocol pop3 { pop3_uidl_format = %08Xu%08Xv } auth_mechanisms = plain login passdb { driver=sql args = /etc/dovecot/dovecot-mysql.conf } userdb { driver=sql args = /etc/dovecot/dovecot-mysql.conf } 用于SMTP验证service auth { unix_listener /var/spool/postfix/private/auth { group = postfix user = postfix mode = 0660 } }建立MYSQL 配置文件，注意修改连接用户名密码，和前面的postfix配置一致 vi /etc/dovecot/dovecot-mysql.conf driver = mysqlconnect = host=localhost dbname=postfix user=postfix password=postfixpassworddefault_pass_scheme = MD5-CRYPTpassword_query = SELECT password FROM mailbox WHERE username = ‘%u’user_query = SELECT maildir, 502 AS uid, 502 AS gid FROM mailbox WHERE username = ‘%u’建立邮箱文件夹,并给予postfix 用户权限 mkdir -pv /var/vmail chown -R postfix.postfix /var/vmail 启动服务 /usr/local/dovecot/sbin/dovecot 6.安装配置 Postfixadmin解压 postfixadmin.tar.gz 并复制到站点目录下面 修改 config.inc.php 文件，参考下面的配置进行，这里的数据库用户名密码均和前面的配置一致 $CONF[‘configured’] = true;$CONF[‘default_language’] = ‘en’;$CONF[‘database_type’] = ‘mysql’;$CONF[‘database_host’] = ‘localhost’;$CONF[‘database_user’] = ‘postfix’;$CONF[‘database_password’] = ‘postfixpassword’;登录mysql 控制台建立postfix数据库，并建立一个postifx用户指定对 posfix 数据库本地访问权限 &gt; create database postfix; grant all privileges on postfix.* to postfix@localhost identified by ‘postfix密码’;flush privileges;exit配置完成后执行 http://ip地址/postfixadmin/setup.php 进行安装 安装后将 修改配置密码，把得到的 加密 密码字串复制并再次修改config.inc.php 文件 $CONF[‘setup_password’] = ‘密码字串’; 然后在页面上输入配置密码，并建立管理员账号,完成后 删除 setup.php 登录 http://ip地址/postfixadmin 可以建立虚拟域和邮箱 7. 配置RoundCube解压 roundclube.tar.gz 并放置在网站目录下 登录mysql 控制台建立roundcube数据库，并建立一个roundcube用户指定对 roundcube数据库本地访问权限 &gt;create database roundcube; grant all privileges on roundcube.* to roundcube@localhost identified by ‘roundcube密码’;flush privileges;exit打开浏览器 http://yourdomain/webmail/installer/ 填写配置后将 main.inc.php 配置 和 db.inc.php 配置复制并覆盖 config/ 下对应文件即可]]></content>
      <categories>
        <category>linux</category>
        <category>centos</category>
        <category>转载</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下yum命令是什么意思 原理和使用方法详解]]></title>
    <url>%2F2016%2F04%2F14%2Flinux%E4%B8%8Byum%E5%91%BD%E4%BB%A4%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%20%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 yum的命令形式一般是如下：yum [options] [command] [package …]其中的[options]是可选的，选项包括-h（帮助），-y（当安装过程提示选择全部为”yes”），-q（不显示安装的过程）等等。[command]为所要进行的操作，[package …]是操作的对象。概括了部分常用的命令包括： 自动搜索最快镜像插件： yum install yum-fastestmirror安装yum图形窗口插件： yum install yumex查看可能批量安装的列表： yum grouplist 1 安装yum install 全部安装yum install package1 安装指定的安装包package1yum groupinsall group1 安装程序组group1 2 更新和升级yum update 全部更新yum update package1 更新指定程序包package1yum check-update 检查可更新的程序yum upgrade package1 升级指定程序包package1yum groupupdate group1 升级程序组group1 3 查找和显示yum info package1 显示安装包信息package1yum list 显示所有已经安装和可以安装的程序包yum list package1 显示指定程序包安装情况package1yum groupinfo group1 显示程序组group1信息yum search string 根据关键字string查找安装包 4 删除程序yum remove | erase package1 删除程序包package1yum groupremove group1 删除程序组group1yum deplist package1 查看程序package1依赖情况 5 清除缓存yum clean packages 清除缓存目录下的软件包yum clean headers 清除缓存目录下的 headersyum clean oldheaders 清除缓存目录下旧的 headersyum clean, yum clean all (= yum clean packages; yum clean oldheaders) 清除缓存目录下的软件包及旧的headers 比如，要安装游戏程序组，首先进行查找：＃：yum grouplist可以发现，可安装的游戏程序包名字是”Games and Entertainment“，这样就可以进行安装：＃：yum groupinstall “Games and Entertainment”所 有的游戏程序包就自动安装了。在这里Games and Entertainment的名字必须用双引号选定，因为linux下面遇到空格会认为文件名结束了，因此必须告诉系统安装的程序包的名字是“Games and Entertainment”而不是“Games”。 此外，还可以修改配置文件/etc/yum.conf选择安装源。可见yum进行配置程序有多方便了吧。更多详细的选项和命令，当然只要在命令提示行下面:man yum 代码如下:yum groupinstall “KDE (K Desktop Environment)” yum install pirut k3b mikmod yum groupinstall “Server Configuration Tools” yum groupinstall “Sound and Video” #yum groupinstall “GNOME Desktop Environment” yum groupinstall “Legacy Software Support” yum groupinstall “Development Libraries” yum groupinstall “Development Tools” #yum groupinstall “Windows File Server” yum groupinstall “System Tools” yum groupinstall “X Window System” yum install php-gdyum install gd-develyum groupinstall “Chinese Support” #yum install samba-common //该执行会一起安装 samba-client #yum install samba yum install gccyum install cppyum install gcc-c++yum install ncursesyum install ncurses-develyum install gd-devel php-gdyum install gd-develyum install gccyum install cppyum install gcc-c++yum install ncursesyum install ncurses-develyum install gd-devel php-gdyum install gd-develyum install zlib-develyum install freetype-devel freetype-demos freetype-utilsyum install libpng-devel libpng10 libpng10-develyum install libjpeg-develyum install ImageMagickyum install php-gdyum install flexyum install ImageMagick-devel #yum install system-config-bind #yum groupinstall “DNS Name Server” //安裝 bind 及 bind-chroot 套件yum groupinstall “MySQL Database”‘ yum clean all 装了个fedora linux不能用中文输入是一件很棘手的事，连搜解决方案都没法搜。只能勉强用几个拼音碰碰运气，看Google能不能识别了。而我就遇见了这样的事。解决方案：yum install scim* -y yum 命令详解：Redhat和Fedora的软件安装命令是rpm，但是用rpm安装软件最大的麻烦就是需要手动寻找安装该软件所需要的一系列依赖关系，超级 麻烦不说，要是软件不用了需要卸载的话由于卸载掉了某个依赖关系而导致其他的软件不能用是非常恼人的。令人高兴的是，Fedora终于推出了类似于 ubuntu中的apt的命令yum，令Fedora的软件安装变得简单容易。Yum 有以下特点：可以同时配置多个资源库(Repository) 简洁的配置文件(/etc/yum.conf)自动解决增加或删除rpm包时遇到的倚赖性问题 使用方便*保持与RPM数据库的一致性yum，是Yellow dog Updater Modified的简称，起初是由yellow dog这一发行版的开发者Terra Soft研发，用python写成，那时还叫做yup(yellow dog updater)，后经杜克大学的Linux@Duke开发团队进行改进，遂有此名。yum的宗旨是自动化地升级，安装/移除rpm包，收集rpm包的相关信息，检查依赖性并自动提示用户解决。yum的关键之处是要有可靠的repository，顾名思义，这是软件的仓库，它可以是http或ftp站点， 也可以是本地软件池，但必须包含rpm的header， header包括了rpm包的各种信息，包括描述，功能，提供的文件，依赖性等.正是收集了这些 header并加以分析，才能自动化地完成余下的任务。1).yum的一切配置信息都储存在一个叫yum.conf的配置文件中，通常位于/etc目 录下，这是整个yum系统的重中之重，我在的F9中查看了这一文件，大家一起来看下：[hanlong@localhost F9常用文档]$ sudo more /etc/yum.conf[main]cachedir=/var/cache/yumkeepcache=0debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1metadata_expire=1800 PUT YOUR REPOS HERE OR IN separate files named file.repoin /etc/yum.repos.d下面简单的对这一文件作简要的说明： cachedir：yum缓存的目录，yum在此存储下载的rpm包和数据库，一般是/var/cache/yum。debuglevel：除错级别，0──10,默认是2logfile：yum的日志文件，默认是/var/log/yum.log。exactarch，有两个选项1和0,代表是否只升级和你安装软件包cpu体系一致的包，如果设为1，则如你安装了一个i386的rpm，则yum不会用686的包来升级。gpgchkeck= 有1和0两个选择，分别代表是否是否进行gpg校验，如果没有这一项，默认好像也是检查的。 2).好了，接下来就是yum的使用了，首先用yum来升级软件，yum的操作大都须有超级用户的权限，当然可以用sudo。yum update，这一步是必须的，yum会从服务器的header目录下载rpm的header，放在本地的缓存中，这可能会花费一定的时间，但比起yum 给我们带来方便，这些时间的花费又算的了什么呢？header下载完毕，yum会判断是否有可更新的软件包，如果有，它会询问你的意见，是否更新，还是说 y吧，把系统始终up to date总是不错的，这时yum开始下载软件包并使用调用rpm安装，这可能要一定时间，取决于要更新软件的数目和网络状况，万一网络断了，也没关系，再 进行一次就可以了。升级完毕，以后每天只要使用yum check-update检查一下有无跟新，如果有，就用yum update进行跟新，时刻保持系统为最新，堵住一切发现的漏洞。用yum update packagename 对某个单独包进行升级。 现在简单的把yum软件升级的一些命令罗列一下： (更新：我在安装wine的时候是用rpm一个一个安装的，先安装以来关系，然后在安装wine的主包，但是刚刚在论坛上发现来一个好的帖子，就yum的本地安装。参数是-localinstall$yum localinstall wine-这样的话，yum会自动安装所有的依赖关系，而不用rpm一个一个的安装了，省了好多工作。还有一个与他类似的参数：$yum localupdate wine-如果有wine的新版本，而且你也下载到来本地，就可以这样本地更新wine了。) 1.列出所有可更新的软件清单命令：yum check-update 2.安装所有更新软件命令：yum update 3.仅安装指定的软件命令：yum install 4.仅更新指定的软件命令：yum update 5.列出所有可安裝的软件清单命令：yum list 3).使用yum安装和卸载软件，有个前提是yum安装的软件包都是rpm格式的。 安装的命令是，yum install xxx，yum会查询数据库，有无这一软件包，如果有，则检查其依赖冲突关系，如果没有依赖冲突，那么最好，下载安装;如果有，则会给出提示，询问是否要同时安装依赖，或删除冲突的包，你可以自己作出判断删除的命令是，yum remove xxx，同安装一样，yum也会查询数据库，给出解决依赖关系的提示。 1.用YUM安装软件包命令：yum install 2.用YUM删除软件包命令：yum remove 4).用yum查询想安装的软件我们常会碰到这样的情况，想要安装一个软件，只知道它和某方面有关，但又不能确切知道它的名字。这时yum的查询功能就起作用了。你可以用 yum search keyword这样的命令来进行搜索，比如我们要则安装一个Instant Messenger，但又不知到底有哪些，这时不妨用 yum search messenger这样的指令进行搜索，yum会搜索所有可用rpm的描述，列出所有描述中和messeger有关的rpm包，于是我们可能得到 gaim，kopete等等，并从中选择。有时我们还会碰到安装了一个包，但又不知道其用途，我们可以用yum info packagename这个指令来获取信息。1.使用YUM查找软件包命令：yum search2.列出所有可安装的软件包命令：yum list3.列出所有可更新的软件包命令：yum list updates4.列出所有已安装的软件包命令：yum list installed5.列出所有已安装但不在 Yum Repository 內的软件包命令：yum list extras6.列出所指定的软件包命令：yum list 7.使用YUM获取软件包信息命令：yum info 8.列出所有软件包的信息命令：yum info9.列出所有可更新的软件包信息命令：yum info updates10.列出所有已安裝的软件包信息命令：yum info installed11.列出所有已安裝但不在 Yum Repository 內的软件包信息命令：yum info extras12.列出软件包提供哪些文件命令：yum provides 5).清除YUM缓存yum 会把下载的软件包和header存储在cache中，而不会自动删除。如果我们觉得它们占用了磁盘空间，可以使用yum clean指令进行清除，更精确的用法是yum clean headers清除header，yum clean packages清除下载的rpm包，yum clean all 清除所有1.清除缓存目录(/var/cache/yum)下的软件包命令：yum clean packages 2.清除缓存目录(/var/cache/yum)下的 headers 命令：yum clean headers 3.清除缓存目录(/var/cache/yum)下旧的 headers 命令：yum clean oldheaders 4.清除缓存目录(/var/cache/yum)下的软件包及旧的headers 命令：yum clean, yum clean all (= yum clean packages; yum clean oldheaders) 以上所有命令参数的使用都可以用man来查看：1、安装图形版yumex：yum install yumex。2、安装额外的软件仓库：rpm.livna.org 的软件包仓库:rpm -ivh http://livna-dl.reloumirrors.net … ease-7-2.noarch.rpm freshrpms.net 的软件包仓库:rpm –ivh http://ftp.freshrpms.net/pub/fre … 1.1-1.fc.noarch.rpm 3、安装最快源 yum install yum-fastestmirror 资源真的是非常丰富，从Centos到Ubuntu，ISO镜像、升级包，应有尽有，上交的兄弟们真是幸福，羡慕啊。不过还好，我们好歹也算是在教育网内，凑合着也可以沾点光，下载一些。网址为：ftp://ftp.sjtu.edu.cn/ 相应的yum的repo为[updates]name=Fedora updatesbaseurl=ftp://ftp.sjtu.edu.cn/fedora/linux/updates/$releasever/$basearch/enabled=1gpgcheck=0[fedora]name=Fedora $releasever – $basearchbaseurl=ftp://ftp.sjtu.edu.cn/fedora/linux/releases/$releasever/Everything/$basearch/os/enabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-fedora file:///etc/pki/rpm-gpg/RPM-GPG-KEY 如果在机器上安装了apt管理器，则相应的源为repomd ftp://ftp.sjtu.edu.cn/ fedora/linux/updates/$(VERSION)/$(ARCH)/ repomd ftp://ftp.sjtu.edu.cn/ fedora/linux/releases/$(VERSION)/Everything/$(ARCH)/os/ 这与前面yum的源的格式上有一些差别，需要加以注意。下面介绍一下fedora 下安装 scim 1． 什么输入法适合我？fcitx和scim是目前比较好的输入法， 但是他们的特点不同，fcitx只能输入中文，而scim可以根据需要，利用不同的码表达到中英日…等等各种语言的输入问题。如果你只懂中文，或者只会输 入英文&amp;中文，那么fcitx是一个不错的选择，因为它漂亮，小巧，实用。如果你还需要输入日文或者其他语言，那么你因该安装scim。通 过合理的配置，他能够让你像在windows里面一样，想输入什么语言就能输入什么语言，同一种语言想用什么输入法就用什么输入法。Scim的扩充性很 强，而且比较稳定，我就是选择的是scim.2． 安装一个新输入法前需要哪些准备？如果你选择fcitx或者scim，那么我建议你删除系统自带的中文输入法。方法如下：rpm –qa | grep iiimf | xargs rpm –erpm –qa | grep Chinput| xargs rpm –e如果有哪一行提示rpm: no packages given for erase那表示本身你的系统里面没有该输入法，不用担心，继续往下看就行了。说 明：rpm –qa是列出所有安装的rpm包，grep iiimf是选择出其中名字含有iiimf的那些包，xargs rpm –e的意思是把前面列出的这些包删除掉。Xargs的作用就是把前面通过 | 传过来的数据作为下面命令的参数。这就好比一个过滤器，首先是放着所有的安装包，然后grep以后，只留下了含有某些特定关键字的rpm包，然后通过 xargs和rpm –e的组合运用，把剩下的这些含有某特定关键字的包删掉。这样就达到了删除该输入法及相关包的目的。下面的Chinput也是如此，在此不再重复。如果你还安装了其他输入法，比如你原来装的是fcitx，现在想装scim，那么你最好模仿上面的样子把fcitx删除，方法就是把iiimf的位置改成 fcitx就可以了。在安装新输入法之前，最好这样做一下，因为多种输入法同时存在一个系统中没有什么好处，你只可能去用一个，而且他们同时存在可能有的时候会出现问题，想想也知道，会互相竞争嘛。所以在此以后，你应该保证系统里面已经没有中文输入法了。通过类似以下方式验证：whereis fcitxwhereis scimwhereis miniChinput… 3． 输入法是何时被系统调用的？很多人不知道输入法到底什么时候被load进来，不知道这个当然就不知道为什么有的时候呼不出输入法（因为可能根本没有调入）当然也不会知道如何配置能够符合自己的要求。大 家都知道，linux下面比较常用的有两个桌面系统，gnome和kde，这都无所谓，他们其实都是架在X系统之上的。简单的说X系统就是一个最核心，也 是最底层的桌面系统，gnome也好，kde也罢，或者其他的什么fvwm之类的，都只不过是X系统和用户之间的另一层软件而已。所以要想达到不管使用什 么桌面系统，都能调入输入法，就是要在X系统启动的时候，让输入法也启动起来，那么这样之后，无论你使用的是gnome还是kde或者其他什么桌面，都能 够调入输入法。因为当轮到他们启动的时候，X系统已经启动好了，输入法已经被系统调入了。那么X系统又是如何启动的呢？让我们从startx开始说起。无论你用什么桌面系统，都是通过startx启动的，那么startx究竟是什么呢？一个应用程序还是一个脚本文件？为什么它能够启动各种桌面系统，并且能够按照相应的配置文件来设置呢？带着疑问，我在console里面输入whereis startx.在 找到了存放startx的路径以后，用编辑器打开它发现原来是一个脚本文件。这个脚本文件的内容可能根据发行版不同，会有差异，如果你懂一些shell的 语言，那么你可以尝试看看，不一定要全部看懂，但是你大致看过以后会发现最后有一个xinit的命令，然后跟着一些参数。我尝试在console下面输入 xinit(注意，不要在图形界面下做此操作)发现图形界面启动拉，但是很丑陋，什么功能都没有，鼠标可以动，还有一个可以输入命令的小窗口。怎 么退出来？ctrl+alt+backspace. 原来如此，startx只是一个脚本，里面通过对一系列配置文件的分析设置，最终利用xinit命令启动图形界面。不管是kde还是gnome，都是在这 个脚本中完成的。那么让我们再打开startx脚本看看里面还做了些什么。你仔细看看，会发现有一个东西很显眼，就是/etc/X11/xinit /xinitrc，这个xinitrc好像很眼熟，在配置输入法的其他贴中总是看到，这里出现了肯定是里面运行了这个脚本。再看其他的一些东西，其实都是利用shell配置出一个xinit启动的参数，用来配置桌面系统用的，不用管它。目前已知的就是startx的时候它会去执行一个/etc/X11 /xinit/xinitrc的脚本，让我们打开来看看里面有什么。打开一看，其中一段我觉得最有价值，是一个for循环，他依次执行了/etc /X11/xinit/xinitrc.d/下面的所有脚本。你可以耐心的找一下，一定可以发现。那么这个目录里面有些什么内容呢？有一个文件看名字就知道和输入相关，他叫xinput。等等，让我们理一下，是怎么从startx到xinput的。首先是执行startx这个脚本文件，里面他会执行xinitrc这个脚本，然后xinitrc脚本里面的，叫xinput。OK, 我们继续，打开xinput看看。 4.安装软件包 rpm -Uvh scim-0.8.2-1.i586.rpmrpm -Uvh scim-chinese-0.2.6-1.i586.rpm 5.修改配置文件 接下来重要的一步就是，修改一下/etc /X11/xinit/xinitrc.d/xinput文件，让SCIM在X启动的时候也启动。我看到网上很多文章也说过，但总是不得要领，经过自己试 验，最简单的就是把xinput文件里的Chinput全部替换为SCIM，chinput替换为scim，保存重启X就可以了。zh_CN*)if [ -e /usr/bin/scim ]; thenXIM=”SCIM”elif [ -e /usr/X11R6/bin/xcin ] ; thenexport LC_CTYPE=zh_CN.GB2312XIM=”xcin-zh_CN.GB2312″fi;; ………………………… SCIM)XIM_PROGRAM=scimXIM_ARGS=”-d”;; 然后修改/etc/gtk-2.0/gtk.immodules，找到这一行:“xim” “X Input Method” “gtk20″ “/usr/share/locale” “ko:ja:zh”改为：“xim” “X Input Method” “gtk20″ “/usr/share/locale” “en:ko:ja:zh”可能表示输入英文时也使用该输入法 安装完毕后运行scim-setup,将输入法的切换键改为ctrl+space yum常用命令总结主要功能是更方便的添加/删除/更新RPM包.它能自动解决包的倚赖性问题.它能便于管理大量系统的更新问题一、yum list|more 列出所有包文件，可搭配grep查询软件包，如yum list |grep kernel二、yum info xxx 显示包xxx详细信息，即使xxx没有安装三、yum update kernel 用yum升级内核四、yum update 全面升级系统五、yum list available 列出升级源上所有可以安装的包(List all packages in the yum repositories available to be installed.)六、yum list updates 列出升级源上所有可以更新的包(List all packages with updates available in the yum repositories.)七、yum list installed 列出已经安装的包八、yum install xxx 安装xxx包九、yum update xxx 升级xxx包十、yum remove xxx 删除xxx包 转载至-爱分享–支持原创 联系我--strivedeer@163.com]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
</search>
